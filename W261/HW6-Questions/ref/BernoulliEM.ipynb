{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR Book 16.3 Concept Code\n",
    "\n",
    "This code implements the Bernoulli EM algorithm as described in the IR Book, example 16.3. This exercise is useful to ensure an understanding of the algorithm and to check functionality. There are a few bits that aren't explained will in the book but become apparent when working with this code. \n",
    "\n",
    "- During the computation of the $r_{n,k}$ classifications it is best to use log(probabilities) in order to prevent underflow\n",
    "\n",
    "- The use of $\\epsilon$ as a smoothing parameter is crucial to the good behavior of the algorithm and to avoid either a divide by zero or a log(0) problem. In the IR book $\\epsilon$ is set to 0.0001. Setting to smaller values causes the algorithm to take more iterations to converge to the same solution as in the book.\n",
    "\n",
    "In a Bernoulli Mixture Model a document is a vector of Booleans indicating the presence of a term.\n",
    "\n",
    "The conditional probability of a document given a set of parameters is given by:\n",
    "\n",
    "$$P(d \\;|\\;\\theta) = \\sum_{k=1}^{K}\\alpha_k(\\prod_{tm \\in d} q_{mk})(\\prod_{tm \\notin d}(1 - q_mk))$$\n",
    "\n",
    "This is the sum for each class of the product of the probabilities of the terms in a document with 1 minus the probabilities of the terms not in the document.\n",
    "\n",
    "The probability that a document from cluster $\\omega_{k}$ containts term $t_{m}$ is given by:\n",
    "\n",
    "$$q_{mk} = P(U_{m} = 1 \\; | \\;\\omega_{k})$$\n",
    "\n",
    "The prior $\\alpha_{k}$ of cluster $\\omega_{k}$ is the probability document $d$ is in $\\omega_{k}$ if we have no other information about it.\n",
    "\n",
    "When we don't know the classifications of the documents we can use Expectation Maximization iteratively to arrive at classifications, $r_{nk}$.\n",
    "\n",
    "**E Step**\n",
    "\n",
    "$$\\tag{1} r_{nk} = \\frac{\\alpha_{k}(\\prod_{tm \\in d_n}q_{mk})(\\prod_{tm \\in d_n}1-q_{mk})}{\\sum_{k=1}^{K}\\alpha_{k}(\\prod_{tm \\in d_n}q_{mk})(\\prod_{tm \\in d_n}1-q_{mk})}$$\n",
    "\n",
    "The actual computation as implemented by taking the sum of the log probabilities as opposed to the products of the probabilities themselves. This is necessary because once you are dealing with many terms, multiplying a lot of small numbers results in numeric underflow. Also, in order to prevent taking the log(0), which will happen in the first iteration, a very small number, $\\epsilon$, is added to the probability before taking the log. So we end up with:\n",
    "\n",
    "$$\\tag{1a} \\frac{\\alpha_{k}e^{\\left(\\sum_{tm \\in d_n}log(q_{mk}+\\epsilon) + \\sum_{tm \\notin d_n}log(1-q_{mk}+\\epsilon) \\right)}}{\\sum_{k}\\alpha_{k} e^{\\left(\\sum_{tm \\in d_n} \\left(log(q_{mk}+\\epsilon) + \\sum_{tm \\notin d_n}log(1-q_{mk}+\\epsilon) \\right) \\right)}}$$\n",
    "\n",
    "In the code below, a single pass is made to calculate the $r$ class soft assignments so that the terms are calculated only once so it may not be obvious what's going on.\n",
    "\n",
    "_Note: the $\\epsilon$ values are important for the algorithm to converge. If you take a straight calculation of the $r_{1,1}$ term in the first iteration you will end up with a divide by zero using the original equations. By including the $\\epsilon$ term this is avoided and you'll see that the result is very close to 1_\n",
    "\n",
    "**M Step**\n",
    "\n",
    "$$\\tag{2} q_{mk} = \\frac{\\sum_{n=1}^{N}r_{nk}I(t_m \\in d_n)}{\\sum_{i=1}^{N}r_{nk}}$$\n",
    "\n",
    "$I(t_{m} \\in d_{n}) = 1$ if term is an element of document n and 0 otherwise.\n",
    "\n",
    "Finally, priors are updated per iteration as:\n",
    "\n",
    "$$\\tag{3} \\alpha_k = \\frac{\\sum_{n=1}^{N}r_{nk}}{N}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save this to a file - it will be handy for MR testing later\n",
    "with open('test.txt','w') as outfile:\n",
    "    outfile.write('hot chocolate cocoa beans\\n')\n",
    "    outfile.write('cocoa ghana africa\\n')\n",
    "    outfile.write('beans harvest ghana\\n')\n",
    "    outfile.write('cocoa butter\\n')\n",
    "    outfile.write('butter truffles\\n')\n",
    "    outfile.write('sweet chocolate\\n')\n",
    "    outfile.write('sweet sugar\\n')\n",
    "    outfile.write('sugar cane brazil\\n')\n",
    "    outfile.write('sweet sugar beet\\n')\n",
    "    outfile.write('sweet cake icing\\n')\n",
    "    outfile.write('cake black forest\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "# read the documents. Each document consists of a list of words.\n",
    "documents = []\n",
    "with open('test.txt', 'r') as docfile:\n",
    "    for line in docfile.readlines():\n",
    "        documents.append(re.split(' ', line.strip()))\n",
    "\n",
    "classes = 2\n",
    "r = [[None] * len(documents), [None] * len(documents)]\n",
    "\n",
    "# set our initial conditions (r_6,1 = 1.0 and r_7,1 = 0.0 and the converse for the other class)\n",
    "r[0][5] = r[1][6] = 1.0\n",
    "r[0][6] = r[1][5] = 0.0\n",
    "\n",
    "# initialize the priors\n",
    "alpha = [0.0] * classes\n",
    "\n",
    "epsilon = 0.0001\n",
    "\n",
    "# conditional term probabilities\n",
    "qm = {}\n",
    "\n",
    "# compute alphas - Equation 3\n",
    "def compute_alphas(alphas):\n",
    "    for k in range(classes):\n",
    "        alphas[k] = sum([x for x in r[k] if x is not None])/len([x for x in r[k] if x is not None])\n",
    "\n",
    "\n",
    "# compute inverted postings list\n",
    "# this is handy for the computation of the qm's\n",
    "def compute_postings():\n",
    "    postings = {}\n",
    "    for i in range(len(documents)):\n",
    "        if r[0][i] is not None:\n",
    "            for word in documents[i]:\n",
    "                if word not in postings:\n",
    "                    postings[word] = [i]\n",
    "                else:\n",
    "                    if i not in postings[word]:\n",
    "                        postings[word].append(i)\n",
    "    return postings\n",
    "\n",
    "# compute qm's - Equation 2\n",
    "def compute_next_qms(qm):\n",
    "    for k in range(classes):\n",
    "        for i in range(len(r[k])):\n",
    "            if r[k][i] is not None:\n",
    "                for word in documents[i]:\n",
    "                    if word not in qm:\n",
    "                        qm[word] = {k: sum([r[k][j] for j in postings[word]]) / \\\n",
    "                                    sum([x for x in r[k] if x is not None])}\n",
    "                    else:\n",
    "                        qm[word][k] = sum([r[k][j] for j in postings[word]]) / \\\n",
    "                                    sum([x for x in r[k] if x is not None])\n",
    "    \n",
    "\n",
    "# compute next iteration of r's - Equation 1a\n",
    "# note - need to do log(probability)\n",
    "def compute_next_r(rs):\n",
    "    for i in range(len(documents)):\n",
    "        vocab_words_in_doc = []\n",
    "        p = np.zeros((classes, 2))\n",
    "        \n",
    "        # find all vocab words in the doc. Note: there may be none.\n",
    "        for word in documents[i]:\n",
    "            if word in qm:\n",
    "                vocab_words_in_doc.append(word)\n",
    "                for k in range(classes):\n",
    "                    p[k][0] += np.log(qm[word][k] + epsilon)\n",
    "        if len(vocab_words_in_doc) > 0:\n",
    "            # find all vocab words not in doc for all classes at the same time\n",
    "            for word in qm:\n",
    "                if word not in vocab_words_in_doc:\n",
    "                    for k in range(classes):\n",
    "                        p[k][1] += np.log(1-qm[word][k] + epsilon)\n",
    "                        \n",
    "            # compute the denominator of Equation 1a for all classes\n",
    "            denom = 0.0\n",
    "            for k in range(classes):\n",
    "                denom += alpha[k]*np.exp(p[k][0]+p[k][1])\n",
    "                \n",
    "            # compute the new r of Equation 1a for all classes\n",
    "            for k in range(classes):\n",
    "                rs[k][i] = alpha[k]*np.exp(p[k][0]+p[k][1])/denom\n",
    "\n",
    "        else:\n",
    "            # set to prior in case of no information\n",
    "            for k in range(classes):\n",
    "                rs[k][i] = alpha[k]\n",
    "\n",
    "# iterate\n",
    "for _ in range(25):\n",
    "    compute_alphas(alpha)\n",
    "    postings = compute_postings()\n",
    "    compute_next_qms(qm)\n",
    "    compute_next_r(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you take out rounding you can see that these values are not really exact\n",
    "np.around(r, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
