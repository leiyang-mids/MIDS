{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459464088528_-300688666","id":"20160331-184128_939116635","dateCreated":"Mar 31, 2016 6:41:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:52","text":"%md\n\n#DATASCI W261, Machine Learning at Scale\n--------\n####Assignement:  week \\#11\n####[Lei Yang](mailto:leiyang@berkeley.edu) | [Michael Kennedy](mailto:mkennedy@ischool.berkeley.edu) | [Natarajan Krishnaswami](mailto:natarajan@krishnaswami.org)\n####Due: 2016-04-05, 8AM PST\n\n###HW11.0  Broadcast versus Caching in Spark\n- What is the difference between broadcasting and caching data in Spark? Give an example (in the context of machine learning) of each mechanism (at a highlevel). Feel free to cut and paste code examples from the lectures to support your answer.\n- Review the following Spark-notebook-based implementation of KMeans and use the broadcast pattern to make this implementation more efficient. Please describe your changes in English first, implement, comment your code and highlight your changes:\n - Notebook: https://www.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb?dl=0\n - Notebook via NBViewer: http://nbviewer.ipython.org/urls/dl.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb","dateUpdated":"Mar 31, 2016 6:46:22 PM","dateFinished":"Mar 31, 2016 6:45:57 PM","dateStarted":"Mar 31, 2016 6:45:57 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>DATASCI W261, Machine Learning at Scale</h1>\n<hr />\n<h4>Assignement:  week #11</h4>\n<h4><a href=\"mailto:leiyang@berkeley.edu\">Lei Yang</a> | <a href=\"mailto:mkennedy@ischool.berkeley.edu\">Michael Kennedy</a> | <a href=\"mailto:natarajan@krishnaswami.org\">Natarajan Krishnaswami</a></h4>\n<h4>Due: 2016-04-05, 8AM PST</h4>\n<h3>HW11.0  Broadcast versus Caching in Spark</h3>\n<ul>\n<li>What is the difference between broadcasting and caching data in Spark? Give an example (in the context of machine learning) of each mechanism (at a highlevel). Feel free to cut and paste code examples from the lectures to support your answer.</li>\n<li>Review the following Spark-notebook-based implementation of KMeans and use the broadcast pattern to make this implementation more efficient. Please describe your changes in English first, implement, comment your code and highlight your changes:</li>\n<li>Notebook: https://www.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb?dl=0</li>\n<li>Notebook via NBViewer: http://nbviewer.ipython.org/urls/dl.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb</li>\n</ul>\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459465695969_547199911","id":"20160331-190815_789329833","dateCreated":"Mar 31, 2016 7:08:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:341","text":"%md\n\n$\\frac{1}{3}$","dateUpdated":"Mar 31, 2016 7:19:55 PM","dateFinished":"Mar 31, 2016 7:19:47 PM","dateStarted":"Mar 31, 2016 7:19:47 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<p>$\\frac{1}{3}$</p>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459464125585_171645066","id":"20160331-184205_459668203","dateCreated":"Mar 31, 2016 6:42:05 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:69","dateUpdated":"Mar 31, 2016 6:47:24 PM","dateFinished":"Mar 31, 2016 6:47:06 PM","dateStarted":"Mar 31, 2016 6:47:06 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>HW11.1  Loss Functions</h3>\n<ul>\n<li>In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a L2 penalized logistic regesssion learning algorithm?</li>\n<li>In your reponse, please discuss the loss functions, and the learnt models, and separating surfaces between the two classes.</li>\n<li>In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a perceptron learning algorithm?</li>\n<li>[OPTIONAL]: generate an artifical binary classification dataset with 2 input features and plot the learnt separating surface for both a linear SVM and for  logistic regression. Comment on the learnt surfaces. Please feel free to do this in Python (no need to use Spark).</li>\n</ul>\n"},"text":"%md\n\n###HW11.1  Loss Functions\n- In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a L2 penalized logistic regesssion learning algorithm? \n- In your reponse, please discuss the loss functions, and the learnt models, and separating surfaces between the two classes.\n- In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a perceptron learning algorithm? \n- [OPTIONAL]: generate an artifical binary classification dataset with 2 input features and plot the learnt separating surface for both a linear SVM and for  logistic regression. Comment on the learnt surfaces. Please feel free to do this in Python (no need to use Spark)."},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459464403621_559470313","id":"20160331-184643_1231542283","dateCreated":"Mar 31, 2016 6:46:43 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:97","dateUpdated":"Mar 31, 2016 6:48:06 PM","dateFinished":"Mar 31, 2016 6:48:00 PM","dateStarted":"Mar 31, 2016 6:48:00 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>HW11.2 Gradient descent</h3>\n<ul>\n<li>In the context of logistic regression describe and define three flavors of penalized loss functions.  Are these all supported in Spark MLLib (include online references to support your answers)?</li>\n<li>Descibe probabilitic interpretations of the L1 and L2 priors for penalized logistic regression (HINT: see synchronous slides for week 11 for details)</li>\n</ul>\n"},"text":"%md \n\n###HW11.2 Gradient descent\n- In the context of logistic regression describe and define three flavors of penalized loss functions.  Are these all supported in Spark MLLib (include online references to support your answers)?\n- Descibe probabilitic interpretations of the L1 and L2 priors for penalized logistic regression (HINT: see synchronous slides for week 11 for details)\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459464480352_-394817518","id":"20160331-184800_1404685140","dateCreated":"Mar 31, 2016 6:48:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:117","dateUpdated":"Mar 31, 2016 6:57:53 PM","dateFinished":"Mar 31, 2016 6:57:49 PM","dateStarted":"Mar 31, 2016 6:57:49 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>HW11.3  Logistic Regression</h3>\n<ul>\n<li><p>Generate 2 sets of linearly separable data with 100 data points each using the data generation code provided below and plot each in separate plots. Call one the training set and the other the testing set.</p>\n<pre><code> def generateData(n):\n \"\"\" \n  generates a 2D linearly separable dataset with n samples. \n  The third element of the sample is the label\n \"\"\"\n xb = (rand(n)*2-1)/2-0.5\n yb = (rand(n)*2-1)/2+0.5\n xr = (rand(n)*2-1)/2+0.5\n yr = (rand(n)*2-1)/2-0.5\n inputs = []\n for i in range(len(xb)):\n     inputs.append([xb[i],yb[i],1])\n     inputs.append([xr[i],yr[i],-1])\n return inputs\n</code></pre>\n</li>\n<li><p>Modify this data generation code to generating non-linearly separable training and testing datasets (with approximately 10% of the data falling on the wrong side of the separating hyperplane. Plot the resulting datasets.</p>\n</li>\n<li><p><strong>NOTE</strong>: For the remainder of this problem please use the non-linearly separable training and testing datasets.</p>\n</li>\n<li><p>Using MLLib  train up a LASSO logistic regression model with the training dataset and evaluate with the testing set. What a good number of iterations for training the logistic regression model? Justify with plots and words.</p>\n</li>\n<li><p>Derive and implement in Spark a weighted  LASSO logistic regression. Implement a convergence test of your choice to check for termination within your training algorithm .</p>\n</li>\n<li><p>Weight the above training dataset as follows:  Weight each example using the inverse vector length (Euclidean norm):</p>\n<ul>\n<li>weight(X)= 1/||X||,</li>\n<li>where ||X|| = SQRT(X.X)= SQRT(X1<sup>2 + X2</sup>2)</li>\n<li>Here X is vector made up of X1 and X2.</li>\n</ul>\n</li>\n<li><p>Evaluate your homegrown weighted  LASSO logistic regression on the test dataset. Report misclassification error (1 - Accuracy) and how many iterations does it took to converge.</p>\n</li>\n<li><p>Does Spark MLLib have a weighted LASSO logistic regression implementation. If so use it and report your findings on the weighted training set and test set.</p>\n</li>\n</ul>\n"},"text":"%md\n###HW11.3  Logistic Regression\n- Generate 2 sets of linearly separable data with 100 data points each using the data generation code provided below and plot each in separate plots. Call one the training set and the other the testing set.\n\n         def generateData(n):\n         \"\"\" \n          generates a 2D linearly separable dataset with n samples. \n          The third element of the sample is the label\n         \"\"\"\n         xb = (rand(n)*2-1)/2-0.5\n         yb = (rand(n)*2-1)/2+0.5\n         xr = (rand(n)*2-1)/2+0.5\n         yr = (rand(n)*2-1)/2-0.5\n         inputs = []\n         for i in range(len(xb)):\n             inputs.append([xb[i],yb[i],1])\n             inputs.append([xr[i],yr[i],-1])\n         return inputs\n\n- Modify this data generation code to generating non-linearly separable training and testing datasets (with approximately 10% of the data falling on the wrong side of the separating hyperplane. Plot the resulting datasets. \n- **NOTE**: For the remainder of this problem please use the non-linearly separable training and testing datasets.\n- Using MLLib  train up a LASSO logistic regression model with the training dataset and evaluate with the testing set. What a good number of iterations for training the logistic regression model? Justify with plots and words. \n- Derive and implement in Spark a weighted  LASSO logistic regression. Implement a convergence test of your choice to check for termination within your training algorithm . \n- Weight the above training dataset as follows:  Weight each example using the inverse vector length (Euclidean norm): \n    - weight(X)= 1/||X||,\n    - where ||X|| = SQRT(X.X)= SQRT(X1^2 + X2^2)\n    - Here X is vector made up of X1 and X2.\n\n- Evaluate your homegrown weighted  LASSO logistic regression on the test dataset. Report misclassification error (1 - Accuracy) and how many iterations does it took to converge.\n- Does Spark MLLib have a weighted LASSO logistic regression implementation. If so use it and report your findings on the weighted training set and test set. \n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459464566685_-1196710846","id":"20160331-184926_834505382","dateCreated":"Mar 31, 2016 6:49:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:139","dateUpdated":"Mar 31, 2016 6:58:36 PM","dateFinished":"Mar 31, 2016 6:58:31 PM","dateStarted":"Mar 31, 2016 6:58:31 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>HW11.4 SVMs</h3>\n<ul>\n<li>Use the non-linearly separable training and testing datasets from HW11.3 in this problem.</li>\n<li>Using MLLib  train up a soft SVM model with the training dataset and evaluate with the testing set. What is a good number of iterations for training the SVM model? Justify with plots and words.</li>\n<li>Derive and Implement in Spark a weighted soft linear svm classification learning algorithm.</li>\n<li>Evaluate your homegrown weighted soft linear svm classification learning algorithm on the weighted training dataset and test dataset from HW11.3. Report misclassification error (1 - Accuracy) and how many iterations does it took to converge?  How many support vectors do you end up?</li>\n<li>Does Spark MLLib have a weighted soft SVM learner. If so use it and report your findings on the weighted training set and test set.</li>\n</ul>\n"},"text":"%md\n\n###HW11.4 SVMs\n\n- Use the non-linearly separable training and testing datasets from HW11.3 in this problem.\n- Using MLLib  train up a soft SVM model with the training dataset and evaluate with the testing set. What is a good number of iterations for training the SVM model? Justify with plots and words. \n- Derive and Implement in Spark a weighted soft linear svm classification learning algorithm.\n- Evaluate your homegrown weighted soft linear svm classification learning algorithm on the weighted training dataset and test dataset from HW11.3. Report misclassification error (1 - Accuracy) and how many iterations does it took to converge?  How many support vectors do you end up?\n- Does Spark MLLib have a weighted soft SVM learner. If so use it and report your findings on the weighted training set and test set. \n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459465111501_237248828","id":"20160331-185831_1389673392","dateCreated":"Mar 31, 2016 6:58:31 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:165","dateUpdated":"Mar 31, 2016 7:07:04 PM","dateFinished":"Mar 31, 2016 7:06:56 PM","dateStarted":"Mar 31, 2016 7:06:56 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>HW11.5 [OPTIONAL]: Distributed Perceptron algorithm.</h3>\n<ul>\n<li>Using the following papers as background:<ul>\n<li>http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36266.pdf</li>\n<li>https://www.dropbox.com/s/a5pdcp0r8ptudgj/gesmundo-tomeh-eacl-2012.pdf?dl=0</li>\n<li>http://www.slideshare.net/matsubaray/distributed-perceptron</li>\n</ul>\n</li>\n<li>Implement each of the following flavors of perceptron learning algorithm:<ol>\n<li>Serial (All Data): This is the classifier returned if trained serially on all the available data.  On a single computer for example (Mistake driven)</li>\n<li>Serial (Sub Sampling): Shard the data, select one shard randomly and train serially.</li>\n<li>Parallel (Parameter Mix): Learn a perceptron locally on each shard: Once learning is complete combine each learnt perceptron using a uniform weighting</li>\n<li>Parallel (Iterative Parameter Mix) as described in the above papers.</li>\n</ol>\n</li>\n</ul>\n"},"text":"%md\n\n###HW11.5 [OPTIONAL]: Distributed Perceptron algorithm.\n- Using the following papers as background:\n    - http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36266.pdf\n    - https://www.dropbox.com/s/a5pdcp0r8ptudgj/gesmundo-tomeh-eacl-2012.pdf?dl=0\n    - http://www.slideshare.net/matsubaray/distributed-perceptron \n- Implement each of the following flavors of perceptron learning algorithm:\n    1. Serial (All Data): This is the classifier returned if trained serially on all the available data.  On a single computer for example (Mistake driven)\n    2. Serial (Sub Sampling): Shard the data, select one shard randomly and train serially. \n    3. Parallel (Parameter Mix): Learn a perceptron locally on each shard: Once learning is complete combine each learnt perceptron using a uniform weighting\n    4. Parallel (Iterative Parameter Mix) as described in the above papers."},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459465176715_-389214326","id":"20160331-185936_1607929726","dateCreated":"Mar 31, 2016 6:59:36 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:185","dateUpdated":"Mar 31, 2016 7:07:51 PM","dateFinished":"Mar 31, 2016 7:07:47 PM","dateStarted":"Mar 31, 2016 7:07:47 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>HW11.6 [OPTIONAL: consider doing this in a group] Evalution of perceptron algorihtms on PennTreeBank POS corpus</h3>\n<ul>\n<li><p>Reproduce the experiments reported in the following paper:</p>\n<ul>\n<li>HadoopPerceptron: a Toolkit for Distributed Perceptron Training and Prediction with MapReduce Andrea Gesmundo and  Nadi Tomeh</li>\n<li>http://www.aclweb.org/anthology/E12-2020</li>\n</ul>\n</li>\n<li><p>These experiments focus on the prediction accuracy on a part-of-speech (POS) task using the PennTreeBank corpus. They use sections 0-18 of the Wall Street Journal for training, and sections 22-24 for testing.</p>\n</li>\n</ul>\n"},"text":"%md\n\n###HW11.6 [OPTIONAL: consider doing this in a group] Evalution of perceptron algorihtms on PennTreeBank POS corpus\n- Reproduce the experiments reported in the following paper:\n    - HadoopPerceptron: a Toolkit for Distributed Perceptron Training and Prediction with MapReduce Andrea Gesmundo and  Nadi Tomeh\n    - http://www.aclweb.org/anthology/E12-2020 \n\n- These experiments focus on the prediction accuracy on a part-of-speech (POS) task using the PennTreeBank corpus. They use sections 0-18 of the Wall Street Journal for training, and sections 22-24 for testing."},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459465314181_1108379990","id":"20160331-190154_2117884839","dateCreated":"Mar 31, 2016 7:01:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:207","dateUpdated":"Mar 31, 2016 7:08:02 PM","dateFinished":"Mar 31, 2016 7:07:59 PM","dateStarted":"Mar 31, 2016 7:07:59 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>HW11.7 [OPTIONAL: consider doing this in a group]  Kernal Adatron</h3>\n<ul>\n<li>Implement the Kernal Adatron in Spark (contact Jimi for details)</li>\n</ul>\n"},"text":"%md\n\n###HW11.7 [OPTIONAL: consider doing this in a group]  Kernal Adatron \n- Implement the Kernal Adatron in Spark (contact Jimi for details)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459465354459_-1686774797","id":"20160331-190234_1138703312","dateCreated":"Mar 31, 2016 7:02:34 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:227"}],"name":"HW11","id":"2BECF7RAS","angularObjects":{"2BEGWVP62":[],"2BGMJY3G4":[],"2BFXQSFAZ":[],"2BJ5WMTYQ":[],"2BG6MJGUE":[],"2BH37RB8E":[],"2BGAJ5FMN":[],"2BF2GH4C2":[],"2BF87DX4C":[],"2BEPN2DG4":[],"2BJ328RQ6":[],"2BHQ4567J":[],"2BF459WUH":[],"2BFP2HNHT":[]},"config":{"looknfeel":"default"},"info":{}}