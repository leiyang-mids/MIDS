{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#5\n",
    "####[Lei Yang](mailto:leiyang@berkeley.edu) | [Michael Kennedy](mailto:mkennedy@ischool.berkeley.edu) | [Natarajan Krishnaswami](mailto:natarajan@krishnaswami.org)\n",
    "####Due: 2016-02-18, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.0.* Q&A\n",
    "\n",
    "####What is a data warehouse? What is a Star schema? When is it used?\n",
    "\n",
    "\n",
    "###*HW 5.1* Q&A\n",
    "####In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "\n",
    "####In what form does ML consume data?\n",
    "\n",
    "####Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.2*\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.):\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "1. Left joining Table Left with Table Right\n",
    "2. Right joining Table Left with Table Right\n",
    "3. Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.3* For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "#### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "#### 2: A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "For each HW 5.3 -5.5 Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations. Then show the results you get with you system.\n",
    "Final show your results on the Google n-grams dataset\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "- \t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "- e.g. 'A Case Study of Limited\t55\t55\t43'\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - The longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_Longest5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_Longest5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Longest5Gram(MRJob):\n",
    "\n",
    "    # stream through lines, yield char count\n",
    "    def mapper(self, _, line):\n",
    "        # get page id\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        yield n_gram, len(n_gram)\n",
    "        \n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.length = 0\n",
    "        self.longest = None\n",
    "\n",
    "    def reducer(self, n_gram, n_char):\n",
    "        cnt = sum(n_char)\n",
    "        if cnt > self.length:\n",
    "            self.longest = n_gram\n",
    "            self.length = cnt\n",
    "\n",
    "    def reducer_final(self):\n",
    "        yield self.longest, (self.length)\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf = {\n",
    "            'mapreduce.job.maps': '30',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper\n",
    "                       # NOTE: combiner doesn't work on EMR Hadoop-1.0.3 & AMI-2.4.2, \n",
    "                       # only on AMI-3.11.0 with Hadoop-2.4.0\n",
    "                       ,combiner_init=self.reducer_init \n",
    "                       ,combiner=self.reducer\n",
    "                       ,combiner_final=self.reducer_final\n",
    "                       ,reducer_init=self.reducer_init\n",
    "                       ,reducer=self.reducer\n",
    "                       ,reducer_final=self.reducer_final\n",
    "                       ,jobconf=jobconf\n",
    "                       )                \n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Longest5Gram.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob\n",
    "- with EMR running for s3://filtered-5grams/, the longest 5-gram is:\n",
    "- \"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\t\n",
    "- number of character: 159\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Longest5Gram.leiyang.20160214.215417.539788\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Longest5Gram.leiyang.20160214.215417.539788/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_3_Longest5Gram.leiyang.20160214.215417.539788/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar8609884381448338019/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob1002621193370530292.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_3_Longest5Gram.leiyang.20160214.215417.539788/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Longest5Gram.leiyang.20160214.215417.539788\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_3_Longest5Gram.leiyang.20160214.215417.539788 from HDFS\n",
      "\"Hydroxytryptamine stimulates inositol phosphate production\"\t58\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_3_Longest5Gram.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop #####\n",
    "#!python HW5_3_Longest5Gram.py ngram_test.txt -r hadoop > debug\n",
    "!python HW5_3_Longest5Gram.py googlebooks-eng-all-5gram-20090715-0-filtered.txt -r hadoop > debug\n",
    "!cat debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_Longest5Gram.py s3://filtered-5grams/ -r emr --output-dir 's3://us-west-2/w261.data/HW5/'  --no-output\n",
    "\n",
    "#!python HW5_3_Longest5Gram.py googlebooks-eng-all-5gram-20090715-0-filtered.txt -r emr --output-dir 's3://us-west-2/w261.data/HW5/'  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - Top 10 most frequent words (count), i.e., unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_Top10Words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_Top10Words.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Top10Words(MRJob):\n",
    "\n",
    "    # stream through lines, yield word count\n",
    "    def mapper(self, _, line):\n",
    "        # get page id\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        cnt = int(cnt)\n",
    "        for w in n_gram.lower().split(' '):\n",
    "            yield w, cnt\n",
    "\n",
    "    # sum word counts, use as combiner too\n",
    "    def reducer(self, word, count):\n",
    "        yield word, sum(count)\n",
    "        \n",
    "    # job to sort the results ###########################\n",
    "    def mapper_sort1(self, word, count):\n",
    "        yield (word, count), None\n",
    "        \n",
    "    def reducer_sort_init1(self):\n",
    "        self.top = 20\n",
    "        self.n = 0\n",
    "        \n",
    "    def reducer_sort1(self, results, dummy):        \n",
    "        if self.n < self.top:\n",
    "            self.n += 1\n",
    "            yield results\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf1 = {  #key value pairs \n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '3',\n",
    "        }\n",
    "        \n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper                       \n",
    "                       ,combiner=self.reducer                                              \n",
    "                       ,reducer=self.reducer                       \n",
    "                       ,jobconf=jobconf1\n",
    "                       )\n",
    "                ,MRStep(mapper=self.mapper_sort1\n",
    "                       ,reducer_init=self.reducer_sort_init1\n",
    "                       ,reducer=self.reducer_sort1\n",
    "                       ,jobconf=jobconf2\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Top10Words.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob\n",
    "- with EMR running for s3://filtered-5grams/, the top 10 words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Top10Words.leiyang.20160215.035412.696451\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Top10Words.leiyang.20160215.035412.696451/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_3_Top10Words.leiyang.20160215.035412.696451/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar5911997957981003498/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3883279646616878019.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6314599712663710894/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3160253093713635552.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_3_Top10Words.leiyang.20160215.035412.696451/output\n",
      "\"a\"\t107417\n",
      "\"of\"\t77055\n",
      "\"the\"\t66314\n",
      "\"history\"\t25820\n",
      "\"united\"\t24965\n",
      "\"after\"\t9673\n",
      "\"in\"\t9511\n",
      "\"clear\"\t9093\n",
      "\"understanding\"\t8527\n",
      "\"to\"\t8362\n",
      "\"and\"\t6799\n",
      "\"is\"\t6020\n",
      "\"for\"\t5121\n",
      "\"study\"\t4721\n",
      "\"was\"\t4072\n",
      "\"that\"\t3937\n",
      "\"on\"\t2588\n",
      "\"according\"\t2116\n",
      "\"this\"\t2088\n",
      "\"about\"\t2050\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Top10Words.leiyang.20160215.035412.696451\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_3_Top10Words.leiyang.20160215.035412.696451 from HDFS\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_3_Top10Words.py ngram_test.txt > debug2\n",
    "\n",
    "##### run it locally on hadoop with a sample file #####\n",
    "#!python HW5_3_Top10Words.py ngram_test.txt -r hadoop > debug38\n",
    "!python HW5_3_Top10Words.py ngram_test.txt -r hadoop\n",
    "#!cat debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_Top10Words.py s3://filtered-5grams/ -r emr --cleanup=NONE  --no-output\n",
    "#!python HW5_3_Top10Words.py ngram_test.txt -r emr --cleanup=NONE  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Hint: save to PART-000\\* and take the head -n 1000\n",
    "- every word appears at least once per page, so the least densely appearing ratio is $1$, and there are a bunch of those\n",
    "- we do reverse sorting below for $\\frac{count}{pages\\_count}$, and show the top 200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_MostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_MostLeastDenseWords.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostLeastDenseWords(MRJob):\n",
    "\n",
    "    # stream through lines, yield word count\n",
    "    def mapper(self, _, line):\n",
    "        # get page id\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        cnt, p_cnt = int(cnt), int(p_cnt)\n",
    "        for w in n_gram.lower().split(' '):\n",
    "            yield w, (cnt, p_cnt)\n",
    "            \n",
    "    # combiner\n",
    "    def combiner(self, word, counts):\n",
    "        cnt = p_cnt = 0\n",
    "        for c in counts:\n",
    "            cnt += c[0]\n",
    "            p_cnt += c[1]\n",
    "        yield word, (cnt, p_cnt)\n",
    "   \n",
    "    # sum word counts, use as combiner too\n",
    "    def reducer(self, word, counts):\n",
    "        cnt = p_cnt = 0\n",
    "        for c in counts:\n",
    "            cnt += c[0]\n",
    "            p_cnt += c[1]\n",
    "        yield word, 1.0*cnt/p_cnt\n",
    "        \n",
    "    # job to sort the results ###########################\n",
    "    def mapper_sort1(self, word, ratio):\n",
    "        yield (word, ratio), None\n",
    "        \n",
    "    def reducer_sort_init1(self):\n",
    "        self.top = 200\n",
    "        self.n = 0\n",
    "        \n",
    "    def reducer_sort1(self, results, dummy):        \n",
    "        if self.n < self.top:\n",
    "            self.n += 1\n",
    "            yield results\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf1 = {  #key value pairs \n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '3',\n",
    "        }\n",
    "        \n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper                       \n",
    "                       ,combiner=self.combiner\n",
    "                       ,reducer=self.reducer                       \n",
    "                       ,jobconf=jobconf1\n",
    "                       )\n",
    "                ,MRStep(mapper=self.mapper_sort1\n",
    "                       ,reducer_init=self.reducer_sort_init1\n",
    "                       ,reducer=self.reducer_sort1\n",
    "                       ,jobconf=jobconf2\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostLeastDenseWords.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6935029832103145630/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob9152720224773007202.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar2229929535559806893/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob7809952110008506371.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658 from HDFS\n",
      "\"front\"\t1.6140350877192982\n",
      "\"banker\"\t1.3333333333333333\n",
      "\"acceptable\"\t1.279503105590062\n",
      "\"goods\"\t1.279503105590062\n",
      "\"identification\"\t1.279503105590062\n",
      "\"photometer\"\t1.2636363636363637\n",
      "\"philosophical\"\t1.2592592592592593\n",
      "\"political\"\t1.2592592592592593\n",
      "\"chief\"\t1.2487562189054726\n",
      "\"secretary\"\t1.2487562189054726\n",
      "\"council\"\t1.2261306532663316\n",
      "\"acting\"\t1.1984126984126984\n",
      "\"need\"\t1.1944444444444444\n",
      "\"able\"\t1.1931818181818181\n",
      "\"read\"\t1.1931818181818181\n",
      "\"write\"\t1.1931818181818181\n",
      "\"dirty\"\t1.1688311688311688\n",
      "\"funny\"\t1.1688311688311688\n",
      "\"projected\"\t1.1614906832298137\n",
      "\"representative\"\t1.1571428571428573\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_3_MostLeastDenseWords.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop with a sample file #####\n",
    "!python HW5_3_MostLeastDenseWords.py ngram_test.txt -r hadoop > debug\n",
    "!head -20 debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_MostLeastDenseWords.py s3://filtered-5grams/ -r emr --cleanup=NONE  --no-output\n",
    "#!python HW5_3_MostLeastDenseWords.py ngram_test.txt -r emr --cleanup=NONE  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. \n",
    "- Hint: save to PART-000* and take the head -n 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_Distribution5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_Distribution5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Distribution5Gram(MRJob):\n",
    "\n",
    "    # stream through lines, yield word count\n",
    "    def mapper(self, _, line):\n",
    "        # get counts\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        cnt = int(cnt)\n",
    "        yield '*', cnt\n",
    "        yield n_gram, cnt\n",
    "            \n",
    "    # combiner\n",
    "    def combiner(self, n_gram, count):        \n",
    "        yield n_gram, sum(count)\n",
    "        \n",
    "    # reducer init\n",
    "    def reducer_init(self):\n",
    "        self.total = 0\n",
    "        \n",
    "    def reducer(self, n_gram, count):\n",
    "        if n_gram == '*':\n",
    "            self.total = sum(count)\n",
    "        else:\n",
    "            yield 'n', 1.0*sum(count)/self.total\n",
    "        \n",
    "    # job to sort the results ###########################\n",
    "    def mapper_sort1(self, word, ratio):\n",
    "        yield (word, ratio), None\n",
    "        \n",
    "        \n",
    "    def reducer_sort1(self, results, dummy):               \n",
    "        yield results\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf1 = {  #key value pairs \n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "        }\n",
    "        \n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper                       \n",
    "                       ,combiner=self.combiner\n",
    "                       ,reducer_init=self.reducer_init\n",
    "                       ,reducer=self.reducer           \n",
    "                       #,jobconf=jobconf1\n",
    "                       )\n",
    "                ,MRStep(mapper=self.mapper_sort1                \n",
    "                       ,reducer=self.reducer_sort1\n",
    "                       #,jobconf=jobconf2\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Distribution5Gram.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-0-mapper-sorted\n",
      "> sort /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-0-mapper_part-00000\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-1-mapper-sorted\n",
      "> sort /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-1-mapper_part-00000\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/step-1-reducer_part-00000 -> /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/output/part-00000\n",
      "Streaming final output from /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160215.045611.259460\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "!python HW5_3_Distribution5Gram.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop with a sample file #####\n",
    "#!python HW5_3_Distribution5Gram.py ngram_test.txt -r hadoop > debug\n",
    "#!head -20 debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_Distribution5Gram.py s3://filtered-5grams/ -r emr --cleanup=NONE  --no-output\n",
    "#!python HW5_3_Distribution5Gram.py ngram_test.txt -r emr --cleanup=NONE  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - *OPTIONAL* Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Write SYSTEMS_TEST_DATASET.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SYSTEMS_TEST_DATASET.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile SYSTEMS_TEST_DATASET.txt\n",
    "DocA {'X':20, 'Y':30, 'Z':5}\n",
    "DocB {'X':100, 'Y':20}\n",
    "DocC {'M':5, 'N':20, 'Z':5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Paper-and-Pencil Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.4 * (over 2Gig of Data)\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "1. Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "2. Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "**Design notes for (1)**\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "**Design notes for (2)**\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "- ...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.4 - MRJob to get the top 10,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.4 - MrJob for Jaccard/Cosine Similarity\n",
    "- **Jaccard Similarity:** \n",
    "$$S_{A,B}=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|}$$\n",
    "\n",
    "\n",
    "- **Job 0** - for n-gram input, build the pseudo-doc (co-ocurrence matrix/word stripes) first\n",
    "  - **mapper**: emit *((w1,w2) ~ count)* for pair count\n",
    "  - **reducer**: obtain co-ocurrence matrix\n",
    "- **Job 1** - build inverted index for n-gram\n",
    "  - **mapper**: emit *((term,n_gram_id/doc_id) ~ 1)* for each word in the n-gram. \n",
    "   - **Note**: since the test file and n-gram file have different format, we will create two mapper to data from each, but both emits should have identical format \n",
    "  - **combiner**: local aggregation of co-ocurrence\n",
    "  - **partitioner**: -k1,1 -k2,2 apply secondary sorting\n",
    "  - **reducer**: summary to build inverted index\n",
    "- **NOTE**: for co-ocurrence matrix, the inverted indexing is its transpose, and because co-ocurrence is symmetric, inverted index is identical. Thus we can skip *Job 1* for synonym dection.\n",
    "- **Job 2** - obtain pairwise similarity for words from n-gram\n",
    "  - **mapper**: \n",
    "   - emit ((*,term) ~ 1) for each term in the inverted index, using order inversion to calculate $|A|$ and $|B|$\n",
    "   - emit ((A,B) ~ payload) for each *sorted* pair in the inverted index, to get all components for $|A\\cap B|$\n",
    "  - **combiner**: local aggregation\n",
    "  - **single reducer**: calcualte $S_{A,B}$, **or** use customized partitioner to have (*,word) available for all reducers.\n",
    "   - **Note**: since $S_{A,B}$ requires both $|A|$ and $|B|$, which are also needed in evaluating similarity for any other pairs that have them, it's impossible to have both norm in a realtime fashion, thus we cache them in the reducer. Hopefully this is not entirely unrealistic given that the amount of single word is relatively small.\n",
    "- **Job 3** - get top n similarities for synonym\n",
    "  - **mapper**: emit *($S_{A,B}$ ~ (A,B))*\n",
    "  - **partitioner**: -k1,1nr\n",
    "  - **reducer**: print out first n paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_4_Jaccard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_4_Jaccard.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from math import sqrt\n",
    " \n",
    "class Jaccard(MRJob):\n",
    "    \n",
    "    #################################  job 0 - create co-occurrence matrix ##############################\n",
    "    \n",
    "    # job 0 mapper for SYSTEMS_TEST_DATASET\n",
    "    def j0_mapper_read_test(self, _, line):        \n",
    "        # time of mapper being called\n",
    "        self.increment_counter('HW5_4', 'mapper_test', 1)\n",
    "        # parse line, get doc id and terms\n",
    "        word, strip = line.strip().split(' ', 1)\n",
    "        cmd = 'strip = ' + strip\n",
    "        exec cmd\n",
    "        # emit co-occurrence matrix\n",
    "        yield word, strip\n",
    "            \n",
    "    # job 0 mapper for 5-gram: build pseudo-document (co-ocurrence matrix) & emit inverted index\n",
    "    def j0_mapper_read_5gram(self, _, line):\n",
    "        # parse line, get words and counts\n",
    "        grams, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        grams = grams.lower().split(' ')\n",
    "        n_gram = len(grams)\n",
    "        # emit co-ocurrence for each pair (MUST include all pairs to have correct inverted index)\n",
    "        for w1, w2 in [[grams[i], grams[j]] for i in range(n_gram) for j in range(n_gram)]:\n",
    "            yield (w1, w2), int(cnt)\n",
    "                        \n",
    "    # job 0 combiner - local aggregation of co-occurrence\n",
    "    def j0_combiner(self, pair, count):\n",
    "        yield (pair), sum(count)\n",
    "    \n",
    "    # job 0 reducer_init()\n",
    "    def j0_reducer_init(self):\n",
    "        self.current_term = None\n",
    "        self.current_strip = {}\n",
    "                        \n",
    "    # job 0 reducer\n",
    "    def j0_reducer(self, key, count):        \n",
    "        w1, w2 = key[0], key[1]     \n",
    "        if w1 == w2:\n",
    "            return\n",
    "        if self.current_term == w1:\n",
    "            # accumulate co-occurent words            \n",
    "            self.current_strip[w2] = sum(count)\n",
    "        else:\n",
    "            # yield previous word and stripe\n",
    "            if self.current_term:\n",
    "                yield self.current_term, self.current_strip\n",
    "            # reset new term\n",
    "            self.current_term = w1\n",
    "            self.current_strip = {w2:sum(count)}\n",
    "            \n",
    "    # job 0 reducer final - emit last word strip\n",
    "    def j0_reducer_final(self):\n",
    "        if self.current_term:\n",
    "            yield self.current_term, self.current_strip\n",
    "    \n",
    "    #################################  job 1 - create inverted indexing ##############################\n",
    "    \n",
    "    # job 1 mapper - build inverted index\n",
    "    def j1_mapper_jaccard(self, w1, stripe):\n",
    "        # here stripe is a dictionary \n",
    "        norm = sqrt(len(stripe))\n",
    "        for w2 in stripe:                        \n",
    "            yield (w2, w1), 1/norm\n",
    "            \n",
    "    def j1_mapper_cosine(self, w1, stripe):\n",
    "        # here stripe is a dictionary    \n",
    "        norm = sqrt(sum(pow(x,2) for x in stripe.values()))\n",
    "        for w2 in stripe:            \n",
    "            yield (w2, w1), stripe[w2]/norm\n",
    "    \n",
    "    # job 1 reducer_init()\n",
    "    def j1_reducer_init(self):\n",
    "        self.current_term = None\n",
    "        self.current_strip = {}\n",
    "                                \n",
    "    # job 1 reducer\n",
    "    def j1_reducer(self, pair, count):        \n",
    "        w2, w1 = pair[0], pair[1]\n",
    "        if self.current_term == w2:\n",
    "            # accumulate postings\n",
    "            self.current_strip[w1] = sum(count)\n",
    "        else:\n",
    "            # yield previous term and stripe\n",
    "            if self.current_term:\n",
    "                yield self.current_term, self.current_strip\n",
    "            # reset new term\n",
    "            self.current_term = w2\n",
    "            self.current_strip = {w1:sum(count)}\n",
    "                    \n",
    "    # job 1 reducer final - emit last index strip\n",
    "    def j1_reducer_final(self):\n",
    "        if self.current_term:\n",
    "            yield self.current_term, self.current_strip\n",
    "            \n",
    "    #################################  job 2 - evaluate similarity between words ##############################\n",
    "            \n",
    "    # job 2 mapper - emit pair-wise similarity from strips\n",
    "    def j2_mapper(self, term, postings):\n",
    "        # get all postings from generator\n",
    "        posts = postings.keys() # [p for p in postings]\n",
    "        posts.sort()\n",
    "        size = len(posts)        \n",
    "        # emit pairs on sorted stripe, so we only evaluate half of the symmetric relation\n",
    "        for w1, w2 in [[posts[i], posts[j]] for i in range(size) for j in range(i+1, size)]:\n",
    "            yield (w1, w2), postings[w1]*postings[w2]\n",
    "            \n",
    "    # job 2 reducer - get pair similarity\n",
    "    def j2_reducer(self, pair, prod):\n",
    "        # calculate similarity\n",
    "        yield (pair), sum(prod)\n",
    "            \n",
    "    #################################  job 3 - rank pairwise similarities ##############################\n",
    "                   \n",
    "    # job 3 mapper - for secondary sort\n",
    "    def j3_mapper(self, pair, sim):\n",
    "        yield (sim, pair), None\n",
    "        \n",
    "    # job 3 reducer_init\n",
    "    def j3_reducer_init(self):\n",
    "        self.top = 300\n",
    "        self.n = 0\n",
    "    \n",
    "    # job 3 reducer - show top 100 pairs\n",
    "    def j3_reducer(self, result, _):\n",
    "        self.n += 1\n",
    "        if self.n <= self.top:\n",
    "            yield result\n",
    "            \n",
    "    #################################  mrjob definition ##############################\n",
    "            \n",
    "    # MapReduce steps\n",
    "    def steps(self):\n",
    "        jobconf0 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            #'mapreduce.partition.keycomparator.options': '-k1,1r -k2,2r', # no need to sort            \n",
    "            'mapreduce.partition.keypartitioner.options': '-k1,1',            \n",
    "            'mapreduce.job.maps': '5',\n",
    "            'mapreduce.job.reduces': '1', # on local cluster partitioner setting doesn't work, neither on EMR!!!\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ',',\n",
    "            'stream.map.output.field.separator': '\\t',\n",
    "        }\n",
    "        jobconf1 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            #'mapreduce.partition.keycomparator.options': '-k1,1r -k2,2r', # no need to sort            \n",
    "            'mapreduce.partition.keypartitioner.options': '-k1,1',            \n",
    "            'mapreduce.job.maps': '5',\n",
    "            'mapreduce.job.reduces': '1', # on local cluster partitioner setting doesn't work \n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ',',\n",
    "            'stream.map.output.field.separator': '\\t',\n",
    "        }\n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            #'mapreduce.partition.keycomparator.options': '-k1,1r -k2,2r', # -k2,2r',\n",
    "            'mapreduce.partition.keypartitioner.options': '-k1,2',\n",
    "            'mapreduce.job.maps': '10',\n",
    "            'mapreduce.job.reduces': '10',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ',',\n",
    "            'stream.map.output.field.separator': '\\t',\n",
    "        }\n",
    "        jobconf3 = {  #key value pairs \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ',',\n",
    "            'stream.map.output.field.separator': '\\t',\n",
    "        }\n",
    "        \n",
    "        # NOTE: DO NOT use jobconf when running with Python locally\n",
    "        return [\n",
    "                ######## job 0: get co-ocurrence matrix ########\n",
    "                ### for SYSTEMS_TEST_DATASET.txt ###\n",
    "                #MRStep(mapper=self.j0_mapper_read_test)\n",
    "                ### for n-gram file ###\n",
    "                MRStep(mapper=self.j0_mapper_read_5gram\n",
    "                        , combiner=self.j0_combiner, reducer_init=self.j0_reducer_init\n",
    "                        , reducer=self.j0_reducer, reducer_final=self.j0_reducer_final\n",
    "                        , jobconf=jobconf0\n",
    "                      )\n",
    "                ######## job 1: get inverted indexing ########\n",
    "                #,MRStep(mapper=self.j1_mapper_cosine                \n",
    "                #        , reducer_init=self.j1_reducer_init\n",
    "                #        , reducer=self.j1_reducer, reducer_final=self.j1_reducer_final\n",
    "                #        , jobconf=jobconf1\n",
    "                #      )\n",
    "                ######## job 2: calculate pair similarity between words ########\n",
    "                #,MRStep(mapper=self.j2_mapper, combiner=self.j2_reducer\n",
    "                #        , reducer=self.j2_reducer\n",
    "                #        , jobconf=jobconf2\n",
    "                #      )\n",
    "                ######## job 3: sort similarities ########\n",
    "                #,MRStep(mapper=self.j3_mapper, reducer_init=self.j3_reducer_init\n",
    "                #        , reducer=self.j3_reducer\n",
    "                #        , jobconf=jobconf3\n",
    "                #       )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Jaccard.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute the Jaccard Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Jaccard.leiyang.20160214.191706.874149\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Jaccard.leiyang.20160214.191706.874149/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_4_Jaccard.leiyang.20160214.191706.874149/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6873049612776553931/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6172447378409335304.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_4_Jaccard.leiyang.20160214.191706.874149/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Jaccard.leiyang.20160214.191706.874149\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_4_Jaccard.leiyang.20160214.191706.874149 from HDFS\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_4_Jaccard.py SYSTEMS_TEST_DATASET.txt\n",
    "#!python HW5_4_Jaccard.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop #####\n",
    "!python HW5_4_Jaccard.py ngram_test.txt -r hadoop > debug\n",
    "\n",
    "##### run it on emr #####\n",
    "# s3 folder: s3://aws-logs-149687825236-us-east-1/elasticmapreduce/\n",
    "\n",
    "#!python HW5_4_Jaccard.py googlebooks-eng-all-5gram-20090715-0-filtered.txt -r emr > debug\n",
    "\n",
    "#!python HW5_4_Jaccard.py s3://filtered-5grams/ -r emr --output-dir=s3://w261.data/HW5/  --no-output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.5*\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n",
    "\n",
    "\n",
    "###*HW 5.5.1 (optional)*\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    ">> from nltk.corpus import stopwords\n",
    ">>> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "###*HW 5.6 (optional)*\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams.\n",
    "\n",
    "\n",
    "###*Hw 5.7 (optional)*\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW4.3:* Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).\n",
    "\n",
    "###MrJob Steps:\n",
    "- mapper 1: emit V~count pairs\n",
    "- reducer 1: count visit times n for each page V\n",
    "- reducer 2: get 5 most frequently visited pages, with n sorted reversely on numeric order by the partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_3.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class FreqVisitPage(MRJob):\n",
    "    #n_freq, i = 5, 0\n",
    "        \n",
    "    def mapper_count(self, dummy, line): \n",
    "        self.increment_counter('HW4_3','page_map',1)\n",
    "        # get page id\n",
    "        pID = line.strip().split(',')[1]\n",
    "        yield pID.strip(), 1\n",
    "        \n",
    "    def reducer_count(self, page, count):\n",
    "        self.increment_counter('HW4_3','page_count',1)\n",
    "        yield page, sum(count)\n",
    "        \n",
    "    def mapper_sort(self, page, count):        \n",
    "        yield (page, count), None\n",
    "       \n",
    "    def reducer_init(self):\n",
    "        self.i = 0\n",
    "        self.n_freq = 5\n",
    "    \n",
    "    def reducer_sort(self, key, _):\n",
    "        self.increment_counter('HW4_3','page_sort',1)\n",
    "        if True: #self.i < self.n_freq:\n",
    "            self.i += 1\n",
    "            yield key        \n",
    "        \n",
    "    def steps(self):             \n",
    "        sort_jobconf = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',\n",
    "        }\n",
    "        \n",
    "        count_jobconf = {\n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '2',\n",
    "        }\n",
    "        \n",
    "        return [MRStep(mapper=self.mapper_count, reducer=self.reducer_count, jobconf=count_jobconf)\n",
    "                ,MRStep(mapper=self.mapper_sort, reducer_init=self.reducer_init,\n",
    "                        reducer=self.reducer_sort, jobconf=sort_jobconf)\n",
    "               ]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FreqVisitPage.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_3.leiyang.20160215.014812.928158\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_3.leiyang.20160215.014812.928158/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW4_3.leiyang.20160215.014812.928158/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar636095742612592001/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob5314780447831769713.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6937957205234406169/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob7638823193819438911.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW4_3.leiyang.20160215.014812.928158/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_3.leiyang.20160215.014812.928158\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW4_3.leiyang.20160215.014812.928158 from HDFS\n"
     ]
    }
   ],
   "source": [
    "### running the job locally\n",
    "#!python HW4_3.py HW4_2_results > HW4_3_results\n",
    "\n",
    "### running the job on hadoop\n",
    "!python HW4_3.py HW4_2_results -r hadoop > debug\n",
    "\n",
    "### results\n",
    "#!cat HW4_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW4.4:* Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.\n",
    "\n",
    "**Dev Notes:**\n",
    "- mapper 1: repeat HW4.2 mapper to conver log format first\n",
    "- mapper 2: emit dummy key for webpage item, such that it alway comes immediately before website/visitor lines\n",
    " - for rows start with A, key is \n",
    " - partitioner: sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_4.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class FreqVisitor(MRJob):\n",
    "    \n",
    "    # member variables: visitor ID and url\n",
    "    url = visitorID = None\n",
    "    current_page = None\n",
    "    current_max = 0\n",
    "    \n",
    "    # 1. mapper\n",
    "    def convert_mapper(self, _, line):        \n",
    "        # time of mapper being called\n",
    "        self.increment_counter('HW4_2', 'lines', 1)\n",
    "        # only emit lines start with C and V\n",
    "        line = line.strip()\n",
    "        if line[0] not in ['C', 'V', 'A']:\n",
    "            return\n",
    "        temp = line.split(',')\n",
    "        # process A, C, and V lines\n",
    "        if line[0] == 'C':            \n",
    "            # get the latest visitor ID\n",
    "            self.visitorID = temp[2]  \n",
    "        elif line[0] == 'A':\n",
    "            # emit V_pageID_*_url as key, dummy 1 as value\n",
    "            yield 'V_%s_*_%s' %(temp[1], temp[4].strip('\"')), 1\n",
    "        else:\n",
    "            # emit V_pageID_C_visitorID as key, 1 as value\n",
    "            yield 'V_%s_C_%s' %(temp[1], self.visitorID), 1\n",
    "     \n",
    "    # 2. reducer to get count for each visitor on each page\n",
    "    def count_reducer(self, key, value):     \n",
    "        temp = key.strip().split('_')\n",
    "        # save webpage url for the following visisting records\n",
    "        if temp[2] == '*':\n",
    "            self.url = temp[3]\n",
    "        else:\n",
    "            yield key+'_'+self.url, sum(value)\n",
    "            \n",
    "    # 3. mapper for sorting: partition by page id, secondary sorting/ranking by count\n",
    "    def rank_mapper(self, key, count):   \n",
    "        v, pID, c, cID, url = key.strip().split('_')\n",
    "        yield 'V_%s' %pID, (count, 'C_%s - URL: %s' %(cID, url))\n",
    "    \n",
    "    # 4. reducer get max vistor of each page\n",
    "    # NOTE: this implementation doesn't show all ties, just one of the record with the biggest count\n",
    "    def rank_reducer(self, key, value):\n",
    "        # most frequent vistor of the webpage\n",
    "        yield key, max(value)\n",
    "           \n",
    "    \n",
    "    # 0. MapReduce steps\n",
    "    def steps(self):\n",
    "        count_jobconf = {  #key value pairs                        \n",
    "            'mapreduce.job.maps': '1',\n",
    "            'mapreduce.job.reduces': '1', # must only use 1 reducer to have the proper order\n",
    "        }\n",
    "        \n",
    "        rank_jobconf = {\n",
    "            #'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            #'mapreduce.partition.keycomparator.options': '-k1,1r', # reverse order page ID '-k2,2nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            #'stream.num.map.output.key.fields': '2',\n",
    "            #'mapreduce.map.output.key.field.separator': '',\n",
    "            #'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "        return [MRStep(mapper=self.convert_mapper, reducer=self.count_reducer, jobconf=count_jobconf)\n",
    "                ,MRStep(mapper=self.rank_mapper, reducer=self.rank_reducer, jobconf=rank_jobconf)\n",
    "               ]\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    FreqVisitor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_4.leiyang.20160207.221613.945000\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_4.leiyang.20160207.221613.945000/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW4_4.leiyang.20160207.221613.945000/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar8262919340003780482/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob8665858738147888764.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar4820819124527597538/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6003131137893457708.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW4_4.leiyang.20160207.221613.945000/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_4.leiyang.20160207.221613.945000\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW4_4.leiyang.20160207.221613.945000 from HDFS\n",
      "\"V_1000\"\t[1, \"C_42679 - URL: /regwiz\"]\n",
      "\"V_1001\"\t[1, \"C_42710 - URL: /support\"]\n",
      "\"V_1002\"\t[1, \"C_42592 - URL: /athome\"]\n",
      "\"V_1003\"\t[1, \"C_42709 - URL: /kb\"]\n",
      "\"V_1004\"\t[1, \"C_42707 - URL: /search\"]\n",
      "\"V_1005\"\t[1, \"C_42698 - URL: /norge\"]\n",
      "\"V_1006\"\t[1, \"C_42612 - URL: /misc\"]\n",
      "\"V_1007\"\t[1, \"C_42664 - URL: /ie\"]\n",
      "\"V_1008\"\t[1, \"C_42711 - URL: /msdownload\"]\n",
      "\"V_1009\"\t[1, \"C_42707 - URL: /windows\"]\n",
      "\"V_1010\"\t[1, \"C_42698 - URL: /vbasic\"]\n",
      "\"V_1011\"\t[1, \"C_42557 - URL: /officedev\"]\n",
      "\"V_1012\"\t[1, \"C_42650 - URL: /outlookdev\"]\n",
      "\"V_1013\"\t[1, \"C_42698 - URL: /vbasicsupport\"]\n",
      "\"V_1014\"\t[1, \"C_42698 - URL: /officefreestuff\"]\n",
      "\"V_1015\"\t[1, \"C_42626 - URL: /msexcel\"]\n",
      "\"V_1016\"\t[1, \"C_42626 - URL: /excel\"]\n",
      "\"V_1017\"\t[1, \"C_42692 - URL: /products\"]\n",
      "\"V_1018\"\t[1, \"C_42710 - URL: /isapi\"]\n",
      "\"V_1019\"\t[1, \"C_42311 - URL: /mspowerpoint\"]\n",
      "\"V_1020\"\t[1, \"C_42694 - URL: /msdn\"]\n",
      "\"V_1021\"\t[1, \"C_42523 - URL: /visualc\"]\n",
      "\"V_1022\"\t[1, \"C_42574 - URL: /truetype\"]\n",
      "\"V_1023\"\t[1, \"C_42665 - URL: /spain\"]\n",
      "\"V_1024\"\t[1, \"C_42692 - URL: /iis\"]\n",
      "\"V_1025\"\t[1, \"C_42657 - URL: /gallery\"]\n",
      "\"V_1026\"\t[1, \"C_42708 - URL: /sitebuilder\"]\n",
      "\"V_1027\"\t[1, \"C_42708 - URL: /intdev\"]\n",
      "\"V_1028\"\t[1, \"C_42063 - URL: /oledev\"]\n",
      "\"V_1029\"\t[1, \"C_42675 - URL: /clipgallerylive\"]\n",
      "\"V_1030\"\t[1, \"C_42707 - URL: /ntserver\"]\n",
      "\"V_1031\"\t[1, \"C_42667 - URL: /msoffice\"]\n",
      "\"V_1032\"\t[1, \"C_42700 - URL: /games\"]\n",
      "\"V_1033\"\t[1, \"C_41867 - URL: /logostore\"]\n",
      "\"V_1034\"\t[1, \"C_42705 - URL: /ie\"]\n",
      "\"V_1035\"\t[1, \"C_42710 - URL: /windowssupport\"]\n",
      "\"V_1036\"\t[1, \"C_42629 - URL: /organizations\"]\n",
      "\"V_1037\"\t[1, \"C_42650 - URL: /windows95\"]\n",
      "\"V_1038\"\t[1, \"C_42708 - URL: /sbnmember\"]\n",
      "\"V_1039\"\t[1, \"C_42656 - URL: /isp\"]\n",
      "\"V_1040\"\t[1, \"C_42698 - URL: /office\"]\n",
      "\"V_1041\"\t[1, \"C_42708 - URL: /workshop\"]\n",
      "\"V_1042\"\t[1, \"C_42467 - URL: /vstudio\"]\n",
      "\"V_1043\"\t[1, \"C_42626 - URL: /smallbiz\"]\n",
      "\"V_1044\"\t[1, \"C_42673 - URL: /mediadev\"]\n",
      "\"V_1045\"\t[1, \"C_42633 - URL: /netmeeting\"]\n",
      "\"V_1046\"\t[1, \"C_42702 - URL: /iesupport\"]\n",
      "\"V_1048\"\t[1, \"C_42541 - URL: /publisher\"]\n",
      "\"V_1049\"\t[1, \"C_42539 - URL: /supportnet\"]\n",
      "\"V_1050\"\t[1, \"C_42525 - URL: /macoffice\"]\n",
      "\"V_1051\"\t[1, \"C_41980 - URL: /scheduleplus\"]\n",
      "\"V_1052\"\t[1, \"C_42665 - URL: /word\"]\n",
      "\"V_1053\"\t[1, \"C_42688 - URL: /visualj\"]\n",
      "\"V_1054\"\t[1, \"C_42577 - URL: /exchange\"]\n",
      "\"V_1055\"\t[1, \"C_42424 - URL: /kids\"]\n",
      "\"V_1056\"\t[1, \"C_42507 - URL: /sports\"]\n",
      "\"V_1057\"\t[1, \"C_42667 - URL: /powerpoint\"]\n",
      "\"V_1058\"\t[1, \"C_42707 - URL: /referral\"]\n",
      "\"V_1059\"\t[1, \"C_42706 - URL: /sverige\"]\n",
      "\"V_1060\"\t[1, \"C_42659 - URL: /msword\"]\n",
      "\"V_1061\"\t[1, \"C_42576 - URL: /promo\"]\n",
      "\"V_1062\"\t[1, \"C_42331 - URL: /msaccess\"]\n",
      "\"V_1063\"\t[1, \"C_42337 - URL: /intranet\"]\n",
      "\"V_1064\"\t[1, \"C_42704 - URL: /activeplatform\"]\n",
      "\"V_1065\"\t[1, \"C_42641 - URL: /java\"]\n",
      "\"V_1066\"\t[1, \"C_42260 - URL: /musicproducer\"]\n",
      "\"V_1067\"\t[1, \"C_42456 - URL: /frontpage\"]\n",
      "\"V_1068\"\t[1, \"C_42025 - URL: /vbscript\"]\n",
      "\"V_1069\"\t[1, \"C_42613 - URL: /windowsce\"]\n",
      "\"V_1070\"\t[1, \"C_42692 - URL: /activex\"]\n",
      "\"V_1071\"\t[1, \"C_42621 - URL: /automap\"]\n",
      "\"V_1072\"\t[1, \"C_42446 - URL: /vinterdev\"]\n",
      "\"V_1073\"\t[1, \"C_42469 - URL: /taiwan\"]\n",
      "\"V_1074\"\t[1, \"C_42597 - URL: /ntworkstation\"]\n",
      "\"V_1075\"\t[1, \"C_42622 - URL: /jobs\"]\n",
      "\"V_1076\"\t[1, \"C_42701 - URL: /ntwkssupport\"]\n",
      "\"V_1077\"\t[1, \"C_42626 - URL: /msofficesupport\"]\n",
      "\"V_1078\"\t[1, \"C_42643 - URL: /ntserversupport\"]\n",
      "\"V_1079\"\t[1, \"C_42572 - URL: /australia\"]\n",
      "\"V_1080\"\t[1, \"C_42432 - URL: /brasil\"]\n",
      "\"V_1081\"\t[1, \"C_42694 - URL: /accessdev\"]\n",
      "\"V_1082\"\t[1, \"C_42240 - URL: /access\"]\n",
      "\"V_1083\"\t[1, \"C_42448 - URL: /msaccesssupport\"]\n",
      "\"V_1084\"\t[1, \"C_42516 - URL: /uk\"]\n",
      "\"V_1085\"\t[1, \"C_42577 - URL: /exchangesupport\"]\n",
      "\"V_1086\"\t[1, \"C_40233 - URL: /oem\"]\n",
      "\"V_1087\"\t[1, \"C_42577 - URL: /proxy\"]\n",
      "\"V_1088\"\t[1, \"C_42650 - URL: /outlook\"]\n",
      "\"V_1089\"\t[1, \"C_42598 - URL: /officereference\"]\n",
      "\"V_1090\"\t[1, \"C_42445 - URL: /gamessupport\"]\n",
      "\"V_1091\"\t[1, \"C_42650 - URL: /hwdev\"]\n",
      "\"V_1092\"\t[1, \"C_41717 - URL: /vfoxpro\"]\n",
      "\"V_1093\"\t[1, \"C_42025 - URL: /vba\"]\n",
      "\"V_1094\"\t[1, \"C_39554 - URL: /mshome\"]\n",
      "\"V_1095\"\t[1, \"C_42234 - URL: /catalog\"]\n",
      "\"V_1096\"\t[1, \"C_42566 - URL: /mspress\"]\n",
      "\"V_1097\"\t[1, \"C_42435 - URL: /latam\"]\n",
      "\"V_1098\"\t[1, \"C_42616 - URL: /devonly\"]\n",
      "\"V_1099\"\t[1, \"C_42543 - URL: /cio\"]\n",
      "\"V_1100\"\t[1, \"C_42568 - URL: /education\"]\n",
      "\"V_1101\"\t[1, \"C_41626 - URL: /oledb\"]\n",
      "\"V_1102\"\t[1, \"C_42546 - URL: /homeessentials\"]\n",
      "\"V_1103\"\t[1, \"C_41711 - URL: /works\"]\n",
      "\"V_1104\"\t[1, \"C_41560 - URL: /hk\"]\n",
      "\"V_1105\"\t[1, \"C_42281 - URL: /france\"]\n",
      "\"V_1106\"\t[1, \"C_41001 - URL: /cze\"]\n",
      "\"V_1107\"\t[1, \"C_38331 - URL: /slovakia\"]\n",
      "\"V_1108\"\t[1, \"C_42598 - URL: /teammanager\"]\n",
      "\"V_1109\"\t[1, \"C_42537 - URL: /technet\"]\n",
      "\"V_1110\"\t[1, \"C_41897 - URL: /mastering\"]\n",
      "\"V_1111\"\t[1, \"C_41318 - URL: /ssafe\"]\n",
      "\"V_1112\"\t[1, \"C_42445 - URL: /canada\"]\n",
      "\"V_1113\"\t[1, \"C_42682 - URL: /security\"]\n",
      "\"V_1114\"\t[1, \"C_41916 - URL: /servad\"]\n",
      "\"V_1115\"\t[1, \"C_36277 - URL: /hun\"]\n",
      "\"V_1116\"\t[1, \"C_40678 - URL: /switzerland\"]\n",
      "\"V_1117\"\t[1, \"C_41101 - URL: /sidewinder\"]\n",
      "\"V_1118\"\t[1, \"C_42598 - URL: /sql\"]\n",
      "\"V_1119\"\t[1, \"C_42453 - URL: /corpinfo\"]\n",
      "\"V_1120\"\t[1, \"C_10241 - URL: /switch\"]\n",
      "\"V_1121\"\t[1, \"C_41556 - URL: /magazine\"]\n",
      "\"V_1122\"\t[1, \"C_41995 - URL: /mindshare\"]\n",
      "\"V_1123\"\t[1, \"C_42708 - URL: /germany\"]\n",
      "\"V_1124\"\t[1, \"C_42667 - URL: /industry\"]\n",
      "\"V_1125\"\t[1, \"C_42237 - URL: /imagecomposer\"]\n",
      "\"V_1126\"\t[1, \"C_42030 - URL: /mediamanager\"]\n",
      "\"V_1127\"\t[1, \"C_42699 - URL: /netshow\"]\n",
      "\"V_1128\"\t[1, \"C_10286 - URL: /msf\"]\n",
      "\"V_1129\"\t[1, \"C_27780 - URL: /ado\"]\n",
      "\"V_1130\"\t[1, \"C_42603 - URL: /syspro\"]\n",
      "\"V_1131\"\t[1, \"C_42418 - URL: /moneyzone\"]\n",
      "\"V_1132\"\t[1, \"C_40053 - URL: /msmoneysupport\"]\n",
      "\"V_1133\"\t[1, \"C_41992 - URL: /frontpagesupport\"]\n",
      "\"V_1134\"\t[1, \"C_42479 - URL: /backoffice\"]\n",
      "\"V_1135\"\t[1, \"C_42659 - URL: /mswordsupport\"]\n",
      "\"V_1136\"\t[1, \"C_42364 - URL: /usa\"]\n",
      "\"V_1137\"\t[1, \"C_42704 - URL: /mscorp\"]\n",
      "\"V_1138\"\t[1, \"C_42572 - URL: /mind\"]\n",
      "\"V_1139\"\t[1, \"C_41482 - URL: /k-12\"]\n",
      "\"V_1140\"\t[1, \"C_42678 - URL: /netherlands\"]\n",
      "\"V_1141\"\t[1, \"C_42605 - URL: /europe\"]\n",
      "\"V_1142\"\t[1, \"C_42467 - URL: /southafrica\"]\n",
      "\"V_1143\"\t[1, \"C_42286 - URL: /workshoop\"]\n",
      "\"V_1144\"\t[1, \"C_41640 - URL: /devnews\"]\n",
      "\"V_1145\"\t[1, \"C_39965 - URL: /vfoxprosupport\"]\n",
      "\"V_1146\"\t[1, \"C_42269 - URL: /msp\"]\n",
      "\"V_1147\"\t[1, \"C_42555 - URL: /msft\"]\n",
      "\"V_1148\"\t[1, \"C_42697 - URL: /channel\"]\n",
      "\"V_1149\"\t[1, \"C_39863 - URL: /adc\"]\n",
      "\"V_1150\"\t[1, \"C_42197 - URL: /infoserv\"]\n",
      "\"V_1151\"\t[1, \"C_41774 - URL: /mspowerpointsupport\"]\n",
      "\"V_1152\"\t[1, \"C_42312 - URL: /rus\"]\n",
      "\"V_1153\"\t[1, \"C_39053 - URL: /venezuela\"]\n",
      "\"V_1154\"\t[1, \"C_42467 - URL: /project\"]\n",
      "\"V_1155\"\t[1, \"C_42353 - URL: /sidewalk\"]\n",
      "\"V_1156\"\t[1, \"C_41710 - URL: /powered\"]\n",
      "\"V_1157\"\t[1, \"C_42001 - URL: /win32dev\"]\n",
      "\"V_1158\"\t[1, \"C_42705 - URL: /imedia\"]\n",
      "\"V_1159\"\t[1, \"C_41444 - URL: /transaction\"]\n",
      "\"V_1160\"\t[1, \"C_41764 - URL: /visualcsupport\"]\n",
      "\"V_1161\"\t[1, \"C_42263 - URL: /workssupport\"]\n",
      "\"V_1162\"\t[1, \"C_42285 - URL: /infoservsupport\"]\n",
      "\"V_1163\"\t[1, \"C_40475 - URL: /opentype\"]\n",
      "\"V_1164\"\t[1, \"C_41702 - URL: /smsmgmt\"]\n",
      "\"V_1165\"\t[1, \"C_42007 - URL: /poland\"]\n",
      "\"V_1166\"\t[1, \"C_41248 - URL: /mexico\"]\n",
      "\"V_1167\"\t[1, \"C_42650 - URL: /hwtest\"]\n",
      "\"V_1168\"\t[1, \"C_42646 - URL: /salesinfo\"]\n",
      "\"V_1169\"\t[1, \"C_42642 - URL: /msproject\"]\n",
      "\"V_1170\"\t[1, \"C_41306 - URL: /mail\"]\n",
      "\"V_1171\"\t[1, \"C_42374 - URL: /merchant\"]\n",
      "\"V_1172\"\t[1, \"C_42129 - URL: /belgium\"]\n",
      "\"V_1173\"\t[1, \"C_31767 - URL: /moli\"]\n",
      "\"V_1174\"\t[1, \"C_40040 - URL: /nz\"]\n",
      "\"V_1175\"\t[1, \"C_41737 - URL: /msprojectsupport\"]\n",
      "\"V_1176\"\t[1, \"C_41737 - URL: /jscript\"]\n",
      "\"V_1177\"\t[1, \"C_42437 - URL: /events\"]\n",
      "\"V_1178\"\t[1, \"C_31500 - URL: /msdownload.\"]\n",
      "\"V_1179\"\t[1, \"C_41490 - URL: /colombia\"]\n",
      "\"V_1180\"\t[1, \"C_35728 - URL: /slovenija\"]\n",
      "\"V_1181\"\t[1, \"C_42083 - URL: /kidssupport\"]\n",
      "\"V_1182\"\t[1, \"C_38633 - URL: /fortran\"]\n",
      "\"V_1183\"\t[1, \"C_42613 - URL: /italy\"]\n",
      "\"V_1184\"\t[1, \"C_42626 - URL: /msexcelsupport\"]\n",
      "\"V_1185\"\t[1, \"C_41832 - URL: /sna\"]\n",
      "\"V_1186\"\t[1, \"C_42539 - URL: /college\"]\n",
      "\"V_1187\"\t[1, \"C_42036 - URL: /odbc\"]\n",
      "\"V_1188\"\t[1, \"C_42506 - URL: /korea\"]\n",
      "\"V_1189\"\t[1, \"C_41768 - URL: /internet\"]\n",
      "\"V_1190\"\t[1, \"C_41570 - URL: /repository\"]\n",
      "\"V_1191\"\t[1, \"C_41812 - URL: /management\"]\n",
      "\"V_1192\"\t[1, \"C_38976 - URL: /visualjsupport\"]\n",
      "\"V_1193\"\t[1, \"C_41518 - URL: /offdevsupport\"]\n",
      "\"V_1194\"\t[1, \"C_40708 - URL: /china\"]\n",
      "\"V_1195\"\t[1, \"C_40458 - URL: /portugal\"]\n",
      "\"V_1196\"\t[1, \"C_11431 - URL: /ie40\"]\n",
      "\"V_1197\"\t[1, \"C_42285 - URL: /sqlsupport\"]\n",
      "\"V_1198\"\t[1, \"C_40310 - URL: /pictureit\"]\n",
      "\"V_1199\"\t[1, \"C_11644 - URL: /feedback\"]\n",
      "\"V_1200\"\t[1, \"C_42451 - URL: /benelux\"]\n",
      "\"V_1201\"\t[1, \"C_42203 - URL: /hardware\"]\n",
      "\"V_1202\"\t[1, \"C_41172 - URL: /advtech\"]\n",
      "\"V_1203\"\t[1, \"C_42518 - URL: /danmark\"]\n",
      "\"V_1204\"\t[1, \"C_40792 - URL: /msscheduleplus\"]\n",
      "\"V_1205\"\t[1, \"C_41597 - URL: /hardwaresupport\"]\n",
      "\"V_1206\"\t[1, \"C_42321 - URL: /select\"]\n",
      "\"V_1207\"\t[1, \"C_42008 - URL: /icp\"]\n",
      "\"V_1208\"\t[1, \"C_41548 - URL: /israel\"]\n",
      "\"V_1209\"\t[1, \"C_42513 - URL: /turkey\"]\n",
      "\"V_1210\"\t[1, \"C_31871 - URL: /snasupport\"]\n",
      "\"V_1211\"\t[1, \"C_42344 - URL: /smsmgmtsupport\"]\n",
      "\"V_1212\"\t[1, \"C_42071 - URL: /worldwide\"]\n",
      "\"V_1213\"\t[1, \"C_37985 - URL: /corporate\"]\n",
      "\"V_1214\"\t[1, \"C_35031 - URL: /finserv\"]\n",
      "\"V_1215\"\t[1, \"C_42572 - URL: /developer\"]\n",
      "\"V_1216\"\t[1, \"C_41329 - URL: /vrml\"]\n",
      "\"V_1217\"\t[1, \"C_38711 - URL: /ireland\"]\n",
      "\"V_1218\"\t[1, \"C_42541 - URL: /publishersupport\"]\n",
      "\"V_1219\"\t[1, \"C_20439 - URL: /ads\"]\n",
      "\"V_1220\"\t[1, \"C_41611 - URL: /macofficesupport\"]\n",
      "\"V_1221\"\t[1, \"C_41273 - URL: /mstv\"]\n",
      "\"V_1222\"\t[1, \"C_42103 - URL: /msofc\"]\n",
      "\"V_1223\"\t[1, \"C_42649 - URL: /finland\"]\n",
      "\"V_1224\"\t[1, \"C_40025 - URL: /atec\"]\n",
      "\"V_1225\"\t[1, \"C_42453 - URL: /piracy\"]\n",
      "\"V_1226\"\t[1, \"C_41980 - URL: /msschedplussupport\"]\n",
      "\"V_1227\"\t[1, \"C_42435 - URL: /argentina\"]\n",
      "\"V_1228\"\t[1, \"C_40882 - URL: /vtest\"]\n",
      "\"V_1229\"\t[1, \"C_26913 - URL: /uruguay\"]\n",
      "\"V_1230\"\t[1, \"C_40928 - URL: /mailsupport\"]\n",
      "\"V_1231\"\t[1, \"C_41626 - URL: /win32devsupport\"]\n",
      "\"V_1232\"\t[1, \"C_37637 - URL: /standards\"]\n",
      "\"V_1233\"\t[1, \"C_14363 - URL: /vbscripts\"]\n",
      "\"V_1234\"\t[1, \"C_42626 - URL: /off97cat\"]\n",
      "\"V_1235\"\t[1, \"C_37345 - URL: /onlineeval\"]\n",
      "\"V_1236\"\t[1, \"C_38325 - URL: /globaldev\"]\n",
      "\"V_1237\"\t[1, \"C_32583 - URL: /devdays\"]\n",
      "\"V_1238\"\t[1, \"C_26885 - URL: /exceldev\"]\n",
      "\"V_1239\"\t[1, \"C_38020 - URL: /msconsult\"]\n",
      "\"V_1240\"\t[1, \"C_39891 - URL: /thailand\"]\n",
      "\"V_1241\"\t[1, \"C_34793 - URL: /india\"]\n",
      "\"V_1242\"\t[1, \"C_41937 - URL: /msgarden\"]\n",
      "\"V_1243\"\t[1, \"C_28084 - URL: /usability\"]\n",
      "\"V_1244\"\t[1, \"C_41835 - URL: /devwire\"]\n",
      "\"V_1245\"\t[1, \"C_31934 - URL: /ofc\"]\n",
      "\"V_1246\"\t[1, \"C_42323 - URL: /gamesdev\"]\n",
      "\"V_1247\"\t[1, \"C_30024 - URL: /wineguide\"]\n",
      "\"V_1248\"\t[1, \"C_18347 - URL: /softimage\"]\n",
      "\"V_1249\"\t[1, \"C_41914 - URL: /fortransupport\"]\n",
      "\"V_1250\"\t[1, \"C_37938 - URL: /middleeast\"]\n",
      "\"V_1251\"\t[1, \"C_40693 - URL: /referencesupport\"]\n",
      "\"V_1252\"\t[1, \"C_37561 - URL: /giving\"]\n",
      "\"V_1253\"\t[1, \"C_32638 - URL: /worddev\"]\n",
      "\"V_1254\"\t[1, \"C_20190 - URL: /ie3\"]\n",
      "\"V_1255\"\t[1, \"C_22918 - URL: /msmq\"]\n",
      "\"V_1256\"\t[1, \"C_30930 - URL: /sia\"]\n",
      "\"V_1257\"\t[1, \"C_40127 - URL: /devvideos\"]\n",
      "\"V_1258\"\t[1, \"C_30514 - URL: /peru\"]\n",
      "\"V_1259\"\t[1, \"C_21424 - URL: /controls\"]\n",
      "\"V_1260\"\t[1, \"C_21894 - URL: /trial\"]\n",
      "\"V_1261\"\t[1, \"C_36145 - URL: /diyguide\"]\n",
      "\"V_1262\"\t[1, \"C_37425 - URL: /chile\"]\n",
      "\"V_1263\"\t[1, \"C_27503 - URL: /services\"]\n",
      "\"V_1264\"\t[1, \"C_40427 - URL: /se\"]\n",
      "\"V_1265\"\t[1, \"C_39038 - URL: /ssafesupport\"]\n",
      "\"V_1266\"\t[1, \"C_35105 - URL: /licenses\"]\n",
      "\"V_1267\"\t[1, \"C_42071 - URL: /caribbean\"]\n",
      "\"V_1268\"\t[1, \"C_27503 - URL: /javascript\"]\n",
      "\"V_1269\"\t[1, \"C_41054 - URL: /business\"]\n",
      "\"V_1270\"\t[1, \"C_28493 - URL: /developr\"]\n",
      "\"V_1271\"\t[1, \"C_28493 - URL: /mdsn\"]\n",
      "\"V_1272\"\t[1, \"C_28493 - URL: /softlib\"]\n",
      "\"V_1273\"\t[1, \"C_28493 - URL: /mdn\"]\n",
      "\"V_1274\"\t[1, \"C_28493 - URL: /pdc\"]\n",
      "\"V_1275\"\t[1, \"C_28903 - URL: /security.\"]\n",
      "\"V_1276\"\t[1, \"C_40810 - URL: /vtestsupport\"]\n",
      "\"V_1277\"\t[1, \"C_30111 - URL: /stream\"]\n",
      "\"V_1278\"\t[1, \"C_41317 - URL: /hed\"]\n",
      "\"V_1279\"\t[1, \"C_31062 - URL: /msgolf\"]\n",
      "\"V_1280\"\t[1, \"C_41643 - URL: /music\"]\n",
      "\"V_1281\"\t[1, \"C_37099 - URL: /intellimouse\"]\n",
      "\"V_1282\"\t[1, \"C_41244 - URL: /home\"]\n",
      "\"V_1283\"\t[1, \"C_41033 - URL: /cinemania\"]\n",
      "\"V_1284\"\t[1, \"C_41108 - URL: /partner\"]\n",
      "\"V_1295\"\t[1, \"C_42616 - URL: /train\"]\n"
     ]
    }
   ],
   "source": [
    "### running the job locally\n",
    "#!python HW4_4.py anonymous-msweb.data > HW4_4_results\n",
    "\n",
    "### running the job on hadoop\n",
    "!python HW4_4.py anonymous-msweb.data --hadoop-home '/usr/local/Cellar/hadoop/2.7.1/' -r hadoop > HW4_4_results\n",
    "\n",
    "### results sample\n",
    "!cat HW4_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 4.5* \n",
    "Here you will use a different dataset consisting of word-frequency distributions\n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "- 0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "- 1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "- 2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "- 3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "**topUsers_Apr-Jul_2014_1000-words.txt**\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several\n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "- (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "- (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the\n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "**Extra Notes:**\n",
    "- For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "- For (B), (C), and (D)  you will have to use data from the auxiliary file: \n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "- (1) The 1000-user-wide aggregate, which you will perturb for initializations\n",
    "in parts (B) and (C), and\n",
    "- (2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "\n",
    "In part (D), just use the (row-normalized) class-level aggregates as 'trained'\n",
    "starting centroids (the training is already done for you!).\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate \n",
    "(after initially normalizing by its sum, which is also provided).\n",
    "So if in (B) you want to create 2 perturbations of the aggregate, start\n",
    "with (1), normalize, and generate 1000 random numbers uniformly \n",
    "from the unit interval (0,1) twice (for two centroids), using:\n",
    "\n",
    "    from numpy import random\n",
    "    numbers = random.sample(1000)\n",
    "\n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids.\n",
    "\n",
    "\n",
    "###Data pre-processing & Initialization\n",
    "- (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "- (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (D) K=4 \"trained\" centroids, determined by the sums across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pre-processing done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# load the raw data\n",
    "raw_data = np.genfromtxt('topUsers_Apr-Jul_2014_1000-words.txt', delimiter=',')\n",
    "# normalize\n",
    "train_data = [a[3:]/a[2] for a in raw_data]\n",
    "# save as training data file\n",
    "np.savetxt('HW4_5_train_data.csv', train_data, delimiter = \",\")\n",
    "\n",
    "# load the auxiliary file\n",
    "!tail -n +2 topUsers_Apr-Jul_2014_1000-words_summaries.txt | cut -d ',' -f 3-2000 > HW4_5_aux.txt\n",
    "aux_data = np.genfromtxt('HW4_5_aux.txt', delimiter=',')\n",
    "aux_norm = [a[1:]/a[0] for a in aux_data]\n",
    "\n",
    "# get centroids initialization A\n",
    "K, n = 4, 1000\n",
    "idx = np.random.randint(n, size=4)\n",
    "centroids = [train_data[i] for i in idx]\n",
    "np.savetxt('HW4_5_init_A.txt', centroids, delimiter = \",\")\n",
    "\n",
    "# get centroids initialization B\n",
    "K, n = 2, 1000\n",
    "centroids = [[a+p for a, p in zip(aux_norm[0], np.random.sample(n))] for i in range(K)]\n",
    "norm = np.sum(centroids, axis = 1)\n",
    "cen_norm = [c/n for c,n in zip(centroids, norm)]\n",
    "np.savetxt('HW4_5_init_B.txt', cen_norm, delimiter = \",\")\n",
    "\n",
    "# get centroids initialization C\n",
    "K, n = 4, 1000\n",
    "centroids = [[a+p for a, p in zip(aux_norm[0], np.random.sample(n))] for i in range(K)]\n",
    "norm = np.sum(centroids, axis = 1)\n",
    "cen_norm = [c/n for c,n in zip(centroids, norm)]\n",
    "np.savetxt('HW4_5_init_C.txt', cen_norm, delimiter = \",\")\n",
    "\n",
    "# get centroids initialization D\n",
    "centroids = aux_norm[1:]\n",
    "np.savetxt('HW4_5_init_D.txt', centroids, delimiter = \",\")\n",
    "\n",
    "print 'Data pre-processing done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MrJob to provide *one* training iteration for k-mean model\n",
    "- ready for running on Hadoop\n",
    "- assuming a file *Centroids.txt*, that stores centroids coordinates, will be updated on Hadoop cluster by the training handler/driver after each iteration\n",
    "- **input**: a csv of all training sample coordinates\n",
    "- **output**: centroids coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_5_Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_5_Kmeans.py\n",
    "\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import subprocess\n",
    "\n",
    "#Calculate find the nearest centroid for data point\n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = np.array(datapoint)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    diff = datapoint - centroid_points\n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = np.argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    # TODO: figure out why set from mapper_init doesn't preserve value\n",
    "    k, n = 2, 1000\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, \n",
    "                   mapper=self.mapper, \n",
    "                   combiner = self.combiner, \n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        hadoopPath = 'Centroids.txt'\n",
    "        localPath = '/Users/leiyang/GitHub/mids/w261/HW4-Questions/ref/Centroids.txt'\n",
    "        cat = subprocess.Popen([\"cat\", hadoopPath], stdout=subprocess.PIPE)\n",
    "        self.centroid_points = [map(float, s.strip().split(',')) for s in cat.stdout]\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point\n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        yield int(MinDist(D,self.centroid_points)), (D,1)\n",
    "\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        num = 0\n",
    "        sum_d = [0]*self.n        \n",
    "        # iterate through the generator\n",
    "        for d,c in inputdata:\n",
    "            num += c                \n",
    "            sum_d = [a+b for a,b in zip(sum_d, d)]\n",
    "        yield idx, (sum_d, num)\n",
    "\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata):\n",
    "        num = [0]*self.k\n",
    "        centroids = [[0]*self.n for i in range(self.k)]\n",
    "        # iterate through the generator\n",
    "        for d, c in inputdata:\n",
    "            num[idx] += c\n",
    "            centroids[idx] = [a+b for a,b in zip(centroids[idx], d)]          \n",
    "        # recalculate centroids\n",
    "        centroids[idx] = [a/num[idx] for a in centroids[idx]]\n",
    "        yield idx, (centroids[idx])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mean training driver\n",
    "- control the training process \n",
    "- update Centroids.txt file from Hadoop clusters after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_5_Kmeans_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_5_Kmeans_driver.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "from HW4_5_Kmeans import MRKmeans, stop_criterion\n",
    "\n",
    "# create MrJob: specify input, available file, hadoop home\n",
    "mr_job = MRKmeans(args=['HW4_5_train_data.csv', '--file', 'Centroids.txt',\n",
    "                        '--hadoop-home', '/usr/local/Cellar/hadoop/2.7.1/', '-r', 'hadoop'])\n",
    "\n",
    "# load initial centroids\n",
    "centroid_points = np.genfromtxt('Centroids.txt', delimiter=',')\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 1\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    np.savetxt('c_old', centroid_points_old, delimiter=',')\n",
    "    print \"Iteration \"+str(i)+\" ...\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output         \n",
    "        for line in runner.stream_output():\n",
    "            key,value = mr_job.parse_output_line(line)            \n",
    "            centroid_points[key] = value        \n",
    "            \n",
    "    i += 1\n",
    "    # put the latest centroids in the file for next iteration    \n",
    "    np.savetxt('Centroids.txt', centroid_points, delimiter = \",\")\n",
    "        \n",
    "    # check stopping criterion\n",
    "    if stop_criterion(np.genfromtxt('c_old', delimiter=','), centroid_points, 0.001):\n",
    "        break\n",
    "        \n",
    "print '\\nK-means training completed after %d iterations!' %(i-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find cluster composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_5_get_composition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_5_get_composition.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from HW4_5_Kmeans import MinDist\n",
    "\n",
    "# load the centroids\n",
    "centroids = np.genfromtxt('Centroids.txt', delimiter=',') \n",
    "compo = [[0,0,0,0] for i in range(len(centroids))]\n",
    "\n",
    "# load the raw data\n",
    "raw_data = np.genfromtxt('topUsers_Apr-Jul_2014_1000-words.txt', delimiter=',')\n",
    "\n",
    "# find the cluster for each point, and record code count\n",
    "for r in raw_data:\n",
    "    code, point = r[1], r[3:]/r[2]\n",
    "    compo[int(MinDist(point, centroids))][int(code)] += 1\n",
    "    \n",
    "# show results\n",
    "for i in range(len(compo)):\n",
    "    print '\\n'\n",
    "    for j in range(4):\n",
    "        print 'Centroid %d - code %d: %d (%.2f%%)' %(i, j, compo[i][j], 100.0*compo[i][j]/sum(compo[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "- run MrJob to train k-means model\n",
    "- check composition of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 ...\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "Iteration 5 ...\n",
      "Iteration 6 ...\n",
      "Iteration 7 ...\n",
      "\n",
      "K-means training completed after 7 iterations!\n",
      "\n",
      "\n",
      "Centroid 0 - code 0: 67 (45.89%)\n",
      "Centroid 0 - code 1: 0 (0.00%)\n",
      "Centroid 0 - code 2: 12 (8.22%)\n",
      "Centroid 0 - code 3: 67 (45.89%)\n",
      "\n",
      "\n",
      "Centroid 1 - code 0: 1 (0.76%)\n",
      "Centroid 1 - code 1: 88 (66.67%)\n",
      "Centroid 1 - code 2: 39 (29.55%)\n",
      "Centroid 1 - code 3: 4 (3.03%)\n",
      "\n",
      "\n",
      "Centroid 2 - code 0: 683 (94.73%)\n",
      "Centroid 2 - code 1: 3 (0.42%)\n",
      "Centroid 2 - code 2: 3 (0.42%)\n",
      "Centroid 2 - code 3: 32 (4.44%)\n",
      "\n",
      "\n",
      "Centroid 3 - code 0: 1 (100.00%)\n",
      "Centroid 3 - code 1: 0 (0.00%)\n",
      "Centroid 3 - code 2: 0 (0.00%)\n",
      "Centroid 3 - code 3: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "!cat HW4_5_init_A.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 ...\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "\n",
      "K-means training completed after 4 iterations!\n",
      "\n",
      "\n",
      "Centroid 0 - code 0: 751 (86.62%)\n",
      "Centroid 0 - code 1: 3 (0.35%)\n",
      "Centroid 0 - code 2: 14 (1.61%)\n",
      "Centroid 0 - code 3: 99 (11.42%)\n",
      "\n",
      "\n",
      "Centroid 1 - code 0: 1 (0.75%)\n",
      "Centroid 1 - code 1: 88 (66.17%)\n",
      "Centroid 1 - code 2: 40 (30.08%)\n",
      "Centroid 1 - code 3: 4 (3.01%)\n"
     ]
    }
   ],
   "source": [
    "!cat HW4_5_init_B.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 ...\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "Iteration 5 ...\n",
      "Iteration 6 ...\n",
      "\n",
      "K-means training completed after 6 iterations!\n",
      "\n",
      "\n",
      "Centroid 0 - code 0: 0 (0.00%)\n",
      "Centroid 0 - code 1: 2 (13.33%)\n",
      "Centroid 0 - code 2: 13 (86.67%)\n",
      "Centroid 0 - code 3: 0 (0.00%)\n",
      "\n",
      "\n",
      "Centroid 1 - code 0: 751 (87.53%)\n",
      "Centroid 1 - code 1: 3 (0.35%)\n",
      "Centroid 1 - code 2: 5 (0.58%)\n",
      "Centroid 1 - code 3: 99 (11.54%)\n",
      "\n",
      "\n",
      "Centroid 2 - code 0: 1 (1.32%)\n",
      "Centroid 2 - code 1: 35 (46.05%)\n",
      "Centroid 2 - code 2: 36 (47.37%)\n",
      "Centroid 2 - code 3: 4 (5.26%)\n",
      "\n",
      "\n",
      "Centroid 3 - code 0: 0 (0.00%)\n",
      "Centroid 3 - code 1: 51 (100.00%)\n",
      "Centroid 3 - code 2: 0 (0.00%)\n",
      "Centroid 3 - code 3: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "!cat HW4_5_init_C.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (D) K=4 \"trained\" centroids, determined by the sums across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 ...\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "Iteration 5 ...\n",
      "\n",
      "K-means training completed after 5 iterations!\n",
      "\n",
      "\n",
      "Centroid 0 - code 0: 749 (93.16%)\n",
      "Centroid 0 - code 1: 3 (0.37%)\n",
      "Centroid 0 - code 2: 14 (1.74%)\n",
      "Centroid 0 - code 3: 38 (4.73%)\n",
      "\n",
      "\n",
      "Centroid 1 - code 0: 0 (0.00%)\n",
      "Centroid 1 - code 1: 51 (100.00%)\n",
      "Centroid 1 - code 2: 0 (0.00%)\n",
      "Centroid 1 - code 3: 0 (0.00%)\n",
      "\n",
      "\n",
      "Centroid 2 - code 0: 1 (1.22%)\n",
      "Centroid 2 - code 1: 37 (45.12%)\n",
      "Centroid 2 - code 2: 40 (48.78%)\n",
      "Centroid 2 - code 3: 4 (4.88%)\n",
      "\n",
      "\n",
      "Centroid 3 - code 0: 2 (3.17%)\n",
      "Centroid 3 - code 1: 0 (0.00%)\n",
      "Centroid 3 - code 2: 0 (0.00%)\n",
      "Centroid 3 - code 3: 61 (96.83%)\n"
     ]
    }
   ],
   "source": [
    "!cat HW4_5_init_D.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW4.6  (OPTIONAL)* Scaleable K-MEANS++\n",
    "\n",
    "- Read the following paper entitled \"Scaleable K-MEANS++\" located at:\n",
    "\n",
    "http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf\n",
    "\n",
    "- In MrJob, implement K-MEANS|| and compare with a random initialization for the dataset above.\n",
    "Report on the number passes over the training data, and time required to run all  clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###No code, only implementation notes\n",
    "**A MapReduce job to perform one iteration of finding centroid:**\n",
    "- **mapper_init():** load current centroids set $C$.\n",
    "- **mapper():** calculate $d^2(x,C)$, and emit:\n",
    "  - $(*, d^2(x,C), None)$:, order inversion to calcuate $\\sum_{x\\in X}d^2(x,C)$.\n",
    "  - $(id, d^2(x,C), point\\_coordinates)$ in numerical ascending order\n",
    "- **single reducer():** \n",
    "  - calculate $\\sum_{x\\in X}d^2(x,C)$ with the emits with $*$ keys;\n",
    "  - generate $\\Omega(K)$ random values $L_i$ such that $0<L_i\\leq \\sum_{x\\in X}d^2(x,C)$;\n",
    "  - choose $\\Omega(K)$ new centers $c_i$, selecting the $c_i = x_j$ such that $\\sum_{m=1}^{j-1}{d^2(x_m, C)}<L_i\\leq \\sum_{m=1}^{j}{d^2(x_m, C)}$;\n",
    "  - **[note]:** last two steps are the impelementation of *sample each point $x\\in X$ independently with probability $p_x=\\frac{\\Omega(K)d^2(x,C)}{\\sum_{x\\in X}d^2(x,C)}$*, according to [this paper](http://www.ojs.academypublisher.com/index.php/jetwi/article/viewFile/jetwi04015159/4277).\n",
    "\n",
    "**A driver program to control the loops:**\n",
    "1. randomly select a point from $X$, save to a file\n",
    "2. run mrjob in loop\n",
    " - specify the centroids file to the job\n",
    " - after each run, collect centroids from the reducer, update the file\n",
    " - check stopping condition\n",
    "3. merge centroids to have the final $K$, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW4.7   (OPTIONAL)* Canopy Clustering\n",
    "\n",
    "An alternative way to intialize the k-means algorithm is the  canopy clustering. The canopy clustering \n",
    "algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and \n",
    "Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the \n",
    "Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, \n",
    "where using another algorithm directly may be impractical due to the size of the data set.\n",
    "\n",
    "For more details on the Canopy Clustering algorithm see:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Canopy_clustering_algorithm\n",
    "\n",
    "- Plot the initialation centroids and the centroid trajectory as the Canopy Clustering based K-MEANS algorithm iterates. \n",
    "- Repeat this for a random initalization (i.e., pick a training vector at random for each inital centroid)\n",
    "of the kmeans algorithm. \n",
    "- Comment on the trajectories of both algorithms.\n",
    "- Report on the number passes over the training data, and time required to run both  clustering algorithms.\n",
    "- Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n",
    "**4.7.1** \n",
    "- Apply your implementation Canopy Clustering based K-MEANS algorithm to the dataset  in HW 4.5 and compare to the a random initalization (i.e., pick a training vector at random for each inital centroid) of the kmeans algorithm.\n",
    "- Report on the number passes over the training data, and time required to run both  clustering algorithms. \n",
    "- Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MrJob for Canopy Clustering - *(incomplete, still in code debug)*\n",
    "#### job to get all canopies\n",
    "- **mapper**: creates canopies on parallel for each chunk of data the mapper gets [[ref](http://www.kamalnigam.com/papers/canopy-kdd00.pdf)]\n",
    " - as record streams through, decide if it:\n",
    "   - belongs to the inner circle of any existin canopy\n",
    "   - belongs to the outer circle of any existin canopy\n",
    "   - belongs to a new canopy\n",
    " - for each role the record plays, emit a line such that:\n",
    "   - all inner and/or outer circle points of a canopy will arrive reducer consecutively (through partitioner)\n",
    "   - *key*: canopy ID - can use message ID\n",
    "   - *value*: (point coordinates, 1)\n",
    " - **note**: the mapper will maintain a collection of canopy centroids, given that we want more mappers, caching centroid is presumably ok\n",
    "- **combiner**: calculate intermediate canopy centroid\n",
    " - need to maintain record count if adopt\n",
    "- **single reducer**: get all canopies and yield the centroids \n",
    " - get centroid for each canopy by averaging all inner and outer circle points\n",
    "- merge centroids if necessary to have the final $K$ as K-mean initialization\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_7_Canopy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_7_Canopy.py\n",
    "\n",
    "#import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "from math import pow\n",
    "\n",
    "class MRCanopy(MRJob):\n",
    "    centroids = {}\n",
    "    # TODO: figure out why set from mapper_init doesn't preserve value\n",
    "    k, n = 2, 1000\n",
    "    T1, T2 = 0.001, 0.0001\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            # job 1\n",
    "            MRStep(mapper=self.mapper1\n",
    "                   ,combiner = self.combiner1\n",
    "                   ,reducer=self.reducer1\n",
    "                  )            \n",
    "               ]\n",
    "\n",
    "    # Load data and output each point with the canopy it belongs to or defines\n",
    "    def mapper1(self, _, line):            \n",
    "        D = [map(float, line.strip().split(','))]        \n",
    "        name, point = str(D[0]), D[1:]\n",
    "        yield name, (point, 1)\n",
    "        if len(self.centroids) == 0:\n",
    "            # first point, add canopy\n",
    "            self.centroids[name] = point        \n",
    "            #print 'first'\n",
    "            yield name, (point, 1)\n",
    "        else:\n",
    "            # compare with each centroid to determine status\n",
    "            isNew = True\n",
    "            for cen in self.centroids:\n",
    "                # calculate distance\n",
    "                dist = sum([pow(a-b, 2) for a,b in zip(self.centroids[cen], point)])\n",
    "                # inside inner circle of cen\n",
    "                if self.T2 > dist:                    \n",
    "                    isNew = False\n",
    "                    #print 'inner'\n",
    "                    yield cen, (point, 1)\n",
    "                # in between circles of cen\n",
    "                elif self.T2 < dist and dist < self.T1:                    \n",
    "                    #print 'close'\n",
    "                    yield cen, (point, 1)\n",
    "            # not in anyone's inner circle, add new canopy\n",
    "            if isNew:                \n",
    "                self.centroids[name] = point\n",
    "                #print 'new'\n",
    "                yield name, (point, 1)\n",
    "                \n",
    "\n",
    "    # Combine sum of data points locally\n",
    "    def combiner1(self, name, inputdata):\n",
    "        num = 0\n",
    "        sum_d = [0]*self.n\n",
    "        # iterate through the generator\n",
    "        for d,c in inputdata:\n",
    "            num += c\n",
    "            sum_d = [a+b for a,b in zip(sum_d, d)]\n",
    "        yield name, (sum_d, num)\n",
    "\n",
    "    # Aggregate sum for each canopy, and then calculate the new centroids\n",
    "    def reducer1(self, name, inputdata):\n",
    "        num = 0 #[0]*self.k\n",
    "        centroid = [0]*self.n\n",
    "        current_canopy = None\n",
    "        if current_canopy != name:\n",
    "            # aggregation completes for previous canopy, emit\n",
    "            centroid = [a/num for a in centroid]\n",
    "            yield current_canopy, centroid\n",
    "            # reset for new canopy\n",
    "            current_canopy = name\n",
    "            num = 0\n",
    "            centroid = [0]*self.n\n",
    "        # iterate through the generator\n",
    "        for d, c in inputdata:\n",
    "            num += c\n",
    "            centroid = [a+b for a,b in zip(centroid, d)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRCanopy.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Get training data from HW4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load the raw data\n",
    "raw_data = np.genfromtxt('topUsers_Apr-Jul_2014_1000-words.txt', delimiter=',')\n",
    "# normalize\n",
    "train_data = [np.insert(a[3:]/a[2],0,a[0]) for a in raw_data]\n",
    "# save as training data file\n",
    "np.savetxt('HW4_7_train_data.csv', train_data, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute the MrJob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_7_Canopy.leiyang.20160210.041424.388486\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_7_Canopy.leiyang.20160210.041424.388486/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_7_Canopy.leiyang.20160210.041424.388486/step-0-mapper_part-00000 -> /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_7_Canopy.leiyang.20160210.041424.388486/output/part-00000\n",
      "Streaming final output from /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_7_Canopy.leiyang.20160210.041424.388486/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_7_Canopy.leiyang.20160210.041424.388486\n"
     ]
    }
   ],
   "source": [
    "!python HW4_7_Canopy.py HW4_7_train_data.csv > debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping historyserver\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ stop historyserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 22654. Stop it first.\n",
      "localhost: nodemanager running as process 22755. Stop it first.\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 22904. Stop it first.\n",
      "localhost: datanode running as process 22997. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 23116. Stop it first.\n",
      "historyserver running as process 23224. Stop it first.\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
