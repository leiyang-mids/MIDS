{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#5\n",
    "####[Lei Yang](mailto:leiyang@berkeley.edu) | [Michael Kennedy](mailto:mkennedy@ischool.berkeley.edu) | [Natarajan Krishnaswami](mailto:natarajan@krishnaswami.org)\n",
    "####Due: 2016-02-18, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.0.* Q&A\n",
    "\n",
    "####What is a data warehouse? What is a Star schema? When is it used?\n",
    "\n",
    "\n",
    "###*HW 5.1* Q&A\n",
    "####In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "\n",
    "####In what form does ML consume data?\n",
    "\n",
    "####Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.2*\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.):\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "1. Left joining Table Left with Table Right\n",
    "2. Right joining Table Left with Table Right\n",
    "3. Inner joining Table Left with Table Right\n",
    "\n",
    "###Solution\n",
    "- set A-records as Left table which is stored in memory, because it has fewer data\n",
    "- C- and V-lines are Right table, as the records streaming through, join visitor, page, and URL \n",
    "- emit (page_id, visitor_id)~URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_2_JoinTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_2_JoinTable.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class JoinTable(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.left = {}\n",
    "        self.right = []\n",
    "        self.vistor = None\n",
    "        \n",
    "    # stream through lines, yield char count\n",
    "    def mapper_inner(self, _, line):\n",
    "        # get page id\n",
    "        line = line.strip()\n",
    "        # A-line\n",
    "        if line[0] == 'A':\n",
    "            d1, p_id, d2, p_name, url = line.split(',')\n",
    "            self.left[p_id] = [p_name, url]\n",
    "            return\n",
    "        # C-line\n",
    "        if line[0] == 'C':\n",
    "            d1, d2, v_id = line.split(',')\n",
    "            self.vistor = 'C_' + v_id\n",
    "            return\n",
    "        # V-line\n",
    "        if line[0] == 'V':\n",
    "            d1, p_id, d2 = line.split(',')\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        # inner join\n",
    "        if p_id not in self.left:\n",
    "            return\n",
    "        else:        \n",
    "            self.right.append(p_id)\n",
    "            yield (p_id,self.vistor), self.left[p_id][1]\n",
    "            \n",
    "    def mapper_right(self, _, line):\n",
    "        # get page id\n",
    "        line = line.strip()\n",
    "        # A-line\n",
    "        if line[0] == 'A':\n",
    "            d1, p_id, d2, p_name, url = line.split(',')\n",
    "            self.left[p_id] = [p_name, url]\n",
    "            return\n",
    "        # C-line\n",
    "        if line[0] == 'C':\n",
    "            d1, d2, v_id = line.split(',')\n",
    "            self.vistor = 'C_' + v_id\n",
    "            return\n",
    "        # V-line\n",
    "        if line[0] == 'V':\n",
    "            d1, p_id, d2 = line.split(',')\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        # right join\n",
    "        if p_id not in self.left:\n",
    "            yield (p_id, self.vistor), None\n",
    "        else:            \n",
    "            yield (p_id, self.vistor), self.left[p_id][1]\n",
    "            \n",
    "    def mapper_left(self, _, line):\n",
    "        # get page id\n",
    "        line = line.strip()\n",
    "        # A-line\n",
    "        if line[0] == 'A':\n",
    "            d1, p_id, d2, p_name, url = line.split(',')\n",
    "            self.left[p_id] = [p_name, url]\n",
    "            return\n",
    "        # C-line\n",
    "        if line[0] == 'C':\n",
    "            d1, d2, v_id = line.split(',')\n",
    "            self.vistor = 'C_' + v_id\n",
    "            return\n",
    "        # V-line\n",
    "        if line[0] == 'V':\n",
    "            d1, p_id, d2 = line.split(',')\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        # right join\n",
    "        if p_id in self.left:                        \n",
    "            yield (p_id, self.vistor), self.left[p_id][1]\n",
    "        elif p_id not in self.right:\n",
    "            self.right.append(p_id)\n",
    "    \n",
    "    # left join only, yield left which right doesn't have\n",
    "    def mapper_final(self):\n",
    "        for p_id in self.right:\n",
    "            yield (p_id, None), self.left[p_id][1]\n",
    "        \n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.n_row = 0\n",
    "        \n",
    "    def reducer(self, page, url):        \n",
    "        self.n_row += 1\n",
    "        \n",
    "    def reducer_final(self):\n",
    "        yield None, str(self.n_row)\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf = {\n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper_init=self.mapper_init\n",
    "                       # inner join\n",
    "                       #,mapper=self.mapper_inner\n",
    "                       # right join\n",
    "                       #,mapper=self.mapper_right\n",
    "                       # left join\n",
    "                       ,mapper=self.mapper_left, mapper_final=self.mapper_final\n",
    "                       # reducer\n",
    "                       ,reducer_init=self.reducer_init, reducer=self.reducer, reducer_final=self.reducer_final\n",
    "                       ,jobconf=jobconf\n",
    "                       )                \n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    JoinTable.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob\n",
    "- inner: 98654\n",
    "- left: 98654\n",
    "- right: 98654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484/step-0-mapper-sorted\n",
      "> sort /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484/step-0-mapper_part-00000\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484/step-0-reducer_part-00000 -> /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484/output/part-00000\n",
      "Streaming final output from /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_2_JoinTable.leiyang.20160217.051020.033484\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "!python HW5_2_JoinTable.py anonymous-msweb.data > debug\n",
    "\n",
    "##### run it locally on hadoop #####\n",
    "#!python HW5_2_JoinTable.py anonymous-msweb.data -r hadoop > debug\n",
    "#!cat debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.3* For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "#### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "#### 2: A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "For each HW 5.3 -5.5 Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations. Then show the results you get with you system.\n",
    "Final show your results on the Google n-grams dataset\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "- \t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "- e.g. 'A Case Study of Limited\t55\t55\t43'\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - The longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_Longest5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_Longest5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Longest5Gram(MRJob):\n",
    "\n",
    "    # stream through lines, yield char count\n",
    "    def mapper(self, _, line):\n",
    "        # get page id\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        yield n_gram, len(n_gram)\n",
    "        \n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.length = 0\n",
    "        self.longest = None\n",
    "\n",
    "    def reducer(self, n_gram, n_char):\n",
    "        cnt = sum(n_char)\n",
    "        if cnt > self.length:\n",
    "            self.longest = n_gram\n",
    "            self.length = cnt\n",
    "\n",
    "    def reducer_final(self):\n",
    "        yield self.longest, (self.length)\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf = {\n",
    "            'mapreduce.job.maps': '30',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper\n",
    "                       # NOTE: combiner doesn't work on EMR Hadoop-1.0.3 & AMI-2.4.2, \n",
    "                       # only on AMI-3.11.0 with Hadoop-2.4.0\n",
    "                       ,combiner_init=self.reducer_init \n",
    "                       ,combiner=self.reducer\n",
    "                       ,combiner_final=self.reducer_final\n",
    "                       ,reducer_init=self.reducer_init\n",
    "                       ,reducer=self.reducer\n",
    "                       ,reducer_final=self.reducer_final\n",
    "                       ,jobconf=jobconf\n",
    "                       )                \n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Longest5Gram.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob\n",
    "\n",
    "- with EMR running for s3://filtered-5grams/, the longest 5-gram is:\n",
    "- **\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\t**\n",
    "- number of character: 159\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Longest5Gram.leiyang.20160214.215417.539788\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Longest5Gram.leiyang.20160214.215417.539788/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_3_Longest5Gram.leiyang.20160214.215417.539788/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar8609884381448338019/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob1002621193370530292.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_3_Longest5Gram.leiyang.20160214.215417.539788/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Longest5Gram.leiyang.20160214.215417.539788\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_3_Longest5Gram.leiyang.20160214.215417.539788 from HDFS\n",
      "\"Hydroxytryptamine stimulates inositol phosphate production\"\t58\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_3_Longest5Gram.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop #####\n",
    "#!python HW5_3_Longest5Gram.py ngram_test.txt -r hadoop > debug\n",
    "!python HW5_3_Longest5Gram.py googlebooks-eng-all-5gram-20090715-0-filtered.txt -r hadoop > debug\n",
    "!cat debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_Longest5Gram.py s3://filtered-5grams/ -r emr --output-dir 's3://us-west-2/w261.data/HW5/'  --no-output\n",
    "\n",
    "#!python HW5_3_Longest5Gram.py googlebooks-eng-all-5gram-20090715-0-filtered.txt -r emr --output-dir 's3://us-west-2/w261.data/HW5/'  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - Top 10 most frequent words (count), i.e., unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_Top10Words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_Top10Words.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Top10Words(MRJob):\n",
    "\n",
    "    # stream through lines, yield word count\n",
    "    def mapper(self, _, line):\n",
    "        # get page id\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        cnt = int(cnt)\n",
    "        for w in n_gram.lower().split(' '):\n",
    "            yield w, cnt\n",
    "\n",
    "    # sum word counts, use as combiner too\n",
    "    def reducer(self, word, count):\n",
    "        yield word, sum(count)\n",
    "        \n",
    "    # job to sort the results ###########################\n",
    "    def mapper_sort1(self, word, count):\n",
    "        yield (word, count), None\n",
    "        \n",
    "    def reducer_sort_init1(self):\n",
    "        self.top = 20\n",
    "        self.n = 0\n",
    "        \n",
    "    def reducer_sort1(self, results, dummy):        \n",
    "        if self.n < self.top:\n",
    "            self.n += 1\n",
    "            yield results\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf1 = {  #key value pairs \n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '3',\n",
    "        }\n",
    "        \n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper                       \n",
    "                       ,combiner=self.reducer                                              \n",
    "                       ,reducer=self.reducer                       \n",
    "                       ,jobconf=jobconf1\n",
    "                       )\n",
    "                ,MRStep(mapper=self.mapper_sort1\n",
    "                       ,reducer_init=self.reducer_sort_init1\n",
    "                       ,reducer=self.reducer_sort1\n",
    "                       ,jobconf=jobconf2\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Top10Words.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob\n",
    "- **with EMR running for s3://filtered-5grams/, the top 10 words are:**\n",
    " \n",
    " \n",
    "| Word        | Count           |\n",
    "| ------------- |:-------------:| \n",
    "| \"the\"      | 5490815394 | \n",
    "| \"of\"      | 3698583299      | \n",
    "| \"to\" |  2227866570    | \n",
    "| \"in\" |\t1421312776 |\n",
    "| \"a\" |\t1361123022 |\n",
    "| \"and\" |\t1149577477 |\n",
    "| \"that\" |\t802921147 |\n",
    "| \"is\" |\t758328796 |\n",
    "| \"be\" |\t688707130 |\n",
    "| \"as\" |\t492170314 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Top10Words.leiyang.20160216.005540.666756\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Top10Words.leiyang.20160216.005540.666756/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_3_Top10Words.leiyang.20160216.005540.666756/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar969921165491504766/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6150646407912706716.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar8254010522949119050/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6830498636508549001.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_3_Top10Words.leiyang.20160216.005540.666756/output\n",
      "\"a\"\t107417\n",
      "\"of\"\t77055\n",
      "\"the\"\t66314\n",
      "\"history\"\t25820\n",
      "\"united\"\t24965\n",
      "\"after\"\t9673\n",
      "\"in\"\t9511\n",
      "\"clear\"\t9093\n",
      "\"understanding\"\t8527\n",
      "\"to\"\t8362\n",
      "\"and\"\t6799\n",
      "\"is\"\t6020\n",
      "\"for\"\t5121\n",
      "\"study\"\t4721\n",
      "\"was\"\t4072\n",
      "\"that\"\t3937\n",
      "\"on\"\t2588\n",
      "\"according\"\t2116\n",
      "\"this\"\t2088\n",
      "\"about\"\t2050\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Top10Words.leiyang.20160216.005540.666756\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_3_Top10Words.leiyang.20160216.005540.666756 from HDFS\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_3_Top10Words.py ngram_test.txt > debug2\n",
    "\n",
    "##### run it locally on hadoop with a sample file #####\n",
    "#!python HW5_3_Top10Words.py ngram_test.txt -r hadoop > debug38\n",
    "!python HW5_3_Top10Words.py ngram_test.txt -r hadoop\n",
    "#!cat debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_Top10Words.py s3://filtered-5grams/ -r emr --cleanup=NONE  --no-output\n",
    "#!python HW5_3_Top10Words.py ngram_test.txt -r emr --cleanup=NONE  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Hint: save to PART-000\\* and take the head -n 1000\n",
    "- every word appears at least once per page, so the least densely appearing ratio is $1$, and there are a bunch of those\n",
    "- we do reverse sorting below for $\\frac{count}{pages\\_count}$, and show the top 200.\n",
    "\n",
    "Top 10 most densely appearing words:\n",
    "\n",
    "| word | count/Page_count |\n",
    "| ------------- |:-------------:|\n",
    "|\"xxxx\" |\t11.557291666666666 |\n",
    "|\"blah\"\t| 8.0741599073001158 |\n",
    "|\"nnn\"\t| 7.5333333333333332 |\n",
    "|\"na\"\t| 6.2017491314244637 |\n",
    "|\"oooooooooooooooo\"\t| 4.921875 |\n",
    "|\"nd\"\t| 4.8543057272352703 |\n",
    "|\"llll\"\t| 4.5116279069767442 |\n",
    "|\"oooooo\" |\t4.169650013358269 |\n",
    "|\"ooooo\" |\t3.8586371934672128 |\n",
    "|\"lillelu\" |\t3.7624521072796937 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_MostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_MostLeastDenseWords.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostLeastDenseWords(MRJob):\n",
    "\n",
    "    # stream through lines, yield word count\n",
    "    def mapper(self, _, line):\n",
    "        # get page id\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        cnt, p_cnt = int(cnt), int(p_cnt)\n",
    "        for w in n_gram.lower().split(' '):\n",
    "            yield w, (cnt, p_cnt)\n",
    "            \n",
    "    # combiner\n",
    "    def combiner(self, word, counts):\n",
    "        cnt = p_cnt = 0\n",
    "        for c in counts:\n",
    "            cnt += c[0]\n",
    "            p_cnt += c[1]\n",
    "        yield word, (cnt, p_cnt)\n",
    "   \n",
    "    # sum word counts, use as combiner too\n",
    "    def reducer(self, word, counts):\n",
    "        cnt = p_cnt = 0\n",
    "        for c in counts:\n",
    "            cnt += c[0]\n",
    "            p_cnt += c[1]\n",
    "        yield word, 1.0*cnt/p_cnt\n",
    "        \n",
    "    # job to sort the results ###########################\n",
    "    def mapper_sort1(self, word, ratio):\n",
    "        yield (word, ratio), None\n",
    "        \n",
    "    def reducer_sort_init1(self):\n",
    "        self.top = 200\n",
    "        self.n = 0\n",
    "        \n",
    "    def reducer_sort1(self, results, dummy):        \n",
    "        if self.n < self.top:\n",
    "            self.n += 1\n",
    "            yield results\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf1 = {  #key value pairs \n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '3',\n",
    "        }\n",
    "        \n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper                       \n",
    "                       ,combiner=self.combiner\n",
    "                       ,reducer=self.reducer                       \n",
    "                       ,jobconf=jobconf1\n",
    "                       )\n",
    "                ,MRStep(mapper=self.mapper_sort1\n",
    "                       ,reducer_init=self.reducer_sort_init1\n",
    "                       ,reducer=self.reducer_sort1\n",
    "                       ,jobconf=jobconf2\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostLeastDenseWords.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6935029832103145630/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob9152720224773007202.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar2229929535559806893/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob7809952110008506371.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_3_MostLeastDenseWords.leiyang.20160215.041741.560658 from HDFS\n",
      "\"front\"\t1.6140350877192982\n",
      "\"banker\"\t1.3333333333333333\n",
      "\"acceptable\"\t1.279503105590062\n",
      "\"goods\"\t1.279503105590062\n",
      "\"identification\"\t1.279503105590062\n",
      "\"photometer\"\t1.2636363636363637\n",
      "\"philosophical\"\t1.2592592592592593\n",
      "\"political\"\t1.2592592592592593\n",
      "\"chief\"\t1.2487562189054726\n",
      "\"secretary\"\t1.2487562189054726\n",
      "\"council\"\t1.2261306532663316\n",
      "\"acting\"\t1.1984126984126984\n",
      "\"need\"\t1.1944444444444444\n",
      "\"able\"\t1.1931818181818181\n",
      "\"read\"\t1.1931818181818181\n",
      "\"write\"\t1.1931818181818181\n",
      "\"dirty\"\t1.1688311688311688\n",
      "\"funny\"\t1.1688311688311688\n",
      "\"projected\"\t1.1614906832298137\n",
      "\"representative\"\t1.1571428571428573\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_3_MostLeastDenseWords.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop with a sample file #####\n",
    "!python HW5_3_MostLeastDenseWords.py ngram_test.txt -r hadoop > debug\n",
    "!head -20 debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_MostLeastDenseWords.py s3://filtered-5grams/ -r emr --cleanup=NONE  --no-output\n",
    "#!python HW5_3_MostLeastDenseWords.py ngram_test.txt -r emr --cleanup=NONE  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - Distribution of 5-gram sizes (character length).  \n",
    "- E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. \n",
    "- Plot the data graphically using a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_3_Distribution5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_3_Distribution5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Distribution5Gram(MRJob):\n",
    "\n",
    "    # stream through lines, yield word count\n",
    "    def mapper(self, _, line):\n",
    "        # get counts\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')        \n",
    "        yield len(n_gram), int(cnt)\n",
    "            \n",
    "    # combiner/reducer\n",
    "    def combiner(self, n_gram, count):        \n",
    "        yield n_gram, sum(count)\n",
    "        \n",
    "    # job to sort the results ###########################\n",
    "    def mapper_sort1(self, word, ratio):\n",
    "        yield (word, ratio), None\n",
    "        \n",
    "    def reducer_sort1(self, results, dummy):               \n",
    "        yield results\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf1 = {  #key value pairs \n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "        }\n",
    "        \n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '4',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper                       \n",
    "                       ,combiner=self.combiner                       \n",
    "                       ,reducer=self.combiner           \n",
    "                       ,jobconf=jobconf1\n",
    "                       )\n",
    "                ,MRStep(mapper=self.mapper_sort1                \n",
    "                       ,reducer=self.reducer_sort1\n",
    "                       ,jobconf=jobconf2\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Distribution5Gram.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160216.032520.071600\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160216.032520.071600/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_3_Distribution5Gram.leiyang.20160216.032520.071600/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar562392759491923876/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2542307027320679323.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6889228344687883282/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob801441194733408061.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_3_Distribution5Gram.leiyang.20160216.032520.071600/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_3_Distribution5Gram.leiyang.20160216.032520.071600\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_3_Distribution5Gram.leiyang.20160216.032520.071600 from HDFS\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_3_Distribution5Gram.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop with a sample file #####\n",
    "!python HW5_3_Distribution5Gram.py ngram_test.txt -r hadoop > debug\n",
    "#!head -20 debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_3_Distribution5Gram.py s3://filtered-5grams/ -r emr --cleanup=NONE  --no-output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Plot 5-gram Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEZCAYAAAB8culNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bd3a3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "\n",
    "mat = loadtxt('Dist5Gram')\n",
    "plt.bar(mat[:,0],mat[:,1],0.4)\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('5-gram Size')\n",
    "plt.title('Count of 5-gram Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3 - *OPTIONAL* Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Write SYSTEMS_TEST_DATASET.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SYSTEMS_TEST_DATASET.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile SYSTEMS_TEST_DATASET.txt\n",
    "DocA {'X':20, 'Y':30, 'Z':5}\n",
    "DocB {'X':100, 'Y':20}\n",
    "DocC {'M':5, 'N':20, 'Z':5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Paper-and-Pencil Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.4 * (over 2Gig of Data)\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "1. Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "2. Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "**Design notes for (1)**\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "**Design notes for (2)**\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "- ...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.4 - MRJob to get the top 10,000 words\n",
    "- only output 1,001 ~ 10,000 on the top 10,000 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_4_Get10kWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_4_Get10kWords.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Get10kWords(MRJob):\n",
    "\n",
    "    # stream through lines, yield word count\n",
    "    def mapper(self, _, line):\n",
    "        # get page id\n",
    "        n_gram, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        cnt = int(cnt)\n",
    "        for w in n_gram.lower().split(' '):\n",
    "            yield w, cnt\n",
    "\n",
    "    # sum word counts, use as combiner too\n",
    "    def reducer(self, word, count):\n",
    "        yield word, sum(count)\n",
    "\n",
    "    # job to sort the results ###########################\n",
    "    def mapper_sort1(self, word, count):\n",
    "        yield (word, count), None\n",
    "\n",
    "    def reducer_sort_init1(self):\n",
    "        self.cut = 1000\n",
    "        self.total = 10000\n",
    "        self.n = 0\n",
    "\n",
    "    def reducer_sort1(self, results, dummy):\n",
    "        self.n += 1\n",
    "        if self.n > self.cut and self.n <= self.total:            \n",
    "            yield results\n",
    "\n",
    "    def steps(self):\n",
    "        jobconf1 = {  #key value pairs\n",
    "            'mapreduce.job.maps': '20',\n",
    "            'mapreduce.job.reduces': '20',\n",
    "        }\n",
    "\n",
    "        jobconf2 = {  #key value pairs\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '15',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',\n",
    "        }\n",
    "\n",
    "        return [MRStep(mapper=self.mapper\n",
    "                       ,combiner=self.reducer\n",
    "                       ,reducer=self.reducer\n",
    "                       ,jobconf=jobconf1\n",
    "                       )\n",
    "                ,MRStep(mapper=self.mapper_sort1\n",
    "                       ,reducer_init=self.reducer_sort_init1\n",
    "                       ,reducer=self.reducer_sort1\n",
    "                       ,jobconf=jobconf2\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Get10kWords.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob\n",
    "- top 10,000 words, except from 1 - 1000, from the corpus are [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Get10kWords.leiyang.20160216.010646.797154\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Get10kWords.leiyang.20160216.010646.797154/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_4_Get10kWords.leiyang.20160216.010646.797154/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar471521431493938887/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob5993568477184740852.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar3827907617680541949/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2431220369631799157.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_4_Get10kWords.leiyang.20160216.010646.797154/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Get10kWords.leiyang.20160216.010646.797154\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_4_Get10kWords.leiyang.20160216.010646.797154 from HDFS\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_4_Get10kWords.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop #####\n",
    "!python HW5_4_Get10kWords.py ngram_test.txt -r hadoop > debug2\n",
    "\n",
    "##### run it on emr #####\n",
    "# s3 folder: s3://aws-logs-149687825236-us-east-1/elasticmapreduce/\n",
    "\n",
    "#!python HW5_4_Get10kWords.py ngram_test.txt -r emr --cleanup=NONE --no-output > debug\n",
    "\n",
    "#!python HW5_4_Get10kWords.py s3://filtered-5grams/ -r emr --cleanup=NONE --no-output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.4 - MrJob for Jaccard/Cosine Similarity\n",
    "- **Jaccard Similarity:** \n",
    "$$S_{A,B}=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|}$$\n",
    "\n",
    "\n",
    "- **Job 0** - for n-gram input, build the pseudo-doc (co-ocurrence matrix/word stripes) first\n",
    "  - **mapper**: emit *((w1,w2) ~ count)* for pair count\n",
    "  - **reducer**: obtain co-ocurrence matrix\n",
    "- **Job 1** - build inverted index for n-gram\n",
    "  - **mapper**: emit *((term,n_gram_id/doc_id) ~ 1)* for each word in the n-gram. \n",
    "   - **Note**: since the test file and n-gram file have different format, we will create two mapper to data from each, but both emits should have identical format \n",
    "  - **combiner**: local aggregation of co-ocurrence\n",
    "  - **partitioner**: -k1,1 -k2,2 apply secondary sorting\n",
    "  - **reducer**: summary to build inverted index\n",
    "- **NOTE**: for co-ocurrence matrix, the inverted indexing is its transpose, and because co-ocurrence is symmetric, inverted index is identical. Thus we can skip *Job 1* for synonym dection.\n",
    "- **Job 2** - obtain pairwise similarity for words from n-gram\n",
    "  - **mapper**: \n",
    "   - emit ((*,term) ~ 1) for each term in the inverted index, using order inversion to calculate $|A|$ and $|B|$\n",
    "   - emit ((A,B) ~ payload) for each *sorted* pair in the inverted index, to get all components for $|A\\cap B|$\n",
    "  - **combiner**: local aggregation\n",
    "  - **single reducer**: calcualte $S_{A,B}$, **or** use customized partitioner to have (*,word) available for all reducers.\n",
    "   - **Note**: since $S_{A,B}$ requires both $|A|$ and $|B|$, which are also needed in evaluating similarity for any other pairs that have them, it's impossible to have both norm in a realtime fashion, thus we cache them in the reducer. Hopefully this is not entirely unrealistic given that the amount of single word is relatively small.\n",
    "- **Job 3** - get top n similarities for synonym\n",
    "  - **mapper**: emit *($S_{A,B}$ ~ (A,B))*\n",
    "  - **partitioner**: -k1,1nr\n",
    "  - **reducer**: print out first n paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW5_4_Jaccard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW5_4_Jaccard.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from math import sqrt\n",
    "from subprocess import Popen, PIPE\n",
    " \n",
    "class Jaccard(MRJob):\n",
    "    \n",
    "    PARTITIONER='org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "    \n",
    "    #################################  job 0 - create co-occurrence matrix ##############################\n",
    "    \n",
    "    # job 0 mapper for SYSTEMS_TEST_DATASET\n",
    "    def j0_mapper_read_test(self, _, line):        \n",
    "        # time of mapper being called\n",
    "        self.increment_counter('HW5_4', 'mapper_test', 1)\n",
    "        # parse line, get doc id and terms\n",
    "        word, strip = line.strip().split(' ', 1)\n",
    "        cmd = 'strip = ' + strip\n",
    "        exec cmd\n",
    "        # emit co-occurrence matrix\n",
    "        yield word, strip\n",
    "            \n",
    "    # job 0 mapper_init: load our 9k favorite words, and evaluate those only\n",
    "    def j0_mapper_read_5gram_init(self):\n",
    "        localPath = '/Users/leiyang/GitHub/mids/w261/HW5-Questions/Top10kWords'\n",
    "        cat = Popen([\"cat\", 'Top10kWords'], stdout=PIPE)\n",
    "        self.corpus = [s.split()[0].strip('\"') for s in cat.stdout][:4500]\n",
    "        \n",
    "    # job 0 mapper for 5-gram: build pseudo-document (co-ocurrence matrix) & emit inverted index\n",
    "    def j0_mapper_read_5gram(self, _, line):\n",
    "        # parse line, get words and counts\n",
    "        grams, cnt, p_cnt, b_cnt = line.strip().split('\\t')\n",
    "        # only keep words from the 9k corpus\n",
    "        grams = grams.lower().split(' ')\n",
    "        grams = [x for x in grams if x in self.corpus]\n",
    "        n_gram = len(grams)\n",
    "        # emit co-ocurrence for each pair (MUST include all pairs to have correct inverted index)\n",
    "        for w1, w2 in [[grams[i], grams[j]] for i in range(n_gram) for j in range(n_gram)]:\n",
    "            yield (w1, w2), int(cnt)\n",
    "                        \n",
    "    # job 0 combiner - local aggregation of co-occurrence\n",
    "    def j0_combiner(self, pair, count):\n",
    "        yield (pair), sum(count)\n",
    "    \n",
    "    # job 0 reducer_init()\n",
    "    def j0_reducer_init(self):\n",
    "        self.current_term = None\n",
    "        self.current_strip = {}\n",
    "                        \n",
    "    # job 0 reducer\n",
    "    def j0_reducer(self, key, count):        \n",
    "        w1, w2 = key[0], key[1]     \n",
    "        if w1 == w2:\n",
    "            return\n",
    "        if self.current_term == w1:\n",
    "            # accumulate co-occurent words            \n",
    "            self.current_strip[w2] = sum(count)\n",
    "        else:\n",
    "            # yield previous word and stripe\n",
    "            if self.current_term:\n",
    "                yield self.current_term, self.current_strip\n",
    "            # reset new term\n",
    "            self.current_term = w1\n",
    "            self.current_strip = {w2:sum(count)}\n",
    "            \n",
    "    # job 0 reducer final - emit last word strip\n",
    "    def j0_reducer_final(self):\n",
    "        if self.current_term:\n",
    "            yield self.current_term, self.current_strip\n",
    "    \n",
    "    #################################  job 1 - create inverted indexing ##############################\n",
    "    \n",
    "    # job 1 mapper - build inverted index\n",
    "    def j1_mapper_jaccard(self, w1, stripe):\n",
    "        # here stripe is a dictionary \n",
    "        norm = sqrt(len(stripe))\n",
    "        for w2 in stripe:                        \n",
    "            yield (w2, w1), 1/norm\n",
    "            \n",
    "    def j1_mapper_cosine(self, w1, stripe):\n",
    "        # here stripe is a dictionary    \n",
    "        norm = sqrt(sum(pow(x,2) for x in stripe.values()))\n",
    "        for w2 in stripe:            \n",
    "            yield (w2, w1), stripe[w2]/norm\n",
    "    \n",
    "    # job 1 reducer_init()\n",
    "    def j1_reducer_init(self):\n",
    "        self.current_term = None\n",
    "        self.current_strip = {}\n",
    "                                \n",
    "    # job 1 reducer\n",
    "    def j1_reducer(self, pair, count):        \n",
    "        w2, w1 = pair[0], pair[1]\n",
    "        if self.current_term == w2:\n",
    "            # accumulate postings\n",
    "            self.current_strip[w1] = sum(count)\n",
    "        else:\n",
    "            # yield previous term and stripe\n",
    "            if self.current_term:\n",
    "                yield self.current_term, self.current_strip\n",
    "            # reset new term\n",
    "            self.current_term = w2\n",
    "            self.current_strip = {w1:sum(count)}\n",
    "                    \n",
    "    # job 1 reducer final - emit last index strip\n",
    "    def j1_reducer_final(self):\n",
    "        if self.current_term:\n",
    "            yield self.current_term, self.current_strip\n",
    "            \n",
    "    #################################  job 2 - evaluate similarity between words ##############################\n",
    "            \n",
    "    # job 2 mapper - emit pair-wise similarity from strips\n",
    "    def j2_mapper(self, term, postings):\n",
    "        # get all postings from generator\n",
    "        posts = postings.keys() # [p for p in postings]\n",
    "        posts.sort()\n",
    "        size = len(posts)        \n",
    "        # emit pairs on sorted stripe, so we only evaluate half of the symmetric relation\n",
    "        for w1, w2 in [[posts[i], posts[j]] for i in range(size) for j in range(i+1, size)]:\n",
    "            yield (w1, w2), postings[w1]*postings[w2]\n",
    "            \n",
    "    # job 2 reducer - get pair similarity\n",
    "    def j2_reducer(self, pair, prod):\n",
    "        # calculate similarity\n",
    "        yield pair, sum(prod)\n",
    "            \n",
    "    #################################  job 3 - rank pairwise similarities ##############################\n",
    "                   \n",
    "    # job 3 mapper - for secondary sort\n",
    "    def j3_mapper1(self, page, count):        \n",
    "        yield ('%s__%s' %(page[0], page[1]), count), None\n",
    "       \n",
    "    def j3_reducer_init1(self):\n",
    "        self.i = 0\n",
    "        self.n_freq = 1000\n",
    "    \n",
    "    def j3_reducer1(self, key, _):        \n",
    "        if self.i < self.n_freq:\n",
    "            self.i += 1\n",
    "            yield key     \n",
    "            \n",
    "    #################################  mrjob definition ##############################    \n",
    "    # MapReduce steps\n",
    "    def steps(self):\n",
    "        jobconf0 = {  #key value pairs            \n",
    "            'mapreduce.partition.keypartitioner.options': '-k1,1',            \n",
    "            'mapreduce.job.maps': '30',\n",
    "            'mapreduce.job.reduces': '30', \n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',\n",
    "        }\n",
    "        jobconf1 = {  #key value pairs            \n",
    "            'mapreduce.partition.keypartitioner.options': '-k1,1',            \n",
    "            'mapreduce.job.maps': '30',\n",
    "            'mapreduce.job.reduces': '30',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',\n",
    "        }\n",
    "        jobconf2 = {  #key value pairs            \n",
    "            'mapreduce.job.maps': '30',\n",
    "            'mapreduce.job.reduces': '50',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',\n",
    "        }\n",
    "        jobconf3 = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.maps': '15',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',\n",
    "        }\n",
    "        \n",
    "        # NOTE: DO NOT use jobconf when running with Python locally\n",
    "        return [\n",
    "                ######## job 0: get co-ocurrence matrix ########\n",
    "                ### for SYSTEMS_TEST_DATASET.txt ###\n",
    "                #MRStep(mapper=self.j0_mapper_read_test)\n",
    "                ### for n-gram file ###\n",
    "                MRStep(mapper_init=self.j0_mapper_read_5gram_init, mapper=self.j0_mapper_read_5gram\n",
    "                        , combiner=self.j0_combiner, reducer_init=self.j0_reducer_init\n",
    "                        , reducer=self.j0_reducer, reducer_final=self.j0_reducer_final\n",
    "                        , jobconf=jobconf0 \n",
    "                      )\n",
    "                ######## job 1: get inverted indexing ########\n",
    "                ,MRStep(mapper=self.j1_mapper_cosine                \n",
    "                        , reducer_init=self.j1_reducer_init\n",
    "                        , reducer=self.j1_reducer, reducer_final=self.j1_reducer_final\n",
    "                        , jobconf=jobconf1\n",
    "                      )\n",
    "                ######## job 2: calculate pair similarity between words ########\n",
    "                ,MRStep(mapper=self.j2_mapper, combiner=self.j2_reducer\n",
    "                        , reducer=self.j2_reducer\n",
    "                        , jobconf=jobconf2\n",
    "                      )\n",
    "                ######## job 3: sort similarities ########\n",
    "                ,MRStep(mapper=self.j3_mapper1, reducer_init=self.j3_reducer_init1\n",
    "                        , reducer=self.j3_reducer1\n",
    "                        , jobconf=jobconf3\n",
    "                       )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Jaccard.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute the Jaccard Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Jaccard.leiyang.20160216.033131.894013\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Jaccard.leiyang.20160216.033131.894013/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW5_4_Jaccard.leiyang.20160216.033131.894013/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar1801749633126551766/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob7045958771224949673.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar1954952526579236579/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3625224869642820494.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6299379533476481385/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3339382079611286408.jar tmpDir=null\n",
      "Counters from step 3:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar9196573010697758941/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6883121659836473509.jar tmpDir=null\n",
      "Counters from step 4:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW5_4_Jaccard.leiyang.20160216.033131.894013/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW5_4_Jaccard.leiyang.20160216.033131.894013\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW5_4_Jaccard.leiyang.20160216.033131.894013 from HDFS\n"
     ]
    }
   ],
   "source": [
    "##### run it locally via python #####\n",
    "#!python HW5_4_Jaccard.py SYSTEMS_TEST_DATASET.txt\n",
    "#!python HW5_4_Jaccard.py ngram_test.txt > debug\n",
    "\n",
    "##### run it locally on hadoop #####\n",
    "!python HW5_4_Jaccard.py ngram_test.txt --file Top10kWords -r hadoop > debug\n",
    "\n",
    "##### run it on emr #####\n",
    "#!python HW5_4_Jaccard.py s3://filtered-5grams/ --file Top10kWords -r emr --cleanup=NONE --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 5.5*\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python2.7\n",
    "''' pass a string to this funciton ( eg 'car') and it will give you a list of\n",
    "words which is related to cat, called lemma of CAT. '''\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import numpy as np\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of detection: 0.0050\n"
     ]
    }
   ],
   "source": [
    "text_file = open(\"HW5_4_results.txt\", \"r\")\n",
    "lines = text_file.readline().split('\\r')\n",
    "hits = []\n",
    "\n",
    "for line in lines:\n",
    "    pair = line.split('\\t')[0].split('__')\n",
    "    if len(pair) == 2:        \n",
    "        hits.append(pair[0] in synonyms(pair[1]) or pair[1] in synonyms(pair[0]))\n",
    "print 'Precision of detection: %.4f' %(sum(hits[:1000])/1000.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###*HW 5.5.1 (optional)*\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    ">> from nltk.corpus import stopwords\n",
    ">>> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "###*HW 5.6 (optional)*\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams.\n",
    "\n",
    "\n",
    "###*Hw 5.7 (optional)*\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping historyserver\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ stop historyserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 23891. Stop it first.\n",
      "localhost: nodemanager running as process 23992. Stop it first.\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 24141. Stop it first.\n",
      "localhost: datanode running as process 24237. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 24359. Stop it first.\n",
      "historyserver running as process 24467. Stop it first.\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
