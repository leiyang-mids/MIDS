{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework52.py\n",
    "## Author: Michael Kennedy\n",
    "## Description: Homework 4.3, simple MrJob most frequent pages\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "\n",
    "class urlJoiner(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(urlJoiner, self).configure_options()\n",
    "        self.add_passthrough_option('--join')\n",
    "    \n",
    "    \n",
    "    def load(self, filename):\n",
    "        table = {}\n",
    "        with open(filename,'r') as rf:\n",
    "            for line in rf:\n",
    "                splits = line.strip().split(',')\n",
    "                if len(splits)==5:\n",
    "                    log_type, key, value, pagename, url  = splits[0:5]\n",
    "                    if log_type == 'A':\n",
    "                        table[key] = (pagename, url)\n",
    "        return table\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.metatable = self.load(\"anonymous-msweb.data\")\n",
    "        self.join = self.options.join\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip().split(',')\n",
    "        visit, pageID, number, customer, visitorID = fields\n",
    "        if self.join == \"inner\" or self.join == \"right\":\n",
    "            if pageID in self.metatable:\n",
    "                yield pageID, (self.metatable[pageID][1], visitorID)\n",
    "        elif self.join == \"left\":\n",
    "            if pageID in self.metatable:\n",
    "                yield pageID, (self.metatable[pageID][1], visitorID)\n",
    "            else:\n",
    "                yield pageID, (\"Unknown URL\", visitorID)\n",
    "\n",
    "            \n",
    "\n",
    "#     def combiner(self, word, counts):\n",
    "#         yield word, sum(counts)\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.join = self.options.join\n",
    "        if self.join == \"right\":\n",
    "            self.metatable = self.load(\"anonymous-msweb.data\")\n",
    "            self.visited = {x:False for x in self.metatable.keys()}\n",
    "            \n",
    "    def reducer(self, pageID, data):\n",
    "        if self.join == \"right\":\n",
    "            self.visited[pageID] = True\n",
    "        for x in data:\n",
    "            yield pageID, x\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        if self.join == \"right\":\n",
    "            for key, value in self.visited.iteritems():\n",
    "                if value == False:\n",
    "                    yield key, (self.metatable[key][1], \"Unknown visitor ID\")\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    urlJoiner.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from homework52 import urlJoiner\n",
    "\n",
    "def runJoin(joinType):\n",
    "\n",
    "    mr_job = urlJoiner(args=['clean-msweb.data', '--file', 'anonymous-msweb.data', '--join', joinType])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "outInner = runJoin('inner')\n",
    "outLeft = runJoin('left')\n",
    "outRight = runJoin('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows resulting from join type:\n",
      "\n",
      "inner  98,654\n",
      "left   98,654\n",
      "right  98,663\n"
     ]
    }
   ],
   "source": [
    "print \"Rows resulting from join type:\\n\"\n",
    "for joinType in ['inner', 'left', 'right']:\n",
    "    if joinType == 'inner': out = outInner\n",
    "    elif joinType == 'left': out = outLeft\n",
    "    elif joinType == 'right': out = outRight\n",
    "    \n",
    "    print \"{:7s}{:>4,d}\".format(joinType, len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework53.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework53.py\n",
    "## Author: Michael Kennedy\n",
    "## Description: Homework 5.3, simple MrJob longest 5gram\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "\n",
    "class longFinder(MRJob):\n",
    "    \n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.longest = (0, \"\")\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        ngram = fields[0]\n",
    "        if len(ngram)> self.longest[0]:\n",
    "            self.longest = (len(ngram), ngram)\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        yield self.longest[0], self.longest[1]\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.longest = (0, \"\")\n",
    "            \n",
    "    def reducer(self, local_max, payload):\n",
    "        if local_max > self.longest[0]:\n",
    "            self.longest = (local_max, payload[0])\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        yield self.longest[0], self.longest[1]\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    longFinder.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/02/17 02:54:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from homework53 import longFinder\n",
    "\n",
    "def runEDA():\n",
    "\n",
    "    mr_job = longFinder(args=['-r', 'hadoop', 'hdfs:///user/hadoop/ngrams/'])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "runEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework53wc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework53wc.py\n",
    "## Author: Michael Kennedy\n",
    "## Description: Homework 5.3, simple MrJob longest 5gram\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "\n",
    "class wordCount(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        count = fields[1]\n",
    "        ngram = fields[0]\n",
    "        words = ngram.split(\" \")\n",
    "        for word in words:\n",
    "            yield word, int(count)\n",
    "\n",
    "    def combiner(self, word, wc):\n",
    "        yield word, sum(wc)\n",
    "        \n",
    "            \n",
    "    def reducer(self, word, wc):\n",
    "        yield word, sum(wc)\n",
    "        \n",
    "    def sort_mapper(self, key, value):\n",
    "        yield value, key\n",
    "    \n",
    "    def sort_reducer_init(self):\n",
    "        self.count = 0\n",
    "    \n",
    "    def sort_reducer(self, key, values):\n",
    "        if self.count > 10:\n",
    "            pass\n",
    "        else:\n",
    "            self.count += 1\n",
    "            for value in values:\n",
    "                yield value, key\n",
    "                \n",
    "    def steps(self):        \n",
    "        sort_conf = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "        \n",
    "        return [   \n",
    "            # Data aggregation\n",
    "            MRStep(mapper=self.mapper,\n",
    "               combiner=self.combiner,\n",
    "               reducer=self.reducer),\n",
    "            # Secondary sort\n",
    "            MRStep(mapper=self.sort_mapper,\n",
    "               reducer_init=self.sort_reducer_init,\n",
    "               reducer=self.sort_reducer,\n",
    "               jobconf=sort_conf)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wordCount.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from homework53wc import wordCount\n",
    "\n",
    "def runEDA():\n",
    "# Run on hadoop cluster\n",
    "    mr_job = wordCount(args=['-r', 'hadoop', 'hdfs:///user/hadoop/ngrams/', '--output-dir', 'hdfs:///user/hadoop/wc/'])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "sortedWords = runEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/homework53wc.hadoop.20160219.043919.739398\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /tmp/homework53wc.hadoop.20160219.043919.739398/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/homework53wc.hadoop.20160219.043919.739398/step-0-mapper-sorted\n",
      "> sort /tmp/homework53wc.hadoop.20160219.043919.739398/step-0-mapper_part-00000\n",
      "writing to /tmp/homework53wc.hadoop.20160219.043919.739398/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "writing to /tmp/homework53wc.hadoop.20160219.043919.739398/step-1-mapper_part-00000\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "writing to /tmp/homework53wc.hadoop.20160219.043919.739398/step-1-mapper_part-00001\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /tmp/homework53wc.hadoop.20160219.043919.739398/step-1-mapper-sorted\n",
      "> sort /tmp/homework53wc.hadoop.20160219.043919.739398/step-1-mapper_part-00000 /tmp/homework53wc.hadoop.20160219.043919.739398/step-1-mapper_part-00001\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.map.output.key.field.separator: map.output.key.field.separator\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "writing to /tmp/homework53wc.hadoop.20160219.043919.739398/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /tmp/homework53wc.hadoop.20160219.043919.739398/step-1-reducer_part-00000 -> /tmp/homework53wc.hadoop.20160219.043919.739398/output/part-00000\n",
      "removing tmp directory /tmp/homework53wc.hadoop.20160219.043919.739398\n"
     ]
    }
   ],
   "source": [
    "!python homework53wc.py --no-output filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework53density.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework53density.py\n",
    "## Author: Michael Kennedy\n",
    "## Description: Homework 5.3, simple MrJob longest 5gram\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "\n",
    "class densityFinder(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        ngram, count, pages= fields[0:3]\n",
    "        words = ngram.split(\" \")\n",
    "        for word in words:\n",
    "            yield word, (count, pages)\n",
    "\n",
    "    def combiner(self, word, data):\n",
    "        count = 0\n",
    "        pages = 0\n",
    "        for each in data:\n",
    "            count += each[0]\n",
    "            pages += each[0]\n",
    "        yield word, (count,pages)\n",
    "        \n",
    "            \n",
    "    def reducer(self, word, data):\n",
    "        count = 0\n",
    "        pages = 0\n",
    "        for each in data:\n",
    "            count += each[0]\n",
    "            pages += each[0]\n",
    "        yield word, count/float(pages)\n",
    "        \n",
    "    def sort_mapper(self, key, value):\n",
    "        yield float(value), key\n",
    "    \n",
    "    def sort_reducer_init(self):\n",
    "        self.count = 0\n",
    "    \n",
    "    def sort_reducer(self, key, values):\n",
    "#         if self.count > 10:\n",
    "#             pass\n",
    "#         else:\n",
    "#             self.count += 1\n",
    "#             for val in values:\n",
    "        yield value, key\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    densityFinder.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-11f6a911b7f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mrunEDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-11f6a911b7f3>\u001b[0m in \u001b[0;36mrunEDA\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Run MRJob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Write stream_output to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_job_files_for_upload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_upload_local_files_to_hdfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_job_in_hadoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_input_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run_job_in_hadoop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m                     \u001b[1;31m# reading from master gives us the subprocess's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                     \u001b[1;31m# stderr and stdout (it's a fake terminal)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_stderr_from_streaming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturncode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                     \u001b[0mmaster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_process_stderr_from_streaming\u001b[1;34m(self, stderr)\u001b[0m\n\u001b[0;32m    386\u001b[0m                         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtreat_eio_as_eof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHADOOP_STREAMING_OUTPUT_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HADOOP: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36mtreat_eio_as_eof\u001b[1;34m(iter)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[1;32myield\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# okay for StopIteration to bubble up\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEIO\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from homework53density import densityFinder\n",
    "\n",
    "def runEDA():\n",
    "\n",
    "    mr_job = densityFinder(args=['-r', 'hadoop', 'hdfs:///user/hadoop/ngrams/'])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "density = runEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile homework53dist.py\n",
    "## Author: Michael Kennedy\n",
    "## Description: Homework 5.3, simple MrJob longest 5gram\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "\n",
    "class wc_dist(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        count = fields[1]\n",
    "        ngram = fields[0]\n",
    "        words = ngram.split(\" \")\n",
    "        for word in words:\n",
    "            yield int(count), 1\n",
    "\n",
    "    def combiner(self, word, wc):\n",
    "        yield word, sum(wc)\n",
    "     \n",
    "    def reducer(self, word, wc):\n",
    "        yield word, sum(wc)\n",
    "        \n",
    "    def sort_mapper(self, key, value):\n",
    "        yield value, key\n",
    "    \n",
    "    def sort_reducer_init(self):\n",
    "        self.count = 0\n",
    "    \n",
    "    def sort_reducer(self, key, values):\n",
    "        if self.count > 10:\n",
    "            pass\n",
    "        else:\n",
    "            self.count += 1\n",
    "            for value in values:\n",
    "                yield value, key\n",
    "                \n",
    "    def steps(self):        \n",
    "        sort_conf = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "        \n",
    "        return [   \n",
    "            # Data aggregation\n",
    "            MRStep(mapper=self.mapper,\n",
    "               combiner=self.combiner,\n",
    "               reducer=self.reducer),\n",
    "            # Secondary sort\n",
    "            MRStep(mapper=self.sort_mapper,\n",
    "               reducer_init=self.sort_reducer_init,\n",
    "               reducer=self.sort_reducer,\n",
    "               jobconf=sort_conf)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wordCount.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from homework53dist import wc_dist\n",
    "\n",
    "def runEDA():\n",
    "\n",
    "    mr_job = wc_dist(args=['-r', 'hadoop', 'hdfs:///user/hadoop/ngrams/'])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "            \n",
    "    \n",
    "    return output\n",
    "            \n",
    "runEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework54.py\n",
    "## Author: Michael Kennedy\n",
    "## Description: Homework 5.4, stripe&similarity metric\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "import math\n",
    "\n",
    "class similarity(MRJob):\n",
    "    \n",
    "    def pull_vocab(self):\n",
    "        s = \"Saturday\", \"morning\", \"evening\", \"afternoon\"\n",
    "        return s\n",
    "    \n",
    "    def map_index_init(self):\n",
    "        self.vocab = self.pull_vocab()\n",
    "    \n",
    "    def map_index(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        count = int(fields[1])\n",
    "        ngram = fields[0]\n",
    "        words = ngram.split(\" \")\n",
    "        for firstword in words:\n",
    "            for secondword in words:\n",
    "                if firstword in self.vocab and secondword in self.vocab:\n",
    "                    if firstword == secondword:\n",
    "                        pass\n",
    "                    else: yield firstword, {secondword:1}\n",
    "     \n",
    "    def combine_index(self, key, data):\n",
    "        reference_dict = data.next()\n",
    "        for d in data:\n",
    "            for k, v in d.iteritems():\n",
    "                reference_dict[k] = reference_dict.setdefault(k,0)+v\n",
    "        yield key, reference_dict\n",
    "            \n",
    "    def reduce_index(self, key, data):\n",
    "        reference_dict = data.next()\n",
    "        for d in data:\n",
    "            for k, v in d.iteritems():\n",
    "                reference_dict[k] = reference_dict.setdefault(k,0)+v\n",
    "        yield key, reference_dict\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(similarity, self).configure_options()\n",
    "        self.add_passthrough_option('--metric')\n",
    "        \n",
    "    def map_combinations(self, word, stripe):\n",
    "        for k1, v1 in stripe.iteritems():\n",
    "            for k2, v2 in stripe.iteritems():\n",
    "                if k1<k2:\n",
    "                    yield (k1,k2), (v1, v2)\n",
    "            \n",
    "    def calculate_euclidean(self, vector_list):\n",
    "        return math.sqrt(sum([(x-y)**2 for x, y in vector_list]))\n",
    "    \n",
    "    def calculate_cosine(self, vector_list):\n",
    "        magnitude_a = math.sqrt(sum([x**2 for x in a.itervalues()]))\n",
    "        magnitude_b = math.sqrt(sum([x**2 for x in b.itervalues()]))\n",
    "        dot_product = 0\n",
    "        for item, value in a:\n",
    "            if item in b:\n",
    "                dot_product += value*b[item]\n",
    "        return dot_product / magnitude_a * magnitude_b\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        if self.options.metric == 'euclidean':\n",
    "            self.calculate_similarity = self.calculate_euclidean\n",
    "        elif self.options.metric == 'cosine':\n",
    "            self.calculate_similarity = self.calculate_cosine\n",
    "        else: self.calculate_similarity = self.calculate_euclidean #default euclidean\n",
    "   \n",
    "\n",
    "    def similarity_reducer(self, pair, coincidence_vector):\n",
    "        similarity = self.calculate_similarity(coincidence_vector)\n",
    "        yield \",\".join(pair), similarity\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.map_index_init,\n",
    "                        mapper = self.map_index,\n",
    "                        combiner = self.combine_index,\n",
    "                        reducer = self.reduce_index\n",
    "                       ),\n",
    "               MRStep(mapper=self.map_combinations,\n",
    "                       reducer_init=self.reducer_init,\n",
    "                       reducer=self.similarity_reducer\n",
    "                      )]\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    similarity.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/18 16:57:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/homework54.hadoop.20160218.225715.986346\n",
      "writing wrapper script to /tmp/homework54.hadoop.20160218.225715.986346/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files into hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160218.225715.986346/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar4894620266088176756/] [] /tmp/streamjob480649071154284010.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at spark1b/10.54.199.69:8032\n",
      "HADOOP: Connecting to ResourceManager at spark1b/10.54.199.69:8032\n",
      "HADOOP: Total input paths to process : 190\n",
      "HADOOP: number of splits:190\n",
      "HADOOP: Submitting tokens for job: job_1455698641309_0031\n",
      "HADOOP: Submitted application application_1455698641309_0031\n",
      "HADOOP: The url to track the job: http://spark1:8088/proxy/application_1455698641309_0031/\n",
      "HADOOP: Running job: job_1455698641309_0031\n",
      "HADOOP: Job job_1455698641309_0031 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 1% reduce 0%\n",
      "HADOOP:  map 2% reduce 0%\n",
      "HADOOP:  map 3% reduce 0%\n",
      "HADOOP:  map 5% reduce 0%\n",
      "HADOOP:  map 6% reduce 0%\n",
      "HADOOP:  map 7% reduce 0%\n",
      "HADOOP:  map 9% reduce 0%\n",
      "HADOOP:  map 10% reduce 0%\n",
      "HADOOP:  map 11% reduce 2%\n",
      "HADOOP:  map 13% reduce 3%\n",
      "HADOOP:  map 14% reduce 3%\n",
      "HADOOP:  map 15% reduce 4%\n",
      "HADOOP:  map 16% reduce 4%\n",
      "HADOOP:  map 17% reduce 4%\n",
      "HADOOP:  map 18% reduce 4%\n",
      "HADOOP:  map 19% reduce 6%\n",
      "HADOOP:  map 20% reduce 6%\n",
      "HADOOP:  map 21% reduce 6%\n",
      "HADOOP:  map 22% reduce 6%\n",
      "HADOOP:  map 22% reduce 7%\n",
      "HADOOP:  map 23% reduce 7%\n",
      "HADOOP:  map 23% reduce 8%\n",
      "HADOOP:  map 24% reduce 8%\n",
      "HADOOP:  map 25% reduce 8%\n",
      "HADOOP:  map 26% reduce 8%\n",
      "HADOOP:  map 26% reduce 9%\n",
      "HADOOP:  map 27% reduce 9%\n",
      "HADOOP:  map 28% reduce 9%\n",
      "HADOOP:  map 29% reduce 9%\n",
      "HADOOP:  map 31% reduce 9%\n",
      "HADOOP:  map 32% reduce 9%\n",
      "HADOOP:  map 32% reduce 10%\n",
      "HADOOP:  map 33% reduce 10%\n",
      "HADOOP:  map 35% reduce 11%\n",
      "HADOOP:  map 36% reduce 11%\n",
      "HADOOP:  map 37% reduce 11%\n",
      "HADOOP:  map 38% reduce 12%\n",
      "HADOOP:  map 39% reduce 12%\n",
      "HADOOP:  map 40% reduce 12%\n",
      "HADOOP:  map 41% reduce 13%\n",
      "HADOOP:  map 42% reduce 13%\n",
      "HADOOP:  map 43% reduce 14%\n",
      "HADOOP:  map 44% reduce 14%\n",
      "HADOOP:  map 45% reduce 14%\n",
      "HADOOP:  map 46% reduce 15%\n",
      "HADOOP:  map 47% reduce 15%\n",
      "HADOOP:  map 48% reduce 15%\n",
      "HADOOP:  map 49% reduce 15%\n",
      "HADOOP:  map 50% reduce 15%\n",
      "HADOOP:  map 50% reduce 16%\n",
      "HADOOP:  map 51% reduce 16%\n",
      "HADOOP:  map 52% reduce 16%\n",
      "HADOOP:  map 54% reduce 17%\n",
      "HADOOP:  map 55% reduce 17%\n",
      "HADOOP:  map 56% reduce 18%\n",
      "HADOOP:  map 57% reduce 18%\n",
      "HADOOP:  map 57% reduce 19%\n",
      "HADOOP:  map 58% reduce 19%\n",
      "HADOOP:  map 59% reduce 19%\n",
      "HADOOP:  map 60% reduce 19%\n",
      "HADOOP:  map 61% reduce 19%\n",
      "HADOOP:  map 63% reduce 20%\n",
      "HADOOP:  map 64% reduce 20%\n",
      "HADOOP:  map 65% reduce 21%\n",
      "HADOOP:  map 66% reduce 21%\n",
      "HADOOP:  map 66% reduce 22%\n",
      "HADOOP:  map 67% reduce 22%\n",
      "HADOOP:  map 68% reduce 22%\n",
      "HADOOP:  map 69% reduce 22%\n",
      "HADOOP:  map 70% reduce 23%\n",
      "HADOOP:  map 71% reduce 23%\n",
      "HADOOP:  map 72% reduce 24%\n",
      "HADOOP:  map 73% reduce 24%\n",
      "HADOOP:  map 74% reduce 24%\n",
      "HADOOP:  map 75% reduce 24%\n",
      "HADOOP:  map 76% reduce 24%\n",
      "HADOOP:  map 77% reduce 24%\n",
      "HADOOP:  map 78% reduce 25%\n",
      "HADOOP:  map 79% reduce 25%\n",
      "HADOOP:  map 79% reduce 26%\n",
      "HADOOP:  map 80% reduce 26%\n",
      "HADOOP:  map 82% reduce 26%\n",
      "HADOOP:  map 83% reduce 27%\n",
      "HADOOP:  map 84% reduce 27%\n",
      "HADOOP:  map 85% reduce 28%\n",
      "HADOOP:  map 86% reduce 28%\n",
      "HADOOP:  map 87% reduce 29%\n",
      "HADOOP:  map 88% reduce 29%\n",
      "HADOOP:  map 89% reduce 29%\n",
      "HADOOP:  map 90% reduce 29%\n",
      "HADOOP:  map 91% reduce 29%\n",
      "HADOOP:  map 92% reduce 29%\n",
      "HADOOP:  map 93% reduce 30%\n",
      "HADOOP:  map 94% reduce 30%\n",
      "HADOOP:  map 96% reduce 31%\n",
      "HADOOP:  map 97% reduce 31%\n",
      "HADOOP:  map 97% reduce 32%\n",
      "HADOOP:  map 98% reduce 32%\n",
      "HADOOP:  map 99% reduce 33%\n",
      "HADOOP:  map 100% reduce 33%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1455698641309_0031 completed successfully\n",
      "HADOOP: Counters: 52\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=32357\n",
      "HADOOP: \t\tFILE: Number of bytes written=23440261\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=2156095986\n",
      "HADOOP: \t\tHDFS: Number of bytes written=248\n",
      "HADOOP: \t\tHDFS: Number of read operations=573\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tKilled map tasks=3\n",
      "HADOOP: \t\tLaunched map tasks=191\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tOther local map tasks=2\n",
      "HADOOP: \t\tData-local map tasks=39\n",
      "HADOOP: \t\tRack-local map tasks=150\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=3683252\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=183318\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=3683252\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=183318\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all map tasks=3683252\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all reduce tasks=183318\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all map tasks=3771650048\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all reduce tasks=187717632\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=58682266\n",
      "HADOOP: \t\tMap output records=3144\n",
      "HADOOP: \t\tMap output bytes=81884\n",
      "HADOOP: \t\tMap output materialized bytes=33491\n",
      "HADOOP: \t\tInput split bytes=26870\n",
      "HADOOP: \t\tCombine input records=3144\n",
      "HADOOP: \t\tCombine output records=729\n",
      "HADOOP: \t\tReduce input groups=4\n",
      "HADOOP: \t\tReduce shuffle bytes=33491\n",
      "HADOOP: \t\tReduce input records=729\n",
      "HADOOP: \t\tReduce output records=4\n",
      "HADOOP: \t\tSpilled Records=1458\n",
      "HADOOP: \t\tShuffled Maps =190\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=190\n",
      "HADOOP: \t\tGC time elapsed (ms)=47335\n",
      "HADOOP: \t\tCPU time spent (ms)=607280\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=50278617088\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=400773570560\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=38370017280\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=2156069116\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=248\n",
      "HADOOP: Output directory: hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160218.225715.986346/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar8572989202702195975/] [] /tmp/streamjob5863523181913951296.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at spark1b/10.54.199.69:8032\n",
      "HADOOP: Connecting to ResourceManager at spark1b/10.54.199.69:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1455698641309_0032\n",
      "HADOOP: Submitted application application_1455698641309_0032\n",
      "HADOOP: The url to track the job: http://spark1:8088/proxy/application_1455698641309_0032/\n",
      "HADOOP: Running job: job_1455698641309_0032\n",
      "HADOOP: Job job_1455698641309_0032 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1455698641309_0032 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=456\n",
      "HADOOP: \t\tFILE: Number of bytes written=367010\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=690\n",
      "HADOOP: \t\tHDFS: Number of bytes written=227\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tRack-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=6838\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=4057\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=6838\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=4057\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all map tasks=6838\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all reduce tasks=4057\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all map tasks=7002112\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all reduce tasks=4154368\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=4\n",
      "HADOOP: \t\tMap output records=12\n",
      "HADOOP: \t\tMap output bytes=426\n",
      "HADOOP: \t\tMap output materialized bytes=462\n",
      "HADOOP: \t\tInput split bytes=318\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=6\n",
      "HADOOP: \t\tReduce shuffle bytes=462\n",
      "HADOOP: \t\tReduce input records=12\n",
      "HADOOP: \t\tReduce output records=6\n",
      "HADOOP: \t\tSpilled Records=24\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=243\n",
      "HADOOP: \t\tCPU time spent (ms)=2140\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=679993344\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=6292525056\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=525336576\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=372\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=227\n",
      "HADOOP: Output directory: hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160218.225715.986346/output\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160218.225715.986346/output\n",
      "\"Saturday,afternoon\"\t96.88137075826292\n",
      "\"Saturday,evening\"\t411.0498753192853\n",
      "\"Saturday,morning\"\t514.6338504218314\n",
      "\"afternoon,evening\"\t507.5588635813584\n",
      "\"afternoon,morning\"\t494.87170054469675\n",
      "\"evening,morning\"\t103.58571330062848\n",
      "STDERR: 16/02/18 17:01:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /tmp/homework54.hadoop.20160218.225715.986346\n",
      "deleting hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160218.225715.986346 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -rm -f -r hdfs:///user/hadoop/stripes\n",
    "#!python homework54.py -r hadoop hdfs:///user/hadoop/ngrams/ --output-dir hdfs:///user/hadoop/stripes\n",
    "!python homework54.py -r hadoop hdfs:///user/hadoop/ngrams/ --no-output\n",
    "#!python homework54.py filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"homework54.py\", line 80, in <module>\r\n",
      "    stripe_former.run()\r\n",
      "  File \"/usr/lib/python2.7/site-packages/mrjob/job.py\", line 460, in run\r\n",
      "    mr_job = cls(args=_READ_ARGS_FROM_SYS_ARGV)\r\n",
      "  File \"/usr/lib/python2.7/site-packages/mrjob/job.py\", line 99, in __init__\r\n",
      "    super(MRJob, self).__init__(self.mr_job_script(), args)\r\n",
      "  File \"/usr/lib/python2.7/site-packages/mrjob/launch.py\", line 97, in __init__\r\n",
      "    self.configure_options()\r\n",
      "  File \"homework54.py\", line 43, in configure_options\r\n",
      "    super(similarity, self).configure_options()\r\n",
      "NameError: global name 'similarity' is not defined\r\n"
     ]
    }
   ],
   "source": [
    "!python homework54.py -r hadoop hdfs:///user/hadoop/ngrams/ --output-dir hdfs:///user/hadoop/stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.hadoop:Job failed with return code 1280: ['/home/hadoop/hadoop/bin/hadoop', 'jar', '/home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar', '-files', 'hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/homework54.py#homework54.py,hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/mrjob.tar.gz#mrjob.tar.gz', '-input', 'hdfs:///user/hadoop/ngrams/', '-input', 'hdfs:///user/hadoop/stripes', '-output', 'hdfs:///user/hadoop/utput-dir', '-mapper', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --mapper', '-combiner', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --combiner', '-reducer', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --reducer']\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/home/hadoop/hadoop/bin/hadoop', 'jar', '/home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar', '-files', 'hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/homework54.py#homework54.py,hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/mrjob.tar.gz#mrjob.tar.gz', '-input', 'hdfs:///user/hadoop/ngrams/', '-input', 'hdfs:///user/hadoop/stripes', '-output', 'hdfs:///user/hadoop/utput-dir', '-mapper', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --mapper', '-combiner', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --combiner', '-reducer', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --reducer']' returned non-zero exit status 1280",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-4968deac4ec2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#     return output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mrunStripes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-132-4968deac4ec2>\u001b[0m in \u001b[0;36mrunStripes\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# Run MRJob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Write stream_output to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_job_files_for_upload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_upload_local_files_to_hdfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_job_in_hadoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_input_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run_job_in_hadoop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    370\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause_msg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_stderr_from_streaming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['/home/hadoop/hadoop/bin/hadoop', 'jar', '/home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar', '-files', 'hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/homework54.py#homework54.py,hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/hadoop/tmp/mrjob/homework54.hadoop.20160217.233057.493100/files/mrjob.tar.gz#mrjob.tar.gz', '-input', 'hdfs:///user/hadoop/ngrams/', '-input', 'hdfs:///user/hadoop/stripes', '-output', 'hdfs:///user/hadoop/utput-dir', '-mapper', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --mapper', '-combiner', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --combiner', '-reducer', 'sh -ex setup-wrapper.sh python homework54.py --step-num=0 --reducer']' returned non-zero exit status 1280"
     ]
    }
   ],
   "source": [
    "from homework54 import stripe_former\n",
    "\n",
    "def runStripes():\n",
    "#     mr_job = stripe_former(args=[\"filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt\"])\n",
    "    mr_job = stripe_former(args=['-r', 'hadoop', 'hdfs:///user/hadoop/ngrams/', '--output-dir', 'hdfs:///user/hadoop/stripes'])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "#         for line in runner.stream_output():\n",
    "#             output.append(mr_job.parse_output_line(line))\n",
    "            \n",
    "    \n",
    "#     return output\n",
    "            \n",
    "runStripes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework54similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework54similarity.py\n",
    "## Author: Michael Kennedy\n",
    "## Description: Homework 5.4, stripe&similarity metric\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "import math\n",
    "\n",
    "\n",
    "class similarity(MRJob):\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(similarity, self).configure_options()\n",
    "        self.add_passthrough_option('--metric')\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        word = eval(fields[0])\n",
    "        stripe = eval(fields[1])\n",
    "        del stripe[word]\n",
    "        yield word, (word, stripe)\n",
    "        for other_word in stripe:\n",
    "            yield word, (other_word, stripe)\n",
    "            \n",
    "    def calculate_euclidean(self, a, b):\n",
    "        similarity = math.sqrt(sum((a.get(k,0) - b.get(k,0))**2 for k in set(a.keys()).intersection(set(b.keys()))))\n",
    "        return similarity\n",
    "    \n",
    "    def calculate_cosine(self, a, b):\n",
    "        magnitude_a = math.sqrt(sum([x**2 for x in a.itervalues()]))\n",
    "        magnitude_b = math.sqrt(sum([x**2 for x in b.itervalues()]))\n",
    "        dot_product = 0\n",
    "        for item, value in a:\n",
    "            if item in b:\n",
    "                dot_product += value*b[item]\n",
    "        return dot_product / magnitude_a * magnitude_b\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        if self.options.metric == 'euclidean':\n",
    "            self.calculate_similarity = self.calculate_euclidean\n",
    "        elif self.options.metric == 'cosine':\n",
    "            self.calculate_similarity = self.calculate_cosine\n",
    "        else: self.calculate_similarity = self.calculate_euclidean #default euclidean\n",
    "   \n",
    "\n",
    "    def reducer(self, word, stripes):\n",
    "        reference_stripe = False\n",
    "        compare_stripes = []\n",
    "        for stripe in stripes:\n",
    "            if stripe[0]==word: # Separate the reference stripe\n",
    "                reference_stripe = stripe\n",
    "            else: \n",
    "                compare_stripes[stripe[0]]\n",
    "        for second_stripe in compare_stripes:\n",
    "            second_stripe_name = second_stripe[0]\n",
    "            second_stripe_data = second_stripe[1]\n",
    "            score = self.calculate_similarity(second_stripe[1],second_stripe_data)\n",
    "            yield reference_stripe[0], (second_stripe_name,  score)\n",
    "                \n",
    "#     def sort_mapper(self, key, value):\n",
    "#         yield value, key\n",
    "    \n",
    "#     def sort_reducer_init(self):\n",
    "#         self.count = 0\n",
    "    \n",
    "#     def sort_reducer(self, key, values):\n",
    "#         if self.count > 10:\n",
    "#             pass\n",
    "#         else:\n",
    "#             self.count += 1\n",
    "#             for value in values:\n",
    "#                 yield value, key\n",
    "                \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    similarity.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from homework54similarity import similarity\n",
    "\n",
    "def runStripes():\n",
    "#     mr_job = similarity(args=[\"filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt\"])\n",
    "    mr_job = similarity(args=['-r', 'hadoop', 'hdfs:///user/hadoop/utput-dir/part-00000', '--metric', 'euclidean'])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "#         for line in runner.stream_output():\n",
    "#             output.append(mr_job.parse_output_line(line))\n",
    "            \n",
    "    \n",
    "#     return output\n",
    "            \n",
    "runStripes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
