{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#UC Berkeley MIDS DATASCI W261-02, Machine Learning at Scale\n",
    "##Assignement #1  (version 2016-01-14)\n",
    "## Submitted by Lei Yang ([leiyang@berkeley.edu](mailto:leiyang@berkeley)), 2016-01-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.0.0*: Define big data. Provide an example of a big data problem in your domain of expertise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Answer: \n",
    "Big data is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.0.1*: In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Answer:\n",
    "blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.1*: Read through the provided control script (pNaiveBayes.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.2*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's define pNaiveBayes.sh script first, we only need to do this once since it is the same throughout HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "#echo \"$countfiles\"\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py & reducer.py, and make them executable\n",
    "- mapper.py counts the single specified word for the chunk, and output an integer\n",
    "- reducer.py collates counts from all chunks, and output the total count of the single specified word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "countword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        for word in line.lower().split()[2:]:\n",
    "            if countword in word:\n",
    "                count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for filename in sys.argv[1:]:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            sum += int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">Results: </span>by checking the ouput file, we know there are 10 counts of word 'assistance'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "## display final output results\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.3*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py:\n",
    "- obtains count for each word from the chunk, for spam and non-spam email separately, \n",
    "- records all counts in a dictionary, \n",
    "- outputs the dictionaries (non-spam count, and spam count), (non)spam counts, and keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keyword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keyword + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define reducer.py:\n",
    "- collapse wrod counts from all chunks\n",
    "- estimate NB model parameters: prior and conditional probabilities\n",
    "- classify messages that contains the keyword\n",
    "- **Note:** for messages that don't contain the keyword, the decision is solely based on prior probability, which will always give non-spam prediction, thus we skip those messages and only focus on those with the specified keyword\n",
    "- output results\n",
    "\n",
    "####Parameter estimation background:\n",
    "Assuming *positional independence*, and with *add-one Laplace smoothing*, the multinomial NB conditional probability $P(t | c)$ can be estimated as:\n",
    "$$\n",
    "\\hat{P}(t\\mid c)=\\frac{T_{ct}+1}{(\\sum_{t^\\prime \\in V}{T_{ct^\\prime}})+B},\n",
    "$$\n",
    "\n",
    "where $B=|V|$ is the number of terms in the vocabulary $V$ (including all text classes), and $T_{ct}$ is the count of word *t* in class *c*. \n",
    "\n",
    "To classify a message, the posterior probability of class $c$ can be calculated as:\n",
    "$$\n",
    "c_{map}=\\arg\\max_{c\\in\\mathbb C}[\\log{\\hat{P}(c)}+\\sum_{1\\leqslant k \\leqslant n_d}{\\log{\\hat{P}(t_k\\mid c)}}],\n",
    "$$\n",
    "where $\\hat{P}(t_k\\mid c)$ is estimated above with *positional independence* assumption as $\\hat{P}(t\\mid c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keyword = counts[4]\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with key word: ' + keyword\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "p_word_s = 1.0*((s_count[keyword] if keyword in s_count else 0) + 1) / (tot_s + B)\n",
    "p_word_n = 1.0*((n_count[keyword] if keyword in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### prior probability: same for every message, since it's determined by training data ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "# print model parameters\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "print 'P(%s|spam) = %f' %(keyword, p_word_s)\n",
    "print 'P(%s|non-spam) = %f' %(keyword, p_word_n)\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'isCorrect \\t count \\t P(spam) \\t P(non-spam) \\t Result \\t message_id'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        n_word = sum([1 if keyword in word else 0 for word in words])\n",
    "        # if the message doesn't contain our keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        #### posterior probability ####\n",
    "        p_s_word = math.log(p_s) + n_word * math.log(p_word_s)\n",
    "        p_n_word = math.log(p_n) + n_word * math.log(p_word_n)\n",
    "        isSpam = True if p_s_word > p_n_word else False\n",
    "        isCorrect = '1' if isSpam == int(msg[1]) else '0'\n",
    "        # print results\n",
    "        print isCorrect + '\\t'+str(n_word)+'\\t'+ str(p_s_word) + '\\t' + str(p_n_word) + '\\t' + ('spam' if isSpam else 'non-spam') + '\\t' + msg[0] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">Results: </span>run the NB classifier with keyword 'assistance', the output file are displayed below:\n",
    "- **Model parameters**: \n",
    " - prior \n",
    " - likelihood\n",
    "- **Classification results**: \n",
    " - correctness\n",
    " - keyword count in the message\n",
    " - log probability for spam and non-spam \n",
    " - classification result\n",
    " - message ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify messages with key word: assistance\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000227\r\n",
      "P(assistance|non-spam) = 0.000093\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "isCorrect \t count \t P(spam) \t P(non-spam) \t Result \t message_id\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0002.2004-08-01.bg\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0004.1999-12-10.kaminski\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0005.1999-12-12.kaminski\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0013.2004-08-01.bg\r\n",
      "1\t3\t-25.9964372973\t-28.4175982176\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.4*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of mapper.py remains the same as it still just counts words for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keywords = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keywords + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of reducer.py is modified to consider multiple keywords, which we use dictionaries to represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keywords = counts[4].split()\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with keywords: ' + str(keywords)\n",
    "   \n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "\n",
    "#### prior probability ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "#### conditional probabilities for words ####\n",
    "p_word_s, p_word_n = {}, {}\n",
    "for word in keywords:\n",
    "    p_word_s[word] = 1.0*((s_count[word] if word in s_count else 0) + 1) / (tot_s + B)\n",
    "    p_word_n[word] = 1.0*((n_count[word] if word in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### print model parameters ####\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "for word in keywords:\n",
    "    print 'P(%s|spam) = %f' %(word, p_word_s[word])\n",
    "    print 'P(%s|non-spam) = %f' %(word, p_word_n[word])\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'isCorrect \\t count \\t P(spam) \\t P(non-spam) \\t Result \\t message_id'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        #### initialize posterior probability ####\n",
    "        p_s_word = math.log(p_s)\n",
    "        p_n_word = math.log(p_n)\n",
    "        \n",
    "        #### add likelihood for each keyword ####\n",
    "        n_word = 0\n",
    "        for key in keywords:\n",
    "            n_key = sum([1 if key in word else 0 for word in words])\n",
    "            n_word += n_key\n",
    "            p_s_word += n_key * math.log(p_word_s[key])\n",
    "            p_n_word += n_key * math.log(p_word_n[key])\n",
    "            \n",
    "        # if the message doesn't contain any keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        isSpam = True if p_s_word > p_n_word else False\n",
    "        isCorrect = '1' if isSpam == int(msg[1]) else '0'\n",
    "        # print results\n",
    "        print isCorrect + '\\t'+str(n_word)+'\\t'+ str(p_s_word) + '\\t' + str(p_n_word) + '\\t' + ('spam' if isSpam else 'non-spam') + '\\t' + msg[0] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with keywords: ['assistance', 'valium', 'enlargementwithatypo']\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000227\r\n",
      "P(assistance|non-spam) = 0.000093\r\n",
      "P(valium|spam) = 0.000038\r\n",
      "P(valium|non-spam) = 0.000047\r\n",
      "P(enlargementwithatypo|spam) = 0.000038\r\n",
      "P(enlargementwithatypo|non-spam) = 0.000047\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "isCorrect \t count \t P(spam) \t P(non-spam) \t Result \t message_id\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0002.2004-08-01.bg\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0004.1999-12-10.kaminski\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0005.1999-12-12.kaminski\r\n",
      "0\t1\t-11.0045589364\t-10.5522255833\tnon-spam\t0009.2003-12-18.gp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0013.2004-08-01.bg\r\n",
      "0\t1\t-11.0045589364\t-10.5522255833\tnon-spam\t0016.2003-12-19.gp\r\n",
      "0\t1\t-11.0045589364\t-10.5522255833\tnon-spam\t0017.2004-08-01.bg\r\n",
      "1\t3\t-25.9964372973\t-28.4175982176\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance valium enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.5*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.6*: Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
