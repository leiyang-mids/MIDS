{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#UC Berkeley MIDS DATASCI W261-02, Machine Learning at Scale\n",
    "##Assignement #1  (version 2016-01-14)\n",
    "## Submitted by Lei Yang ([leiyang@berkeley.edu](mailto:leiyang@berkeley)), 2016-01-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.0.0*: Define big data. Provide an example of a big data problem in your domain of expertise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Answer: \n",
    "Big data is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.0.1*: In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Answer:\n",
    "blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.1*: Read through the provided control script (pNaiveBayes.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.2*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's define pNaiveBayes.sh script first, we only need to do this once since it is the same throughout HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "#echo \"$countfiles\"\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py & reducer.py, and make them executable\n",
    "- mapper.py counts the single specified word for the chunk, and output an integer\n",
    "- reducer.py collates counts from all chunks, and output the total count of the single specified word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "countword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        for word in line.lower().split()[2:]:\n",
    "            if countword in word:\n",
    "                count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for filename in sys.argv[1:]:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            sum += int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">Results: </span>by checking the ouput file, we know there are 10 counts of word 'assistance'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "## display final output results\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.3*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py:\n",
    "- obtains count for each word from the chunk, for spam and non-spam email separately, \n",
    "- records all counts in a dictionary, \n",
    "- outputs the dictionaries (non-spam count, and spam count), (non)spam counts, and keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keyword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keyword + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define reducer.py:\n",
    "- collapse wrod counts from all chunks\n",
    "- estimate NB model parameters: prior and conditional probabilities\n",
    "- classify messages that contains the keyword\n",
    "- **Note:** for messages that don't contain the keyword, the decision is solely based on prior probability, which will always give non-spam prediction, thus we skip those messages and only focus on those with the specified keyword\n",
    "- output results\n",
    "\n",
    "####Parameter estimation background:\n",
    "Assuming *positional independence*, and with *add-one Laplace smoothing*, the multinomial NB conditional probability $P(t | c)$ can be estimated as:\n",
    "$$\n",
    "\\hat{P}(t\\mid c)=\\frac{T_{ct}+1}{(\\sum_{t^\\prime \\in V}{T_{ct^\\prime}})+B},\n",
    "$$\n",
    "\n",
    "where $B=|V|$ is the number of terms in the vocabulary $V$ (including all text classes), and $T_{ct}$ is the count of word *t* in class *c*. \n",
    "\n",
    "To classify a message, the posterior probability of class $c$ can be calculated as:\n",
    "$$\n",
    "c_{map}=\\arg\\max_{c\\in\\mathbb C}[\\log{\\hat{P}(c)}+\\sum_{1\\leqslant k \\leqslant n_d}{\\log{\\hat{P}(t_k\\mid c)}}],\n",
    "$$\n",
    "where $\\hat{P}(t_k\\mid c)$ is estimated above with *positional independence* assumption as $\\hat{P}(t\\mid c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keyword = counts[4]\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with key word: ' + keyword\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "p_word_s = 1.0*((s_count[keyword] if keyword in s_count else 0) + 1) / (tot_s + B)\n",
    "p_word_n = 1.0*((n_count[keyword] if keyword in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### prior probability: same for every message, since it's determined by training data ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "# print model parameters\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "print 'P(%s|spam) = %f' %(keyword, p_word_s)\n",
    "print 'P(%s|non-spam) = %f' %(keyword, p_word_n)\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'isCorrect \\t count \\t P(spam) \\t P(non-spam) \\t Result \\t message_id'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        n_word = sum([1 if keyword in word else 0 for word in words])\n",
    "        # if the message doesn't contain our keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        #### posterior probability ####\n",
    "        p_s_word = math.log(p_s) + n_word * math.log(p_word_s)\n",
    "        p_n_word = math.log(p_n) + n_word * math.log(p_word_n)\n",
    "        isSpam = True if p_s_word > p_n_word else False\n",
    "        isCorrect = '1' if isSpam == int(msg[1]) else '0'\n",
    "        # print results\n",
    "        print isCorrect + '\\t'+str(n_word)+'\\t'+ str(p_s_word) + '\\t' + str(p_n_word) + '\\t' + ('spam' if isSpam else 'non-spam') + '\\t' + msg[0] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">Results: </span>run the NB classifier with keyword 'assistance', the output file are displayed below:\n",
    "- **Model parameters**: \n",
    " - prior \n",
    " - likelihood\n",
    "- **Classification results**: \n",
    " - correctness\n",
    " - keyword count in the message\n",
    " - log probability for spam and non-spam \n",
    " - classification result\n",
    " - message ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify messages with key word: assistance\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000227\r\n",
      "P(assistance|non-spam) = 0.000093\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "isCorrect \t count \t P(spam) \t P(non-spam) \t Result \t message_id\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0002.2004-08-01.bg\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0004.1999-12-10.kaminski\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0005.1999-12-12.kaminski\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0013.2004-08-01.bg\r\n",
      "1\t3\t-25.9964372973\t-28.4175982176\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.4*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of mapper.py remains the same as it still just counts words for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keywords = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keywords + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of reducer.py is modified to consider multiple keywords, which we use dictionaries to represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keywords = counts[4].split()\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with keywords: ' + str(keywords)\n",
    "   \n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "\n",
    "#### prior probability ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "#### conditional probabilities for words ####\n",
    "p_word_s, p_word_n = {}, {}\n",
    "for word in keywords:\n",
    "    p_word_s[word] = 1.0*((s_count[word] if word in s_count else 0) + 1) / (tot_s + B)\n",
    "    p_word_n[word] = 1.0*((n_count[word] if word in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### print model parameters ####\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "for word in keywords:\n",
    "    print 'P(%s|spam) = %f' %(word, p_word_s[word])\n",
    "    print 'P(%s|non-spam) = %f' %(word, p_word_n[word])\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'isCorrect \\t count \\t P(spam) \\t P(non-spam) \\t Result \\t message_id'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        #### initialize posterior probability ####\n",
    "        p_s_word = math.log(p_s)\n",
    "        p_n_word = math.log(p_n)\n",
    "        \n",
    "        #### add likelihood for each keyword ####\n",
    "        n_word = 0\n",
    "        for key in keywords:\n",
    "            n_key = sum([1 if key in word else 0 for word in words])\n",
    "            n_word += n_key\n",
    "            p_s_word += n_key * math.log(p_word_s[key])\n",
    "            p_n_word += n_key * math.log(p_word_n[key])\n",
    "            \n",
    "        # if the message doesn't contain any keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        isSpam = True if p_s_word > p_n_word else False\n",
    "        isCorrect = '1' if isSpam == int(msg[1]) else '0'\n",
    "        # print results\n",
    "        print isCorrect + '\\t'+str(n_word)+'\\t'+ str(p_s_word) + '\\t' + str(p_n_word) + '\\t' + ('spam' if isSpam else 'non-spam') + '\\t' + msg[0] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">Results: </span>run the NB classifier with keywords 'assistance', 'valium' and 'enlargementWithATypo', the output file are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with keywords: ['assistance', 'valium', 'enlargementwithatypo']\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000227\r\n",
      "P(assistance|non-spam) = 0.000093\r\n",
      "P(valium|spam) = 0.000038\r\n",
      "P(valium|non-spam) = 0.000047\r\n",
      "P(enlargementwithatypo|spam) = 0.000038\r\n",
      "P(enlargementwithatypo|non-spam) = 0.000047\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "isCorrect \t count \t P(spam) \t P(non-spam) \t Result \t message_id\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0002.2004-08-01.bg\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0004.1999-12-10.kaminski\r\n",
      "0\t1\t-9.21279946713\t-9.85907840269\tspam\t0005.1999-12-12.kaminski\r\n",
      "0\t1\t-11.0045589364\t-10.5522255833\tnon-spam\t0009.2003-12-18.gp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0013.2004-08-01.bg\r\n",
      "0\t1\t-11.0045589364\t-10.5522255833\tnon-spam\t0016.2003-12-19.gp\r\n",
      "0\t1\t-11.0045589364\t-10.5522255833\tnon-spam\t0017.2004-08-01.bg\r\n",
      "1\t3\t-25.9964372973\t-28.4175982176\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "1\t1\t-9.21279946713\t-9.85907840269\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance valium enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.5*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of mapper.py remains the same as it still just counts words for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "#keywords = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of reducer.py is modified to consider all present words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with all words'\n",
    "   \n",
    "# we now estimate NB parameters for all present words\n",
    "allwords = Set(s_count.keys() + n_count.keys())\n",
    "B = len(allwords)\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "\n",
    "#### prior probability ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "#### conditional probabilities for words ####\n",
    "p_word_s, p_word_n = {}, {}\n",
    "for word in allwords:\n",
    "    p_word_s[word] = 1.0*((s_count[word] if word in s_count else 0) + 1) / (tot_s + B)\n",
    "    p_word_n[word] = 1.0*((n_count[word] if word in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### we won't print model parameters, to save some space ####\n",
    "#print '\\n============= Model Parameters ============='\n",
    "#print 'P(spam) = %f' %(p_s)\n",
    "#print 'P(non-spam) = %f' %(p_n)\n",
    "#for word in keywords:\n",
    "#    print 'P(%s|spam) = %f' %(word, p_word_s[word])\n",
    "#    print 'P(%s|non-spam) = %f' %(word, p_word_n[word])\n",
    "\n",
    "#### likelihood: dependend on the frequency of current word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'isCorrect \\t P(spam) \\t P(non-spam) \\t Result \\t message_id'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        #### initialize posterior probability ####\n",
    "        p_s_word = math.log(p_s)\n",
    "        p_n_word = math.log(p_n)\n",
    "        \n",
    "        #### add likelihood for each keyword ####        \n",
    "        for key in allwords:\n",
    "            n_key = sum([1 if key in word else 0 for word in words])\n",
    "            if n_key == 0:\n",
    "                continue\n",
    "            p_s_word += n_key * math.log(p_word_s[key])\n",
    "            p_n_word += n_key * math.log(p_word_n[key])\n",
    "            \n",
    "        isSpam = True if p_s_word > p_n_word else False\n",
    "        isCorrect = '1' if isSpam == int(msg[1]) else '0'\n",
    "        # print results\n",
    "        print isCorrect + '\\t' + str(p_s_word) + '\\t' + str(p_n_word) + '\\t' + ('spam' if isSpam else 'non-spam') + '\\t' + msg[0] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">Results: </span>run the NB classifier all present words, the output file are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with all words\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "isCorrect \t P(spam) \t P(non-spam) \t Result \t message_id\r\n",
      "0\t-338.100513732\t-345.030483106\tspam\t0001.1999-12-10.farmer\r\n",
      "0\t-229.718777033\t-230.28703857\tspam\t0001.1999-12-10.kaminski\r\n",
      "0\t-31418.2819798\t-31692.3584495\tspam\t0001.2000-01-17.beck\r\n",
      "0\t-35849.4027148\t-36366.6160568\tspam\t0001.2000-06-06.lokay\r\n",
      "0\t-3028.73094739\t-3041.39758602\tspam\t0001.2001-02-07.kitchen\r\n",
      "0\t-9049.17283072\t-9205.36454143\tspam\t0001.2001-04-02.williams\r\n",
      "1\t-22534.6737432\t-22206.6598158\tnon-spam\t0002.1999-12-13.farmer\r\n",
      "0\t-3808.38019963\t-3878.07251588\tspam\t0002.2001-02-07.kitchen\r\n",
      "1\t-4761.5205382\t-4967.66217117\tspam\t0002.2001-05-25.sa_and_hp\r\n",
      "1\t-10144.9489428\t-10539.6238736\tspam\t0002.2003-12-18.gp\r\n",
      "1\t-6533.01744078\t-6671.94557924\tspam\t0002.2004-08-01.bg\r\n",
      "0\t-3131.99575113\t-3165.56385382\tspam\t0003.1999-12-10.kaminski\r\n",
      "1\t-618.973999679\t-606.907018404\tnon-spam\t0003.1999-12-14.farmer\r\n",
      "0\t-10964.3799467\t-10999.9072195\tspam\t0003.2000-01-17.beck\r\n",
      "0\t-10447.0950607\t-10478.5608226\tspam\t0003.2001-02-08.kitchen\r\n",
      "1\t-7193.48280863\t-7454.79873682\tspam\t0003.2003-12-18.gp\r\n",
      "1\t-6571.87203943\t-6795.71895037\tspam\t0003.2004-08-01.bg\r\n",
      "0\t-8222.7320034\t-8255.68381115\tspam\t0004.1999-12-10.kaminski\r\n",
      "0\t-8423.64997638\t-8433.54370233\tspam\t0004.1999-12-14.farmer\r\n",
      "0\t-5475.42532362\t-5584.7891813\tspam\t0004.2001-04-02.williams\r\n",
      "1\t-7137.40610149\t-7438.97478169\tspam\t0004.2001-06-12.sa_and_hp\r\n",
      "1\t-4373.34212779\t-4531.50116296\tspam\t0004.2004-08-01.bg\r\n",
      "0\t-6670.54679169\t-6724.53966164\tspam\t0005.1999-12-12.kaminski\r\n",
      "0\t-8665.10509069\t-8722.31809532\tspam\t0005.1999-12-14.farmer\r\n",
      "0\t-3553.88124657\t-3631.22579184\tspam\t0005.2000-06-06.lokay\r\n",
      "0\t-7227.74270348\t-7364.85113998\tspam\t0005.2001-02-08.kitchen\r\n",
      "1\t-1420.7300832\t-1500.14371079\tspam\t0005.2001-06-23.sa_and_hp\r\n",
      "1\t-63963.5884412\t-66426.2521878\tspam\t0005.2003-12-18.gp\r\n",
      "0\t-3584.69030122\t-3634.4747003\tspam\t0006.1999-12-13.kaminski\r\n",
      "0\t-85097.8921207\t-86074.2994759\tspam\t0006.2001-02-08.kitchen\r\n",
      "1\t-2208.92069589\t-2197.77624839\tnon-spam\t0006.2001-04-03.williams\r\n",
      "1\t-2800.82631506\t-2918.64771058\tspam\t0006.2001-06-25.sa_and_hp\r\n",
      "1\t-9181.3019319\t-9472.29704282\tspam\t0006.2003-12-18.gp\r\n",
      "1\t-7911.15523382\t-8092.23563466\tspam\t0006.2004-08-01.bg\r\n",
      "0\t-11033.5913007\t-11158.6198567\tspam\t0007.1999-12-13.kaminski\r\n",
      "0\t-5303.37088379\t-5309.82053734\tspam\t0007.1999-12-14.farmer\r\n",
      "0\t-29205.9012056\t-29366.6059191\tspam\t0007.2000-01-17.beck\r\n",
      "0\t-14815.6881687\t-14963.7148785\tspam\t0007.2001-02-09.kitchen\r\n",
      "1\t-10513.7068665\t-10850.0891477\tspam\t0007.2003-12-18.gp\r\n",
      "1\t-11923.2242301\t-12648.1523688\tspam\t0007.2004-08-01.bg\r\n",
      "0\t-39136.845954\t-39379.4156921\tspam\t0008.2001-02-09.kitchen\r\n",
      "1\t-7137.40610149\t-7438.97478169\tspam\t0008.2001-06-12.sa_and_hp\r\n",
      "1\t-38639.2678916\t-40514.7788878\tspam\t0008.2001-06-25.sa_and_hp\r\n",
      "1\t-8228.52024679\t-8512.16317766\tspam\t0008.2003-12-18.gp\r\n",
      "1\t-47375.9998166\t-48773.359071\tspam\t0008.2004-08-01.bg\r\n",
      "1\t-55591.0882646\t-55210.1139253\tnon-spam\t0009.1999-12-13.kaminski\r\n",
      "0\t-3618.26605912\t-3718.06529907\tspam\t0009.1999-12-14.farmer\r\n",
      "0\t-25808.0819829\t-26035.5091118\tspam\t0009.2000-06-07.lokay\r\n",
      "0\t-52121.7864285\t-52913.024318\tspam\t0009.2001-02-09.kitchen\r\n",
      "1\t-9456.70751766\t-10005.7754238\tspam\t0009.2001-06-26.sa_and_hp\r\n",
      "1\t-4778.58016316\t-4931.84437891\tspam\t0009.2003-12-18.gp\r\n",
      "0\t-10932.7426782\t-10971.914434\tspam\t0010.1999-12-14.farmer\r\n",
      "0\t-1834.63634853\t-1874.504835\tspam\t0010.1999-12-14.kaminski\r\n",
      "0\t-28861.0672429\t-29158.1912559\tspam\t0010.2001-02-09.kitchen\r\n",
      "1\t-29388.8567811\t-30500.8908211\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "1\t-280.921473881\t-296.69492823\tspam\t0010.2003-12-18.gp\r\n",
      "1\t-17755.2079971\t-18742.7596389\tspam\t0010.2004-08-01.bg\r\n",
      "0\t-15803.7632717\t-15959.6833212\tspam\t0011.1999-12-14.farmer\r\n",
      "1\t-29352.6122135\t-30462.8056127\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "1\t-136669.460068\t-142224.322054\tspam\t0011.2001-06-29.sa_and_hp\r\n",
      "1\t-3751.50112332\t-3884.29705657\tspam\t0011.2003-12-18.gp\r\n",
      "1\t-5602.44523014\t-5740.44946432\tspam\t0011.2004-08-01.bg\r\n",
      "0\t-28017.4753752\t-28303.8414264\tspam\t0012.1999-12-14.farmer\r\n",
      "1\t-7064.71218362\t-7061.41992995\tnon-spam\t0012.1999-12-14.kaminski\r\n",
      "0\t-29213.9656278\t-29367.1872145\tspam\t0012.2000-01-17.beck\r\n",
      "0\t-7182.99910384\t-7347.60271719\tspam\t0012.2000-06-08.lokay\r\n",
      "0\t-3901.38959783\t-3945.97204058\tspam\t0012.2001-02-09.kitchen\r\n",
      "1\t-1169.4177616\t-1246.73670573\tspam\t0012.2003-12-19.gp\r\n",
      "0\t-16638.7339658\t-16896.7283729\tspam\t0013.1999-12-14.farmer\r\n",
      "0\t-9745.49977419\t-9802.56864312\tspam\t0013.1999-12-14.kaminski\r\n",
      "0\t-4505.95460642\t-4553.64920901\tspam\t0013.2001-04-03.williams\r\n",
      "1\t-245206.274369\t-256194.128508\tspam\t0013.2001-06-30.sa_and_hp\r\n",
      "1\t-11804.502178\t-12185.8532701\tspam\t0013.2004-08-01.bg\r\n",
      "0\t-13699.3086256\t-13815.7710197\tspam\t0014.1999-12-14.kaminski\r\n",
      "0\t-8522.27567912\t-8571.3904855\tspam\t0014.1999-12-15.farmer\r\n",
      "0\t-11765.9811251\t-11954.9219866\tspam\t0014.2001-02-12.kitchen\r\n",
      "1\t-27729.1731132\t-29352.2941466\tspam\t0014.2001-07-04.sa_and_hp\r\n",
      "1\t-835.844218363\t-887.647575384\tspam\t0014.2003-12-19.gp\r\n",
      "1\t-6172.34836813\t-6338.47266604\tspam\t0014.2004-08-01.bg\r\n",
      "0\t-4965.5528453\t-5026.3552597\tspam\t0015.1999-12-14.kaminski\r\n",
      "0\t-5561.52774159\t-5585.81325357\tspam\t0015.1999-12-15.farmer\r\n",
      "0\t-935.46122742\t-950.515352388\tspam\t0015.2000-06-09.lokay\r\n",
      "0\t-44075.8407162\t-44270.7744246\tspam\t0015.2001-02-12.kitchen\r\n",
      "1\t-7932.99389034\t-8270.0529121\tspam\t0015.2001-07-05.sa_and_hp\r\n",
      "1\t-10298.7730812\t-10779.0004236\tspam\t0015.2003-12-19.gp\r\n",
      "0\t-5675.41148302\t-5734.10014129\tspam\t0016.1999-12-15.farmer\r\n",
      "1\t-9614.05915229\t-9589.44777646\tnon-spam\t0016.2001-02-12.kitchen\r\n",
      "1\t-7932.99389034\t-8270.0529121\tspam\t0016.2001-07-05.sa_and_hp\r\n",
      "1\t-149516.231923\t-156381.225947\tspam\t0016.2001-07-06.sa_and_hp\r\n",
      "1\t-4971.12501857\t-5122.91035171\tspam\t0016.2003-12-19.gp\r\n",
      "1\t-6000.36237095\t-6310.16274163\tspam\t0016.2004-08-01.bg\r\n",
      "0\t-2842.3300082\t-2884.70607478\tspam\t0017.1999-12-14.kaminski\r\n",
      "0\t-29214.5274982\t-29365.8304224\tspam\t0017.2000-01-17.beck\r\n",
      "0\t-2943.74309754\t-2964.58476121\tspam\t0017.2001-04-03.williams\r\n",
      "1\t-1559.18841881\t-1628.33989222\tspam\t0017.2003-12-18.gp\r\n",
      "1\t-7909.66061971\t-8150.63693897\tspam\t0017.2004-08-01.bg\r\n",
      "1\t-21909.1044287\t-22818.4461535\tspam\t0017.2004-08-02.bg\r\n",
      "0\t-6871.80841174\t-7076.85960154\tspam\t0018.1999-12-14.kaminski\r\n",
      "1\t-28334.9059858\t-29272.1238059\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "1\t-30068.1633178\t-31118.1983543\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"dummy\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.6*: Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
