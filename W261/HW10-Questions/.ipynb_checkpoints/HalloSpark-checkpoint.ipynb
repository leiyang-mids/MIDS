{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.9 (default, Dec 15 2014 10:37:34)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/leiyang/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "\n",
    "dataRDD = sc.parallelize(np.random.random_sample(1000))   \n",
    "data2X= dataRDD.map(lambda x: x*2)\n",
    "dataGreaterThan1 = data2X.filter(lambda x: x > 1.0)\n",
    "cachedRDD = dataGreaterThan1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedRDD.filter(lambda x: x<1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26842824317966052,\n",
       " 0.22885512655070683,\n",
       " 0.9758237846106288,\n",
       " 0.33859930537719629,\n",
       " 0.0070283515932643148,\n",
       " 1.9340366746533655,\n",
       " 0.14436696744010025,\n",
       " 0.16345393294606159,\n",
       " 0.19683340264192073,\n",
       " 0.45103274216591682]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2X.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###transformation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), ('a', 34), ('b', 8)])\n",
    "def add1(a, b): return a + [b]\n",
    "def add2(a, b): return a + b\n",
    "def test(a): return [a]\n",
    "b = x.combineByKey(test, add1, add2).collect()\n",
    "c = x.reduceByKey(lambda a,b: [a]+[b]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"B\\t{'C': 1}\",\n",
       " u\"C\\t{'B': 1}\",\n",
       " u\"D\\t{'A': 1, 'B': 1}\",\n",
       " u\"E\\t{'D': 1, 'B': 1, 'F': 1}\",\n",
       " u\"F\\t{'B': 1, 'E': 1}\",\n",
       " u\"G\\t{'B': 1, 'E': 1}\",\n",
       " u\"H\\t{'B': 1, 'E': 1}\",\n",
       " u\"I\\t{'B': 1, 'E': 1}\",\n",
       " u\"J\\t{'E': 1}\",\n",
       " u\"K\\t{'E': 1}\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sc.textFile('hdfs://localhost:9000/user/leiyang/PageRank-test.txt').cache()\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', {'a': [], 'p': 0.5}),\n",
       " ('C', {'a': ['B'], 'p': 1.0}),\n",
       " (u'E', {'a': ['B', 'D', 'F'], 'p': 4.0}),\n",
       " (u'G', {'a': ['B', 'E'], 'p': 0}),\n",
       " (u'I', {'a': ['B', 'E'], 'p': 0}),\n",
       " (u'K', {'a': ['E'], 'p': 0}),\n",
       " (u'H', {'a': ['B', 'E'], 'p': 0}),\n",
       " (u'J', {'a': ['E'], 'p': 0}),\n",
       " (u'B', {'a': ['C'], 'p': 3.833333333333333}),\n",
       " (u'D', {'a': ['A', 'B'], 'p': 0.3333333333333333}),\n",
       " ('F', {'a': ['B', 'E'], 'p': 0.3333333333333333})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNode(line):\n",
    "    nid, adj = line.strip().split('\\t', 1)\n",
    "    cmd = 'adj = %s' %adj\n",
    "    exec cmd\n",
    "    return (nid, adj.keys())\n",
    "\n",
    "def getGraph(line):\n",
    "    # parse line\n",
    "    nid, adj = line.strip().split('\\t', 1)    \n",
    "    cmd = 'adj = %s' %adj\n",
    "    exec cmd\n",
    "    # initialize node struct        \n",
    "    return(nid.strip('\"'), {'a':adj.keys(), 'p':1.0})\n",
    "\n",
    "def distributeMass(v):\n",
    "    node = v[1]\n",
    "    rankMass = node['p']/len(node['a'])\n",
    "    return [[a, rankMass] for a in node['a']]\n",
    "\n",
    "def mapper_job_init(line):      \n",
    "    emit = []\n",
    "    # parse line\n",
    "    nid, adj = line.strip().split('\\t', 1)\n",
    "    nid = nid.strip('\"')\n",
    "    cmd = 'adj = %s' %adj\n",
    "    exec cmd\n",
    "    # initialize node struct        \n",
    "    node = {'a':adj.keys(), 'p':0}\n",
    "    rankMass = 1.0/len(adj)\n",
    "    # emit node\n",
    "    emit.append((nid, node))\n",
    "    # emit pageRank mass        \n",
    "    for m in node['a']:\n",
    "        emit.append((m, rankMass))\n",
    "    return emit\n",
    "\n",
    "def reducer_job_init(value):      \n",
    "    # increase counter for node count\n",
    "    #self.increment_counter('wiki_node_count', 'nodes', 1)\n",
    "    rankMass, node = 0.0, None\n",
    "    # loop through all arrivals\n",
    "    for v in value:            \n",
    "        if isinstance(v, float):\n",
    "            rankMass += v         \n",
    "        else:\n",
    "            node = v\n",
    "    # handle dangling node, create node struct and add missing mass\n",
    "    if not node:            \n",
    "        node = {'a':[], 'p':rankMass}            \n",
    "        #self.increment_counter('wiki_dangling_mass', 'mass', int(1e10))\n",
    "    else:\n",
    "        node['p'] += rankMass            \n",
    "    # emit for next iteration\n",
    "    return node\n",
    "\n",
    "def test(a, b):\n",
    "    if isinstance(a, float) and isinstance(b, float):\n",
    "        return a+b\n",
    "    if isinstance(a, float) and not isinstance(b, float):\n",
    "        b['p'] += a\n",
    "        return b\n",
    "    else: #if not isinstance(a, float) and isinstance(b, float):\n",
    "        a['p'] += b\n",
    "        return a\n",
    "\n",
    "def getDangling(node):    \n",
    "    if isinstance(node[1], float):\n",
    "        # TODO: use the accumulator for loss mass\n",
    "        return (node[0], {'a':[], 'p':node[1]})\n",
    "    else:\n",
    "        return node\n",
    "\n",
    "nodes = y.flatMap(mapper_job_init)\n",
    "#mass = y.flatMap(mapper_job_init)\n",
    "#mass.reduceByKey(lambda x,y:x+y).collect()\n",
    "#graph = y.map(getGraph).cache()\n",
    "#graph.collect()\n",
    "#mass = graph.flatMap(distributeMass).reduceByKey(lambda x,y: x+y)\n",
    "#mass.fullOuterJoin(graph).collect()\n",
    "#nodes.collect()#reduceByKey(reducer_job_init).collect()\n",
    "distMass = nodes.reduceByKey(test) #.collect()\n",
    "distMass.map(getDangling).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'J', ['E']),\n",
       " (u'K', ['E']),\n",
       " (u'B', ['C']),\n",
       " (u'C', ['B']),\n",
       " (u'E', ['B', 'D', 'F']),\n",
       " (u'F', ['B', 'E']),\n",
       " (u'G', ['B', 'E']),\n",
       " (u'H', ['B', 'E']),\n",
       " (u'I', ['B', 'E']),\n",
       " (u'D', ['A', 'B'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.sortBy(lambda l: l[1][0], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "#!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
