{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.9 (default, Dec 15 2014 10:37:34)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/leiyang/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "\n",
    "dataRDD = sc.parallelize(np.random.random_sample(1000))   \n",
    "data2X= dataRDD.map(lambda x: x*2)\n",
    "dataGreaterThan1 = data2X.filter(lambda x: x > 1.0)\n",
    "cachedRDD = dataGreaterThan1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedRDD.filter(lambda x: x<1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26842824317966052,\n",
       " 0.22885512655070683,\n",
       " 0.9758237846106288,\n",
       " 0.33859930537719629,\n",
       " 0.0070283515932643148,\n",
       " 1.9340366746533655,\n",
       " 0.14436696744010025,\n",
       " 0.16345393294606159,\n",
       " 0.19683340264192073,\n",
       " 0.45103274216591682]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2X.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###transformation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), ('a', 34), ('b', 8)])\n",
    "def add1(a, b): return a + [b]\n",
    "def add2(a, b): return a + b\n",
    "def test(a): return [a]\n",
    "b = x.combineByKey(test, add1, add2).collect()\n",
    "c = x.reduceByKey(lambda a,b: [a]+[b]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"1\\t{'2': 1, '3': 1, '4': 1}\",\n",
       " u\"2\\t{'1': 1, '3': 1, '4': 1, '310': 1, '311': 1}\",\n",
       " u\"3\\t{'1': 1, '2': 1, '4': 1}\",\n",
       " u\"4\\t{'1': 1, '2': 1, '3': 1, '311': 1}\",\n",
       " u\"5\\t{'6': 1}\",\n",
       " u\"6\\t{'5': 1}\",\n",
       " u\"7\\t{'8': 1, '9': 1}\",\n",
       " u\"8\\t{'7': 1, '9': 1}\",\n",
       " u\"9\\t{'7': 1, '8': 1, '124': 1, '1316': 1}\",\n",
       " u\"10\\t{'11': 1, '12': 1, '13': 1, '14': 1, '15': 1, '16': 1, '17': 1}\",\n",
       " u\"11\\t{'10': 1, '605': 1, '4034': 1, '12': 1, '13': 1, '4035': 1, '14': 1, '15': 1, '4036': 1, '16': 1, '4037': 1, '4038': 1, '4039': 1, '17': 1}\",\n",
       " u\"12\\t{'10': 1, '11': 1, '4034': 1, '13': 1, '4035': 1, '2209': 1, '3633': 1, '4557': 1, '16': 1, '4037': 1, '4038': 1, '4039': 1, '1168': 1, '4240': 1, '4241': 1, '3052': 1}\",\n",
       " u\"13\\t{'10': 1, '11': 1, '12': 1, '16': 1, '4039': 1}\",\n",
       " u\"14\\t{'10': 1, '605': 1, '11': 1, '4557': 1, '15': 1, '4036': 1, '17': 1}\",\n",
       " u\"15\\t{'10': 1, '605': 1, '1084': 1, '2168': 1, '11': 1, '14': 1, '5309': 1, '4036': 1, '5310': 1, '606': 1, '458': 1, '3702': 1, '541': 1, '5245': 1, '96': 1, '3336': 1, '2884': 1, '3834': 1, '3835': 1, '607': 1, '4687': 1, '99': 1, '3838': 1, '3345': 1, '608': 1, '5311': 1, '5312': 1, '5313': 1, '609': 1, '610': 1, '5314': 1, '5315': 1, '611': 1, '5316': 1, '17': 1, '612': 1, '4966': 1, '4967': 1, '4968': 1, '5317': 1, '613': 1}\",\n",
       " u\"16\\t{'10': 1, '331': 1, '3047': 1, '3048': 1, '11': 1, '4034': 1, '12': 1, '13': 1, '333': 1, '4035': 1, '3049': 1, '4665': 1, '4086': 1, '4087': 1, '4037': 1, '4038': 1, '4039': 1, '1168': 1, '4240': 1, '4241': 1, '3050': 1, '3051': 1, '3052': 1, '334': 1, '4558': 1}\",\n",
       " u\"17\\t{'10': 1, '605': 1, '1685': 1, '11': 1, '1686': 1, '14': 1, '15': 1, '4036': 1, '1687': 1, '1688': 1, '2177': 1, '1706': 1, '7385': 1}\",\n",
       " u\"18\\t{'19': 1, '20': 1, '21': 1}\",\n",
       " u\"19\\t{'18': 1, '20': 1, '21': 1, '225': 1, '226': 1}\",\n",
       " u\"20\\t{'18': 1, '19': 1, '21': 1, '225': 1, '226': 1}\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sc.textFile('hdfs://localhost:9000/user/leiyang/synNet.txt').cache()\n",
    "y.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getNode(line):\n",
    "    nid, adj = line.strip().split('\\t', 1)\n",
    "    cmd = 'adj = %s' %adj\n",
    "    exec cmd\n",
    "    return (nid, adj.keys())\n",
    "\n",
    "nodes = y.map(getNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'997', ['999', '998', '1004', '1003', '1002', '1001', '1000']),\n",
       " (u'998', ['997', '999', '1004', '1003', '1002', '1001']),\n",
       " (u'999', ['997', '1002', '3352', '1004', '1003', '998', '1001', '1000']),\n",
       " (u'1000', ['997', '999', '1004', '1003', '1002', '1001']),\n",
       " (u'1001', ['997', '999', '998', '1000']),\n",
       " (u'1002', ['997', '999', '998', '1000']),\n",
       " (u'1004', ['997', '1000', '999', '998', '3352']),\n",
       " (u'995', ['996']),\n",
       " (u'6034', ['996']),\n",
       " (u'996', ['995', '6034'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.sortBy(lambda l: l[1][0], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
