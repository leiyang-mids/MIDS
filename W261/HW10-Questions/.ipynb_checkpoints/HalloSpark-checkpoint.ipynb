{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.9 (default, Dec 15 2014 10:37:34)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/leiyang/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "\n",
    "dataRDD = sc.parallelize(np.random.random_sample(1000))   \n",
    "data2X= dataRDD.map(lambda x: x*2)\n",
    "dataGreaterThan1 = data2X.filter(lambda x: x > 1.0)\n",
    "cachedRDD = dataGreaterThan1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedRDD.filter(lambda x: x<1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26842824317966052,\n",
       " 0.22885512655070683,\n",
       " 0.9758237846106288,\n",
       " 0.33859930537719629,\n",
       " 0.0070283515932643148,\n",
       " 1.9340366746533655,\n",
       " 0.14436696744010025,\n",
       " 0.16345393294606159,\n",
       " 0.19683340264192073,\n",
       " 0.45103274216591682]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2X.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###transformation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), ('a', 34), ('b', 8)])\n",
    "def add1(a, b): return a + [b]\n",
    "def add2(a, b): return a + b\n",
    "def test(a): return [a]\n",
    "b = x.combineByKey(test, add1, add2).collect()\n",
    "c = x.reduceByKey(lambda a,b: [a]+[b]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"B\\t{'C': 1}\",\n",
       " u\"C\\t{'B': 1}\",\n",
       " u\"D\\t{'A': 1, 'B': 1}\",\n",
       " u\"E\\t{'D': 1, 'B': 1, 'F': 1}\",\n",
       " u\"F\\t{'B': 1, 'E': 1}\",\n",
       " u\"G\\t{'B': 1, 'E': 1}\",\n",
       " u\"H\\t{'B': 1, 'E': 1}\",\n",
       " u\"I\\t{'B': 1, 'E': 1}\",\n",
       " u\"J\\t{'E': 1}\",\n",
       " u\"K\\t{'E': 1}\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sc.textFile('hdfs://localhost:9000/user/leiyang/PageRank-test.txt').cache()\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "def initialize(line):          \n",
    "    # parse line\n",
    "    nid, adj = line.strip().split('\\t', 1)     \n",
    "    exec 'adj = %s' %adj\n",
    "    # initialize node struct        \n",
    "    node = {'a':adj.keys(), 'p':0}\n",
    "    rankMass = 1.0/len(adj)    \n",
    "    # emit pageRank mass and node    \n",
    "    return [(m, rankMass) for m in node['a']] + [(nid.strip('\"'), node)]\n",
    "\n",
    "def accumulateMass(a, b):\n",
    "    if isinstance(a, float) and isinstance(b, float):\n",
    "        return a+b\n",
    "    if isinstance(a, float) and not isinstance(b, float):\n",
    "        b['p'] += a\n",
    "        return b\n",
    "    else:\n",
    "        a['p'] += b\n",
    "        return a\n",
    "    \n",
    "def getDangling(node):    \n",
    "    global nDangling\n",
    "    if isinstance(node[1], float):\n",
    "        nDangling += 1\n",
    "        return (node[0], {'a':[], 'p':node[1]})\n",
    "    else:\n",
    "        return node\n",
    "    \n",
    "def redistributeMass(node):\n",
    "    node[1]['p'] = (p_dangling+node[1]['p'])*damping + alpha\n",
    "    return node\n",
    "    \n",
    "def distributeMass(node):    \n",
    "    global lossMass\n",
    "    mass, adj = node[1]['p'], node[1]['a']\n",
    "    node[1]['p'] = 0\n",
    "    if len(adj) == 0:\n",
    "        lossMass += mass\n",
    "        return [node]\n",
    "    else:\n",
    "        rankMass = mass/len(adj)\n",
    "        return [(x, rankMass) for x in adj]+[node]\n",
    "\n",
    "def logTime():\n",
    "    return str(datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-30 22:11:10.724255: start PageRank initialization ...\n",
      "2016-03-30 22:11:10.924300: initialization completed, dangling node(s): 1, total nodes: 11\n",
      "2016-03-30 22:11:10.927645: running iteration 2 ...\n",
      "2016-03-30 22:11:11.147666: redistributing loss mass: 0.6523\n",
      "2016-03-30 22:11:11.149196: running iteration 3 ...\n",
      "2016-03-30 22:11:11.338347: redistributing loss mass: 0.4174\n",
      "2016-03-30 22:11:11.339764: running iteration 4 ...\n",
      "2016-03-30 22:11:11.528594: redistributing loss mass: 0.7042\n",
      "2016-03-30 22:11:11.529948: running iteration 5 ...\n",
      "2016-03-30 22:11:11.760135: redistributing loss mass: 0.4136\n",
      "2016-03-30 22:11:11.761706: running iteration 6 ...\n",
      "2016-03-30 22:11:11.954510: redistributing loss mass: 0.4254\n",
      "2016-03-30 22:11:11.955928: running iteration 7 ...\n",
      "2016-03-30 22:11:12.151159: redistributing loss mass: 0.3753\n",
      "2016-03-30 22:11:12.152683: running iteration 8 ...\n",
      "2016-03-30 22:11:12.347720: redistributing loss mass: 0.3812\n",
      "2016-03-30 22:11:12.349224: running iteration 9 ...\n",
      "2016-03-30 22:11:12.578277: redistributing loss mass: 0.3659\n",
      "2016-03-30 22:11:12.579839: running iteration 10 ...\n",
      "2016-03-30 22:11:12.799092: redistributing loss mass: 0.3660\n",
      "2016-03-30 22:11:12.865891: normalized weight of the graph: 1.0000\n",
      "2016-03-30 22:11:12.867531: PageRanking completed in 0.04 minutes, top 100 are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('B', 0.3632359489889102),\n",
       " ('C', 0.36288372803871793),\n",
       " (u'E', 0.08114525762548769),\n",
       " ('D', 0.03938466342002967),\n",
       " ('F', 0.03938466342002967),\n",
       " ('A', 0.03293010178620472),\n",
       " (u'G', 0.016207127344124005),\n",
       " (u'I', 0.016207127344124005),\n",
       " (u'K', 0.016207127344124005),\n",
       " (u'H', 0.016207127344124005),\n",
       " (u'J', 0.016207127344124005)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "nDangling = sc.accumulator(0)\n",
    "lossMass = sc.accumulator(0.0)\n",
    "G = p_dangling = 0\n",
    "damping = 0.85\n",
    "alpha = 1 - damping\n",
    "nTop, nIter = 100, 10\n",
    "start = time()\n",
    "print '%s: start PageRank initialization ...' %(logTime())\n",
    "graph = y.flatMap(initialize).reduceByKey(accumulateMass).map(getDangling).cache()\n",
    "G = graph.count()\n",
    "p_dangling = 1.0*nDangling.value/G\n",
    "graph = graph.map(redistributeMass)\n",
    "#graph.cache()\n",
    "print '%s: initialization completed, dangling node(s): %d, total nodes: %d' %(logTime(), nDangling.value, G)\n",
    "\n",
    "for i in range(nIter-1):\n",
    "    print '%s: running iteration %d ...' %(logTime(), i+2)\n",
    "    lossMass.value = 0.0\n",
    "    graph = graph.flatMap(distributeMass).reduceByKey(accumulateMass).cache()\n",
    "    # need to call an action here in order to have loss mass\n",
    "    graph.count()    \n",
    "    p_dangling = lossMass.value/G\n",
    "    print '%s: redistributing loss mass: %.4f' %(logTime(), lossMass.value)\n",
    "    graph = graph.map(redistributeMass)\n",
    "\n",
    "totalMass = graph.aggregate(0, (lambda x, y: y[1]['p'] + x), (lambda x, y: x+y))\n",
    "print '%s: normalized weight of the graph: %.4f' %(logTime(), totalMass/G)\n",
    "print '%s: PageRanking completed in %.2f minutes, top %d are:' %(logTime(), (time()-start)/60.0, nTop)\n",
    "graph.map(lambda n:(n[0],n[1]['p']/G)).sortBy(lambda n: n[1], ascending=False).take(nTop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.999999999999998"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: y[1]['p'] + x)\n",
    "comOp = (lambda x, y: x+y)\n",
    "graph.aggregate(0, (lambda x, y: y[1]['p'] + x), (lambda x, y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 0.03293010178620472),\n",
       " ('C', 0.36288372803871793),\n",
       " (u'E', 0.08114525762548769),\n",
       " (u'G', 0.016207127344124005),\n",
       " (u'I', 0.016207127344124005),\n",
       " (u'K', 0.016207127344124005),\n",
       " (u'H', 0.016207127344124005),\n",
       " (u'J', 0.016207127344124005),\n",
       " ('B', 0.3632359489889102),\n",
       " ('D', 0.03938466342002967),\n",
       " ('F', 0.03938466342002967)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum(graph.map(lambda n:n[1]['p']).collect())\n",
    "graph.map(lambda n:(n[0], n[1]['p']/G)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'J', ['E']),\n",
       " (u'K', ['E']),\n",
       " (u'B', ['C']),\n",
       " (u'C', ['B']),\n",
       " (u'E', ['B', 'D', 'F']),\n",
       " (u'F', ['B', 'E']),\n",
       " (u'G', ['B', 'E']),\n",
       " (u'H', ['B', 'E']),\n",
       " (u'I', ['B', 'E']),\n",
       " (u'D', ['A', 'B'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.sortBy(lambda l: l[1][0], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "#!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
