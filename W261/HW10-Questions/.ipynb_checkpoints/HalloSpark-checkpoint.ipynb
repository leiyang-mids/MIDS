{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.9 (default, Dec 15 2014 10:37:34)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/leiyang/Downloads/spark-1.6.1-bin-hadoop2.6'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataRDD = sc.parallelize(np.random.random_sample(1000))   \n",
    "data2X= dataRDD.map(lambda x: x*2)\n",
    "dataGreaterThan1 = data2X.filter(lambda x: x > 1.0)\n",
    "cachedRDD = dataGreaterThan1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedRDD.filter(lambda x: x<1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26842824317966052,\n",
       " 0.22885512655070683,\n",
       " 0.9758237846106288,\n",
       " 0.33859930537719629,\n",
       " 0.0070283515932643148,\n",
       " 1.9340366746533655,\n",
       " 0.14436696744010025,\n",
       " 0.16345393294606159,\n",
       " 0.19683340264192073,\n",
       " 0.45103274216591682]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2X.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###transformation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), ('a', 34), ('b', 8)])\n",
    "def add1(a, b): return a + [b]\n",
    "def add2(a, b): return a + b\n",
    "def test(a): return [a]\n",
    "b = x.combineByKey(test, add1, add2).collect()\n",
    "c = x.reduceByKey(lambda a,b: [a]+[b]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"B\\t{'C': 1}\",\n",
       " u\"C\\t{'B': 1}\",\n",
       " u\"D\\t{'A': 1, 'B': 1}\",\n",
       " u\"E\\t{'D': 1, 'B': 1, 'F': 1}\",\n",
       " u\"F\\t{'B': 1, 'E': 1}\",\n",
       " u\"G\\t{'B': 1, 'E': 1}\",\n",
       " u\"H\\t{'B': 1, 'E': 1}\",\n",
       " u\"I\\t{'B': 1, 'E': 1}\",\n",
       " u\"J\\t{'E': 1}\",\n",
       " u\"K\\t{'E': 1}\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sc.textFile('hdfs://localhost:9000/user/leiyang/PageRank-test.txt').cache()\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "def initialize(line):          \n",
    "    # parse line\n",
    "    nid, adj = line.strip().split('\\t', 1)     \n",
    "    exec 'adj = %s' %adj\n",
    "    # initialize node struct        \n",
    "    node = {'a':adj.keys(), 'p':0}\n",
    "    rankMass = 1.0/len(adj)    \n",
    "    # emit pageRank mass and node    \n",
    "    return [(m, rankMass) for m in node['a']] + [(nid.strip('\"'), node)]\n",
    "\n",
    "def accumulateMass(a, b):\n",
    "    if isinstance(a, float) and isinstance(b, float):\n",
    "        return a+b\n",
    "    if isinstance(a, float) and not isinstance(b, float):\n",
    "        b['p'] += a\n",
    "        return b\n",
    "    else:\n",
    "        a['p'] += b\n",
    "        return a\n",
    "    \n",
    "def getDangling(node):    \n",
    "    global nDangling\n",
    "    if isinstance(node[1], float):\n",
    "        nDangling += 1\n",
    "        return (node[0], {'a':[], 'p':node[1]})\n",
    "    else:\n",
    "        return node\n",
    "    \n",
    "def redistributeMass(node):\n",
    "    node[1]['p'] = (p_dangling.value+node[1]['p'])*damping + alpha\n",
    "    return node\n",
    "    \n",
    "def distributeMass(node):    \n",
    "    global lossMass\n",
    "    mass, adj = node[1]['p'], node[1]['a']\n",
    "    node[1]['p'] = 0\n",
    "    if len(adj) == 0:\n",
    "        lossMass += mass\n",
    "        return [node]\n",
    "    else:\n",
    "        rankMass = mass/len(adj)\n",
    "        return [(x, rankMass) for x in adj]+[node]\n",
    "\n",
    "def logTime():\n",
    "    return str(datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-15 21:48:28.606817: start PageRank initialization ...\n",
      "2016-04-15 21:48:29.283494: initialization completed, dangling node(s): 1, total nodes: 11\n",
      "2016-04-15 21:48:29.284133: running iteration 2 ...\n",
      "2016-04-15 21:48:29.381162: redistributing loss mass: 0.6523\n",
      "2016-04-15 21:48:29.386752: running iteration 3 ...\n",
      "2016-04-15 21:48:29.470348: redistributing loss mass: 0.4174\n",
      "2016-04-15 21:48:29.475617: running iteration 4 ...\n",
      "2016-04-15 21:48:29.557635: redistributing loss mass: 0.7042\n",
      "2016-04-15 21:48:29.562555: running iteration 5 ...\n",
      "2016-04-15 21:48:29.647760: redistributing loss mass: 0.4136\n",
      "2016-04-15 21:48:29.653744: running iteration 6 ...\n",
      "2016-04-15 21:48:29.734994: redistributing loss mass: 0.4254\n",
      "2016-04-15 21:48:29.741772: running iteration 7 ...\n",
      "2016-04-15 21:48:29.823619: redistributing loss mass: 0.3753\n",
      "2016-04-15 21:48:29.828607: running iteration 8 ...\n",
      "2016-04-15 21:48:29.918053: redistributing loss mass: 0.3812\n",
      "2016-04-15 21:48:29.924970: running iteration 9 ...\n",
      "2016-04-15 21:48:30.007368: redistributing loss mass: 0.3659\n",
      "2016-04-15 21:48:30.014499: running iteration 10 ...\n",
      "2016-04-15 21:48:30.098850: redistributing loss mass: 0.3660\n",
      "2016-04-15 21:48:30.148752: normalized weight of the graph: 1.0000\n",
      "2016-04-15 21:48:30.148867: PageRanking completed in 0.03 minutes, top 100 are:\n",
      "[('B', 0.3632359489889102), ('C', 0.36288372803871793), (u'E', 0.08114525762548769), ('D', 0.03938466342002967), ('F', 0.03938466342002967), ('A', 0.03293010178620472), (u'G', 0.016207127344124005), (u'I', 0.016207127344124005), (u'K', 0.016207127344124005), (u'H', 0.016207127344124005), (u'J', 0.016207127344124005)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "# load the graph\n",
    "graph_file = sc.textFile('hdfs://localhost:9000/user/leiyang/PageRank-test.txt')\n",
    "\n",
    "nDangling = sc.accumulator(0)\n",
    "lossMass = sc.accumulator(0.0)\n",
    "damping = 0.85\n",
    "alpha = 1 - damping\n",
    "nTop, nIter = 100, 10\n",
    "start = time()\n",
    "print '%s: start PageRank initialization ...' %(logTime())\n",
    "graph = graph_file.flatMap(initialize).reduceByKey(accumulateMass).map(getDangling).cache()\n",
    "# get graph size\n",
    "G = graph.count()\n",
    "# broadcast dangling mass for redistribution\n",
    "p_dangling = sc.broadcast(1.0*nDangling.value/G)\n",
    "graph = graph.map(redistributeMass)\n",
    "print '%s: initialization completed, dangling node(s): %d, total nodes: %d' %(logTime(), nDangling.value, G)\n",
    "\n",
    "for i in range(nIter-1):\n",
    "    print '%s: running iteration %d ...' %(logTime(), i+2)\n",
    "    lossMass.value = 0.0\n",
    "    graph = graph.flatMap(distributeMass).reduceByKey(accumulateMass).cache() #checkpoint()?\n",
    "    # need to call an action here in order to have loss mass\n",
    "    graph.count()        \n",
    "    print '%s: redistributing loss mass: %.4f' %(logTime(), lossMass.value)\n",
    "    p_dangling = sc.broadcast(lossMass.value/G)\n",
    "    graph = graph.map(redistributeMass)\n",
    "\n",
    "totalMass = graph.aggregate(0, (lambda x, y: y[1]['p'] + x), (lambda x, y: x+y))\n",
    "print '%s: normalized weight of the graph: %.4f' %(logTime(), totalMass/G)\n",
    "print '%s: PageRanking completed in %.2f minutes, top %d are:' %(logTime(), (time()-start)/60.0, nTop)\n",
    "print graph.map(lambda n:(n[0],n[1]['p']/G)).sortBy(lambda n: n[1], ascending=False).take(nTop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.999999999999998"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: y[1]['p'] + x)\n",
    "comOp = (lambda x, y: x+y)\n",
    "graph.aggregate(0, (lambda x, y: y[1]['p'] + x), (lambda x, y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 0.03293010178620472),\n",
       " ('C', 0.36288372803871793),\n",
       " (u'E', 0.08114525762548769),\n",
       " (u'G', 0.016207127344124005),\n",
       " (u'I', 0.016207127344124005),\n",
       " (u'K', 0.016207127344124005),\n",
       " (u'H', 0.016207127344124005),\n",
       " (u'J', 0.016207127344124005),\n",
       " ('B', 0.3632359489889102),\n",
       " ('D', 0.03938466342002967),\n",
       " ('F', 0.03938466342002967)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum(graph.map(lambda n:n[1]['p']).collect())\n",
    "graph.map(lambda n:(n[0], n[1]['p']/G)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"'Node_a\\tA\\t2\\t0\", u\"'Node_b\\tB\\t1\\t0\", u\"'Node_c\\tC\\t1\\t0\", u\"'Node_d\\tD\\t1\\t0\", u\"'Node_e\\tE\\t2\\t0\", u\"'Node_f\\tF\\t1\\t0\", u\"'Node_g\\tG\\t1\\t0\", u\"'Node_h\\tH\\t1\\t0\", u\"'Node_i\\tI\\t1\\t0\", u\"'Node_j\\tJ\\t1\\t0\", u\"'Node_k\\tK\\t1\\t0\", u\"'crap_k\\tp\\t1\\t0\"]\n"
     ]
    }
   ],
   "source": [
    "#nodes.sortBy(lambda l: l[1][0], ascending=False).take(10)\n",
    "print sc.textFile('file:///Users/leiyang/Downloads/toy_index.txt').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Invoke Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!spark-1.6.1-bin-hadoop2.6/bin/spark-submit \\\n",
    "--master 'spark://host:7077' \\\n",
    "--deploy-mode 'client' \\\n",
    "--name 'PageRank' \\\n",
    "--py-files PageRank.py \\\n",
    "--executor-memory '512m' \\\n",
    "--driver-memory '512m' \\\n",
    "PageRankDriver.py \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "#!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
