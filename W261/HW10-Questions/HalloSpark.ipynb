{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.9 (default, Dec 15 2014 10:37:34)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/leiyang/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "\n",
    "dataRDD = sc.parallelize(np.random.random_sample(1000))   \n",
    "data2X= dataRDD.map(lambda x: x*2)\n",
    "dataGreaterThan1 = data2X.filter(lambda x: x > 1.0)\n",
    "cachedRDD = dataGreaterThan1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedRDD.filter(lambda x: x<1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26842824317966052,\n",
       " 0.22885512655070683,\n",
       " 0.9758237846106288,\n",
       " 0.33859930537719629,\n",
       " 0.0070283515932643148,\n",
       " 1.9340366746533655,\n",
       " 0.14436696744010025,\n",
       " 0.16345393294606159,\n",
       " 0.19683340264192073,\n",
       " 0.45103274216591682]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2X.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###transformation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), ('a', 34), ('b', 8)])\n",
    "def add1(a, b): return a + [b]\n",
    "def add2(a, b): return a + b\n",
    "def test(a): return [a]\n",
    "b = x.combineByKey(test, add1, add2).collect()\n",
    "c = x.reduceByKey(lambda a,b: [a]+[b]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"B\\t{'C': 1}\",\n",
       " u\"C\\t{'B': 1}\",\n",
       " u\"D\\t{'A': 1, 'B': 1}\",\n",
       " u\"E\\t{'D': 1, 'B': 1, 'F': 1}\",\n",
       " u\"F\\t{'B': 1, 'E': 1}\",\n",
       " u\"G\\t{'B': 1, 'E': 1}\",\n",
       " u\"H\\t{'B': 1, 'E': 1}\",\n",
       " u\"I\\t{'B': 1, 'E': 1}\",\n",
       " u\"J\\t{'E': 1}\",\n",
       " u\"K\\t{'E': 1}\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sc.textFile('hdfs://localhost:9000/user/leiyang/PageRank-test.txt').cache()\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', {'a': [], 'p': 0.5}),\n",
       " ('C', {'a': ['B'], 'p': 1.0}),\n",
       " (u'E', {'a': ['B', 'D', 'F'], 'p': 4.0}),\n",
       " (u'G', {'a': ['B', 'E'], 'p': 0}),\n",
       " (u'I', {'a': ['B', 'E'], 'p': 0}),\n",
       " (u'K', {'a': ['E'], 'p': 0}),\n",
       " (u'H', {'a': ['B', 'E'], 'p': 0}),\n",
       " (u'J', {'a': ['E'], 'p': 0}),\n",
       " (u'B', {'a': ['C'], 'p': 3.833333333333333}),\n",
       " (u'D', {'a': ['A', 'B'], 'p': 0.3333333333333333}),\n",
       " ('F', {'a': ['B', 'E'], 'p': 0.3333333333333333})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "def initialize(line):          \n",
    "    # parse line\n",
    "    nid, adj = line.strip().split('\\t', 1)     \n",
    "    exec 'adj = %s' %adj\n",
    "    # initialize node struct        \n",
    "    node = {'a':adj.keys(), 'p':0}\n",
    "    rankMass = 1.0/len(adj)    \n",
    "    # emit pageRank mass and node    \n",
    "    return [(m, rankMass) for m in node['a']] + [(nid.strip('\"'), node)]\n",
    "\n",
    "def accumulateMass(a, b):\n",
    "    if isinstance(a, float) and isinstance(b, float):\n",
    "        return a+b\n",
    "    if isinstance(a, float) and not isinstance(b, float):\n",
    "        b['p'] += a\n",
    "        return b\n",
    "    else: #if not isinstance(a, float) and isinstance(b, float):\n",
    "        a['p'] += b\n",
    "        return a\n",
    "    \n",
    "def getDangling(node):    \n",
    "    if isinstance(node[1], float):\n",
    "        nDangling += 1\n",
    "        return (node[0], {'a':[], 'p':node[1]})\n",
    "    else:\n",
    "        return node\n",
    "    \n",
    "def redistributeMass(node):\n",
    "    node[1]['p'] = (p_dangling+node[1]['p'])*damping + alpha\n",
    "    return node\n",
    "    \n",
    "def distributeMass(node):    \n",
    "    mass, adj = node[1]['p'], node[1]['a']\n",
    "    node[1]['p'] = 0\n",
    "    if len(adj) == 0:\n",
    "        lossMass += mass\n",
    "        return node\n",
    "    else:\n",
    "        rankMass = mass/len(adj)\n",
    "        return [(x, rankMass) for x in adj]+[node]\n",
    "\n",
    "def logTime():\n",
    "    return str(datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nDangling = sc.accumulator(0)\n",
    "lossMass = sc.accumulator(0.0)\n",
    "G = p_dangling = 0\n",
    "damping = 0.85\n",
    "alpha = 1 - damping\n",
    "nTop = 100\n",
    "start = time()\n",
    "print '%: start PageRank initialization ...' %(logTime())\n",
    "graph = y.flatMap(initialize).reduceByKey(accumulateMass).map(getDangling) #.collect()\n",
    "lossMass = nDangling\n",
    "G = graph.count()\n",
    "p_dangling = lossMass/G\n",
    "graph = graph.map(redistributeMass)\n",
    "print '%s: initialization completed, dangling node(s): %d, total nodes: %d' %(logTime(), nDangling, G)\n",
    "\n",
    "for i in range(10-1):\n",
    "    print '%s: running iteration %d ...' %(logTime(), i+2)\n",
    "    lossMass = 0.0\n",
    "    graph = graph.faltMap(distributeMass).reduceByKey(accumulateMass)\n",
    "    p_dangling = lossMass/G\n",
    "    print '%s: redistributing loss mass: %.4f' %(logTime(), lossMass)\n",
    "    graph = graph.map(redistributeMass)\n",
    "\n",
    "print '%s: PageRanking completed in %.1f minutes, top %d are:' %((time()-start)/60.0, nTop)\n",
    "graph.sortBy(lambda n: n[1]['p'], ascending=False).take(nTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'J', ['E']),\n",
       " (u'K', ['E']),\n",
       " (u'B', ['C']),\n",
       " (u'C', ['B']),\n",
       " (u'E', ['B', 'D', 'F']),\n",
       " (u'F', ['B', 'E']),\n",
       " (u'G', ['B', 'E']),\n",
       " (u'H', ['B', 'E']),\n",
       " (u'I', ['B', 'E']),\n",
       " (u'D', ['A', 'B'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.sortBy(lambda l: l[1][0], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "#!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
