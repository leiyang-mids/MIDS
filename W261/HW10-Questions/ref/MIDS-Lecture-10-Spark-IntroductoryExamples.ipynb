{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "name": "",
  "signature": "sha256:6a507f9b038068f72d460f07fd8d031ed89fa0e2c07a05ac778a90cbc70f73fa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "MIDS-Lecture-10-Spark-IntroductoryExamples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys #current as of 9/26/2015\n",
      "spark_home = os.environ['SPARK_HOME'] = \\\n",
      "   '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.5.0-bin-hadoop2.6/'\n",
      "\n",
      "if not spark_home:\n",
      "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
      "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
      "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
      "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Welcome to\n",
        "      ____              __\n",
        "     / __/__  ___ _____/ /__\n",
        "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
        "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0\n",
        "      /_/\n",
        "\n",
        "Using Python version 2.7.9 (v2.7.9:648dcafa7e5f, Dec 10 2014 10:10:46)\n",
        "SparkContext available as sc, HiveContext available as sqlContext.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      " \n",
      "\n",
      "dataRDD = sc.parallelize(np.random.random_sample(1000))   \n",
      "data2X= dataRDD.map(lambda x: x*2)\n",
      "dataGreaterThan1 = data2X.filter(lambda x: x > 1.0)\n",
      "cachedRDD = dataGreaterThan1.cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cachedRDD.filter(lambda x: x<1).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cachedRDD.filter(lambda x: x>1).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "501"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cachedRDD.filter(lambda x: x>1).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "501"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for line in cachedRDD.take(10):\n",
      "    print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.27635884545\n",
        "1.55510407128\n",
        "1.37779018564\n",
        "1.59876656856\n",
        "1.80699073308\n",
        "1.67930938099\n",
        "1.99910019948\n",
        "1.44956955743\n",
        "1.000141101\n",
        "1.12453560506\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#RDD mapper\n",
      "rdd = sc.parallelize([1,2,3,3])\n",
      "rdd.map(lambda x:  x + 1).collect()\n",
      "#returns [2, 3, 4, 4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#reduce ACTION\n",
      "rdd = sc.parallelize([1,2,3,3])\n",
      "sum = rdd.reduce(lambda x, y: x + y)\n",
      "# sum.collect()\n",
      "print sum    #returns 9"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#RDD flatmap ranges of integer ranges\n",
      "rdd = sc.parallelize([1,2,3,3])\n",
      "rdd.flatMap(lambda x:  range(x, 8)).collect()\n",
      "rdd.flatMap(lambda x:  range(x, 10)).distinct().collect()\n",
      "#returns [8, 1, 9, 2, 3, 4, 5, 6, 7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Example 4-1. Creating a pair RDD using the first word as the key in Python\n",
      "\n",
      "lines = sc.parallelize([\"Data line 1\", \"Mining line 2\", \"data line 3\", \"Data line 4\", \"Data Mining line 5\"])\n",
      "pairs = lines.map(lambda x: (x.split(\" \")[0], x))  #first word and the original line\n",
      "pairs.collect()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[('Data', 'Data line 1'),\n",
        " ('Mining', 'Mining line 2'),\n",
        " ('data', 'data line 3'),\n",
        " ('Data', 'Data line 4'),\n",
        " ('Data', 'Data Mining line 5')]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filterTerm(line):\n",
      "    if 'data' in line[1]:\n",
      "        return (line[0])\n",
      "    \n",
      "result = pairs.filter(filterTerm)\n",
      "result.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[('data', 'data line 3')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = pairs.filter(lambda keyValue: len(keyValue[1]) < 12)\n",
      "result.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[('Data', 'Data line 1'), ('data', 'data line 3'), ('Data', 'Data line 4')]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PAIRED RDD with super efficient word count example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.parallelize([\"Data line 1\", \"Mining line 2\", \"data line 3\", \"Data line 4\", \"Data Mining line 5\"])\n",
      "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
      "             .map(lambda word: (word, 1)) \\\n",
      "             .reduceByKey(lambda a, b: a + b)\n",
      "        \n",
      "counts.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[('1', 1),\n",
        " ('line', 5),\n",
        " ('Mining', 2),\n",
        " ('3', 1),\n",
        " ('2', 1),\n",
        " ('data', 1),\n",
        " ('5', 1),\n",
        " ('Data', 3),\n",
        " ('4', 1)]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a more efficent word counter (less network communication)\n",
      "#could hash and do an in-memory combiner at the record level\n",
      "lines = sc.parallelize([\"Data line 1\", \"Mining line 2\", \"data line 3\", \"Data line 4\", \"Data Mining line 5\"])\n",
      "def emitWordCounts(line):\n",
      "    wordCounts = []\n",
      "    for w in line.split(\" \"):\n",
      "        wordCounts.append((w, 1))  #could hash and do an in-memory combiner to be more efficient\n",
      "    return (wordCounts)\n",
      "    \n",
      "counts = lines.flatMap(emitWordCounts) #\\\n",
      "             #.reduceByKey(lambda a, b: a + b)\n",
      "#counts.first()  \n",
      "counts.collect()\n",
      "\n",
      "#Level 1 DEBUG Get the mapper to work in isolation  first\n",
      "#emitWordCounts(\"Mining line 2\")   \n",
      "\n",
      "#Level 2 debug check the mapper is working\n",
      "counts = lines.flatMap(emitWordCounts) #\\\n",
      "#             .reduceByKey(lambda a, b: a + b)\n",
      "counts.collect()\n",
      "\n",
      "#Level 3 debug check the mapper is working\n",
      "counts = lines.flatMap(emitWordCounts) \\\n",
      "            .reduceByKey(lambda a, b: a + b)\n",
      "counts.collect()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[('Data', 1),\n",
        " ('line', 1),\n",
        " ('1', 1),\n",
        " ('Mining', 1),\n",
        " ('line', 1),\n",
        " ('2', 1),\n",
        " ('data', 1),\n",
        " ('line', 1),\n",
        " ('3', 1),\n",
        " ('Data', 1),\n",
        " ('line', 1),\n",
        " ('4', 1),\n",
        " ('Data', 1),\n",
        " ('Mining', 1),\n",
        " ('line', 1),\n",
        " ('5', 1)]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a more efficent word counter (less network communication)\n",
      "lines = sc.parallelize([\"Data line 1\", \"Mining line 2\", \"data line 3\", \"Data line 4\", \"Data Mining line 5\"])\n",
      "def emitWordCounts(line):\n",
      "    wordCounts = []\n",
      "    for w in line.split(\" \"):\n",
      "        wordCounts.append((w, 1))  #could hash and do an in-memory combiner to be more efficient\n",
      "    return (wordCounts)\n",
      "    \n",
      "counts = lines.flatMap(emitWordCounts) \\\n",
      "             .reduceByKey(lambda a, b: a + b)\n",
      "counts.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[('1', 1),\n",
        " ('line', 5),\n",
        " ('Mining', 2),\n",
        " ('3', 1),\n",
        " ('2', 1),\n",
        " ('data', 1),\n",
        " ('5', 1),\n",
        " ('Data', 3),\n",
        " ('4', 1)]"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a more efficent word counter (less network communication)\n",
      "# hash and do an in-memory combiner at the record level\n",
      "lines = sc.parallelize([\"Data line 1\", \"Mining line 2\", \"data line 3\", \"Data line 4\", \"Data Mining line 5\"])\n",
      "def emitWordCounts(line):\n",
      "    wordCounts = {}\n",
      "    for w in line.split(\" \"):\n",
      "        if wordCounts.has_key(w):\n",
      "            wordCounts[w]=wordCounts[w]+1\n",
      "        else:\n",
      "            wordCounts[w]=1\n",
      "    for key, value in  wordCounts.items():\n",
      "        print key,\":\", value      \n",
      "    return (wordCounts.items())  \n",
      "\n",
      "#Level 1 DEBUG Get the mapper to work in isolation  first\n",
      "#emitWordCounts(\"Mining line 2\")   \n",
      "\n",
      "#Level 2 debug check the mapper is working\n",
      "counts = lines.flatMap(emitWordCounts) #\\\n",
      "#             .reduceByKey(lambda a, b: a + b)\n",
      "counts.collect()\n",
      "\n",
      "#Level 3 debug check the mapper is working\n",
      "counts = lines.flatMap(emitWordCounts) \\\n",
      "            .reduceByKey(lambda a, b: a + b)\n",
      "counts.collect()\n",
      "\n",
      "#map partitions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[('1', 1),\n",
        " ('data', 1),\n",
        " ('5', 1),\n",
        " ('Data', 3),\n",
        " ('4', 1),\n",
        " ('line', 5),\n",
        " ('Mining', 2),\n",
        " ('3', 1),\n",
        " ('2', 1)]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd1 = sc.parallelize({(1, 2), (3, 4), (3, 6)})\n",
      "rdd2 = sc.parallelize({(3, 9), (3, 6)})\n",
      "joinedRdd=rdd1.join(rdd2)\n",
      "#RDD1 = {(1, 2), (3, 4), (3, 6)}\n",
      "#RDD2 = {(3, 9) (3, 6)}\n",
      "joinedRdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 6-14 in Learn Spark Book. Average with mapPartitions() in Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def combineCtrs(c1, c2):\n",
      "    return (c1[0] + c2[0], c1[1] + c2[1])\n",
      "def basicAvg(nums):\n",
      "    \"\"\"Compute the average\"\"\"\n",
      "    nums.map(lambda num: (num, 1)).reduce(combineCtrs)\n",
      "\n",
      "def partitionCtr(nums):\n",
      "    \"\"\"Compute sumCounter for partition\"\"\"\n",
      "    sumCount = [0, 0]\n",
      "    for num in nums:\n",
      "        sumCount[0] += num\n",
      "        sumCount[1] += 1\n",
      "    return [sumCount]\n",
      "\n",
      "def fastAvg(nums):\n",
      "    \"\"\"Compute the avg\"\"\"\n",
      "    sumCount = nums.mapPartitions(partitionCtr).reduce(combineCtrs)\n",
      "    return sumCount[0] / float(sumCount[1])\n",
      "\n",
      "nums = fastAvg(sc.parallelize([1,2,3,4]))\n",
      "print(nums)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.5\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize([1,2,3,3]).filter(lambda x: x<3)\n",
      "rdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "[1, 2]"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}