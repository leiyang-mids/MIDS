{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 Solution, DATSCI W261, Master Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FirstName LastName  \n",
    "Email address  \n",
    "Time of Initial Submission: 9:21 PM EST, Monday, January 18, 2016  \n",
    "Time of **Resubmission**: 8:38 AM EST, Friday, January 22, 2016 \n",
    "W261-3, Spring 2016  \n",
    "Week 1 Homework\n",
    "\n",
    "**Acknowledgements**: Jake Ryland Williams, Liang Dai, Nick Hamlin, James G. Shanahan, Patrick Ng, amongst others "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### |Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/jylzkmauxkostck/AAA2pH0cTvb0zDrbbbze3zf-a/hw1_instructions.txt?dl=0)**\n",
    "- [Wikipedia explaination of Naive Bayes document classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification)\n",
    "- [Original paper describing the background of the Enron email corpus](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf)\n",
    "- [Documentation for Scikit-Learn implementation of Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Stanford NLP Group's explaination of Naive Bayes algorithm](http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html)\n",
    "\n",
    "#/Users/jshanahan/anaconda/bin/ipython notebook&"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0: Define big data. Provide an example of a big data problem in your domain of expertise.\n",
    "Data too large to fit on or be processed by a single machine or tradional techniques is big.\n",
    "Big data is also well characterized by the four V's: Volume, Variety, Velocity, and Veracity.\n",
    "In short there is no one definition or definiing characteristic of big data,\n",
    "which is exemplified by the range of domains from which one can draw examples. \n",
    "Other possible talking points for this question: PROCESSING:  Think of your laptop that gets overwhelmed with 3-4 gig of data (disk space is 1TB) \n",
    "STORAGE:  Laptop : 1 TB\n",
    "THROUGH-PUT 1TB would take 3 hours to read it using your laptop. Is this satisfactory? NO!\n",
    "\n",
    "For the quantitative linguist, the text generated by social media is a prime example of big data---it \n",
    "is free form in nature (variety) and generated in real time (velocity) by millions of individuals simultaneously\n",
    "(volume) whose signals are very often muddied by automatons and spammers (veracity).\n",
    "\n",
    "### HW1.0.1: In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?\n",
    "Estimating bias and variance for polynomial models on a dataset with unknown underlying functional form\n",
    "will generally be predicated on a bootstrapping procedure, creating an ensemble of models for each polynomial degree.\n",
    "The model variance will be calculated as the average deviation from the average model (for each degree).\n",
    "Measuring bias is unfortunately somewhat less direct, as one cannot separate the irreducible error \n",
    "(noise) present in the data. To determine an estimate of model bias, one can compute the average \n",
    "squared error (across an ensemble) from the testing data and subtract off the model variance, \n",
    "rendering a value that is the sum of irreducible error and squared bias. Ultimately, to choose a model degree\n",
    "one can minimize the overall error, but should also be sure to keep a balance between bias and variance indivdiually,\n",
    "aiming to keep both as low as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.0.1.\n",
    "*In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreducible error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?*\n",
    "\n",
    "In the case of regression problems where we estimate mean squared error (MSE), we can decompose the overall expected error in terms of the squared bias, variance, and irreducible error:\n",
    "\n",
    "$$\n",
    "E[(\\hat{y}-y)^2]=(E[\\hat{y}]-y_{true})^2+E[\\hat{y}-E[\\hat{y}]]^2+E[(y_{true}-y)^2]\n",
    "$$\n",
    "\n",
    "$$E[(y-h(x))2]  = Bias[h(x)]^2 + Variance[h(x)] + +E[(y_{true}-y)^2]\n",
    "$$\n",
    "\n",
    "Where the h(x) is the model built from each boostrap sample of the test/heldout dataset (see details below).\n",
    "\n",
    " \n",
    "The first term on the RHS is the squared bias, which measures the average error of the model.  The second term is the variance, which measures how much our model's predictions vary from one training set to another.  The final term represent the irreducible error; the variation between our model and reality that we are unable to do anything about (thus the name \"irreducible error\").\n",
    "\n",
    "We'd like to choose the model that minimizes both bias and variance. However, we don't know the true function from which T was derived, so calculating this directly doesn't work. In practice, we can calculate the  Expected Error (and its correspending squared bias, and variance) through boostrap sampling of the available training data  (say N bootsrap samples). Using the boostrap training samples, for each hyperparamter (polynomial regression models of different degrees from 1 to M) setting for the training algorithm in question,  we  create N regression models for each degree of polynomial. Each model is then  used to make predictions over the test data. The resulting predictions can then be used  to calculate a biased-squared term, a variance term and the Expected Mean Squared error for the test set (for each candidate hyperparamter setting). The model that has the lowest combined variance AND  squared bias will be the one we choose as best model. In the event that multiple models produce the minimum combination of bias and variance, we should choose the simplest (in this case, the model using the lowest degree polynomial).\n",
    "\n",
    "Generally, as we increase the degree of the polynomial we use, our bias will drop because we'll fit the points in our training data more closely.  However, this will likely come at a cost of increased variance, as the higher degree polynomials will also mean that our model will be comes less likely to generalize well to new, unseen data points.  Consequently, our ability to successfully classify our training data will fall as our model complexity rises, but our performance on our test sample will begin to suffer as we overfit to the training data.  Referring to the following graph, it is clear that the combine sum of Bias squared and Variance E[MSE] is lowest for a polynomial regression model. This happens to coincide with the E[MSE] with respect to the test set. Note the test error in this graph includes the irreducible error here since we know the true function (in this artifical example). For more details on this example see https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/.\n",
    "\n",
    "\n",
    "![title](bias-variance-tradeoff.png)\n",
    "\n",
    "We can use the following pseudocode for these calculations (other variations are also possible such as having an independent test dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CASE 1 in a simulated world where we know the ground truth\n",
    "#HW1.0.1 Pseudocode\n",
    "# Given \n",
    "# A training data samples of the form (X, yObserved) that is generated from a noisy function fNoisy\n",
    "# and a test set has the form X, y_true in our artificial world that is generated from \n",
    "# Running our simulation generates a new column of predictions for each row of the form\n",
    "#     X, y_true, h_star1, ...h_star_n,\n",
    "# \n",
    "for model in models:\n",
    "    #this is the bagging step needed to calculate variance\n",
    "    #where n is some constant (like 50)\n",
    "    for iteration from 1:n \n",
    "        generate a sample of data from our noisy function \n",
    "        Train model using train_data\n",
    "        h_star=predict results for test_data\n",
    "    h_bar=calculate average prediction across all iterations\n",
    "    #y_true is the vector of true classes in the test_data\n",
    "    bias=h_bar-y_true \n",
    "    variance=sum((h_bar-h_star)^2)/n #in practice, one would need to go through each iteration to compute this\n",
    "    noise=mean((y_true-h_star)^2) #As with variance, this needs to be calculated across all iterations\n",
    "    \n",
    "choose model that minimizes (bias^2+variance)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CASE 2 in the real world when we do NOT know the true target function\n",
    "\n",
    "#HW1.0.1 Pseudocode\n",
    "# Given training data of the form (X, yObserved) \n",
    "# y_true is true value but in most real world problems we do NOT know the ground truth \n",
    "# So in practice we set y_true =  yObserved and set noise =0\n",
    "for model in models:\n",
    "    #this is the bagging step needed to calculate variance\n",
    "    #where n is some constant (like 50)\n",
    "    for iteration from 1:n \n",
    "        Split training data randomly into train_data and test_data\n",
    "        Train model using train_data\n",
    "        h_star=predict results for test_data\n",
    "    h_bar=calculate average prediction across all iterations\n",
    "    #y_true is the vector of true classes in the test_data\n",
    "    bias=h_bar-y_true \n",
    "    variance=sum((h_bar-h_star)^2)/n #in practice, one would need to go through each iteration to compute this\n",
    "    #noise=mean((y_true-h_star)^2) #As with variance, this needs to be calculated across all iterations\n",
    "    noise=0 #set the noise =0 since we can NOT estimate it\n",
    "choose model that minimizes (bias^2+variance)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1: Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below.\n",
    "No response required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## pNaiveBayes.sh\n",
      "## Author: Jake Ryland Williams\n",
      "## Usage: pNaiveBayes.sh m wordlist\n",
      "## Input:\n",
      "##       m = number of processes (maps), e.g., 4\n",
      "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
      "##\n",
      "## Instructions: Read this script and its comments closely.\n",
      "##               Do your best to understand the purpose of each command,\n",
      "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
      "##               as this will determine how the python scripts take input.\n",
      "##               When you are comfortable with the unix code below,\n",
      "##               answer the questions on the LMS for HW1 about the starter code.\n",
      "\n",
      "## collect user input\n",
      "m=$1 ## the number of parallel processes (maps) to run\n",
      "wordlist=$2 ## if set to \"*\", then all words are used\n",
      "\n",
      "## a test set data of 100 messages\n",
      "data=\"enronemail_1h.txt\" \n",
      "\n",
      "## the full set of data (33746 messages)\n",
      "# data=\"enronemail.txt\" \n",
      "\n",
      "## 'wc' determines the number of lines in the data\n",
      "## 'perl -pe' regex strips the piped wc output to a number\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
      "\n",
      "## determine the lines per chunk for the desired number of processes\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
      "\n",
      "## split the original file into chunks by line\n",
      "split -l $linesinchunk $data $data.chunk.\n",
      "\n",
      "## assign python mappers (mapper.py) to the chunks of data\n",
      "## and emit their output to temporary files\n",
      "for datachunk in $data.chunk.*; do\n",
      "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
      "    ####\n",
      "    ####\n",
      "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
      "    ####\n",
      "    ####\n",
      "done\n",
      "## wait for the mappers to finish their work\n",
      "wait\n",
      "\n",
      "## 'ls' makes a list of the temporary count files\n",
      "## 'perl -pe' regex replaces line breaks with spaces\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
      "\n",
      "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
      "####\n",
      "####\n",
      "./reducer.py $countfiles > $data.output\n",
      "####\n",
      "####\n",
      "\n",
      "## clean up the data chunks and temporary count files\n",
      "\\rm $data.chunk.*\n",
      "\n",
      "Question 1.1: DONE\n"
     ]
    }
   ],
   "source": [
    "## HW 1.1 Code\n",
    "\n",
    "#Display contents of pNaiveBayes.sh (it's convenient to keep everything in one notebook)\n",
    "!cat pNaiveBayes.sh\n",
    "!echo \"\"\n",
    "!echo \"Question 1.1: DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.2: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occur- rences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "Note that we use the same mapper for each of parts 1.2-1.5 if we structure the code\n",
    "so that records are printed out at the email ID level, i.e., the email IDs should be the keys\n",
    "(this will help with classification later). In fact, this part (1.2) is the only one for which we will\n",
    "need to construct a different reducer (since output is aggregated word counts and not a classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jake Ryland Williams\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "        \n",
    "## open the input file\n",
    "with open (filename, \"r\") as myfile:\n",
    "    ## loop over the lines\n",
    "    for line in myfile.readlines( ) :\n",
    "\n",
    "        ## check to see if we are counting up a specified set of words\n",
    "        ## and if so initialize the list\n",
    "        ALLWORDS=False\n",
    "        counts = {}        \n",
    "        if len(findwords) == 1 and findwords[0] == \"*\":\n",
    "            ALLWORDS=True\n",
    "        else:\n",
    "            for word in findwords:\n",
    "                counts[word] = 0\n",
    "                counts[word] = 0\n",
    "        \n",
    "        ## strip off line breaks\n",
    "        line = re.sub('\\n','',line)\n",
    "        ## make sure the format of the line is correct\n",
    "        if re.match(\"(.*?)\\t(.*?)\\t(.*?)\\t(.*?)\",line):\n",
    "            ## store the line's data\n",
    "            [ID,SPAM,SUBJECT,CONTENT] = re.split(\"\\t\",line)            \n",
    "            DAT = CONTENT.lower()\n",
    "            counts[\"SPAM\"] = SPAM\n",
    "            if DAT != \"NA\":\n",
    "                ## gather words by a regex split\n",
    "                words = re.split('[^A-Za-z\\-\\']+',DAT)\n",
    "                ## loop over words\n",
    "                for word in words:\n",
    "                    ## clean up each word\n",
    "                    neword = re.sub('^\\'+','',word)\n",
    "                    neword = re.sub('^\\-+','',neword)\n",
    "                    neword = re.sub('\\-+$','',neword)                    \n",
    "                    neword = re.sub('\\'+','\\'',neword)\n",
    "                    neword = re.sub('\\-+','-',neword)\n",
    "                    ## if there is a word left after the regex, count it\n",
    "                    if re.match(\"[a-zA-Z]\",neword):                        \n",
    "                        if ALLWORDS:\n",
    "                            if neword in counts.keys():\n",
    "                                counts[neword] += 1\n",
    "                            else:\n",
    "                                counts[neword] = 1\n",
    "                        elif neword in counts.keys():\n",
    "                            counts[neword] += 1\n",
    "\n",
    "                dicStr = str(counts)\n",
    "                ## print out the counts\n",
    "                print ID + '\\t' + dicStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jake Ryland Williams\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "countfiles = sys.argv[1:len(sys.argv)]\n",
    "\n",
    "## set up local objects\n",
    "counts = {}\n",
    "\n",
    "## loop over count files\n",
    "for filename in countfiles:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        ## loop over the lines\n",
    "        for line in myfile.readlines():\n",
    "            ## clean up data\n",
    "            line = re.sub('\\n','',line)\n",
    "            ## ignore email ID for now\n",
    "            [ID,countStr] = re.split(\"\\t\",line)\n",
    "            ## convert the countStr back to a dictionary of counts from the email\n",
    "            IDcounts = eval(countStr)\n",
    "            for word in IDcounts.keys():\n",
    "                ## ignore SPAM key, which associates to binary email class value\n",
    "                if word != \"SPAM\":\n",
    "                    counts.setdefault(word,0)\n",
    "                    counts[word] += int(IDcounts[word])\n",
    "                    \n",
    "for word in counts.keys():\n",
    "    print \"The word \\\"\" + word + \"\\\" appeared \" + str(counts[word]) + \" times in the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make scripts executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The call for HW1.2:\n",
    "Note that the word assistance appears only 9 times total in the email's contents,\n",
    "and 1 time in the email's subjects. This implementation and those below focus strictly on the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"assistance\" appeared 9 times in the dataset.\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\"; cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.3: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. \n",
    "Here, our reducer must take the word counts from the individual emails passed along from the mapper,\n",
    "store them in memory by email ID key (for classification), and aggregate them to produce a likelihood function.\n",
    "\n",
    "We include 2 versions of HW1.3: \n",
    "\n",
    "-- one without smoothing and without  log probabilities; \n",
    "-- one with smoothing and log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 1.3 - Mapper function\n",
    "#This mapper function will send one line for every instance of every word to the reducer. This approach, while easier to write and debug, is unlikely to be the best choice for a larger scale implementation because of the large volume of data that would have to be sent to the reducers. A potentially more-streamlined alternative would be to add a \"combiner\" step at the end of the mapper that would send one line for each word-email combination (E.G. Key:email-word-flag, Value:count).\n",
    "#In addition, if we only care about generating the conditional probabilities for each word and don't need to classify all the training emails, we could simplify the implementation even more and not send the email contents themselves to the reducer. Not only would this decrease throughput, but it would also dramatically reduce the amount of information that the reducer would need to store in memory. In this situation, our mapper could simply emit words along with their conditional class counts. I have not implemented this in this homework, but we'll want to keep this in mind for the future.\n",
    " \n",
    "#HW 1.3 - Mapper Function Code\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].lower() \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject and body fields from the line, and combine into one string\n",
    "        words=re.findall(WORD_RE,subject_and_body) #create list of words\n",
    "        for word in words:\n",
    "            #This flag indicates to the reducer that a given word should be considered\n",
    "            #by the reducer when calculating the conditional probabilities\n",
    "            flag=0\n",
    "            if word in findwords:\n",
    "                flag=1 \n",
    "                \n",
    "            #This will send one row for every word instance to the reducer.\n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1\\t'+str(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW 1.3 - Reducer function\n",
    "#The reducer maintains two associative arrays. \n",
    "#The first stores information about each word, including how \n",
    "#many times it appears in spam and ham messages, as well as if it's been flagged in the mapper. \n",
    "#The  second stores information about emails, including whether it is marked as spam, as well as a list of \n",
    "#words it contains. \n",
    "#As described above, a more scalable solution that does not need to maintain \n",
    "#all the contents of the emails in memory for classification could simply calculate conditional \n",
    "#probabilities \"lazily\" and only store the running probability values rather than the words themselves.\n",
    "#Once all the data has arrived from the mappers, the array containing words is updated with the calculated \n",
    "#conditional probabilities of spam and ham. At this point, the model is \"trained\". Finally, these \n",
    "#conditional probabilities are reapplied to the word lists associated with each email to make the \n",
    "#final spam/ham classification.\n",
    "#Note: I have also included (in the comments) an alternative calculation for conditional probabilities that applies a Laplace smoothing approach, since I'd already implemented it before the assignment instructions were updated. I've done the same thing with the log probability calculations.\n",
    "\n",
    "#HW 1.3 - Reducer Function Code\n",
    "from __future__ import division #Python 3-style division syntax is much cleaner\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "words={}\n",
    "emails={}\n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "flagged_words=[]\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the incoming line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])\n",
    "            word=result[2]\n",
    "            flag=int(result[4])\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0,'flag':flag}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "            \n",
    "            if flag==1 and word not in flagged_words:\n",
    "                flagged_words.append(word)\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    " \n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=1-prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    " \n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - previously used, but removed for now\n",
    "    #p_spam=log(prior_spam)\n",
    "    #p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    for word in email['words']:\n",
    "        if word in flagged_words:\n",
    "            try:\n",
    "                #p_spam+=log(words[word]['p_spam']) #Log version - No longer used\n",
    "                p_spam*=(words[word]['p_spam'])\n",
    "            except ValueError:\n",
    "                continue #This means that words that do not appear in a class will use the class prior\n",
    "            try:\n",
    "                #p_ham+=log(words[word]['p_ham']) #Log version - No longer used\n",
    "                p_ham*=(words[word]['p_ham'])\n",
    "            except ValueError:\n",
    "                continue          \n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.3 - Results\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-01-17.beck\t0\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0015.2001-02-12.kitchen\t0\t1\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t1\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t1\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t1\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t1\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0011.1999-12-14.farmer\t0\t1\n",
      "0013.1999-12-14.kaminski\t0\t1\n",
      "0001.2001-02-07.kitchen\t0\t1\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t1\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0015.1999-12-15.farmer\t0\t1\n",
      "0009.1999-12-13.kaminski\t0\t1\n",
      "0001.2000-06-06.lokay\t0\t1\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0002.1999-12-13.farmer\t0\t1\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t1\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.kaminski\t0\t1\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t0\n"
     ]
    }
   ],
   "source": [
    "#Run our HW 1.3 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!echo \"HW 1.3 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance' only\n",
      "Training error: 0.38\n"
     ]
    }
   ],
   "source": [
    "#HW 1.3-1.6 Training Error Function\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "def calculate_training_error(pred, true):\n",
    "    \"\"\"Calculates the training error given a vector \n",
    "    of predictions and a vector of true classes\"\"\"\n",
    "    \n",
    "    num_wrong=0\n",
    "    for i in zip(pred,true):\n",
    "        if i[0]!=i[1]: #If predicted value doesn't equal true value, increment our count\n",
    "            num_wrong+=1\n",
    "            \n",
    "    #Divide number of incorrect examples by total number of examples in the data\n",
    "    print \"Training error: \"+str(num_wrong/len(pred))\n",
    "    \n",
    "#HW 1.3 Evaluation Code\n",
    "# To install pandas: at the SHELL prompt type conda install pandas \n",
    "import pandas as pd #Use pandas to quickly read results from our output file\n",
    "#conda install pandas \n",
    "def eval_1_3():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance' only\" \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. \n",
    "\n",
    "**VERSION 2: WITH smoothing and log probabilities** \n",
    "\n",
    "Here, our reducer must take the word counts from the individual emails passed along from the mapper,\n",
    "store them in memory by email ID key (for classification), and aggregate them to produce a likelihood function.\n",
    "This will totally be overkill for this part (1.3), since the single-word naive Bayes classifier will simplify\n",
    "to a classification by the prior, and classify all emails to be the class that there was \n",
    "more of in the training set (HAM). However for brevity, and since the procedure is generalizable, \n",
    "we will use the more complicated design below here as well as for HW1.4 and HW1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jake Ryland Williams\n",
    "## Description: reducer code for HW1.3-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "countfiles = sys.argv[1:len(sys.argv)]\n",
    "\n",
    "## set up local objects\n",
    "counts = {}\n",
    "counts[\"HAM\"] = {}\n",
    "counts[\"SPAM\"] = {}\n",
    "counts[\"SPAM\"][\"T-RECORDS\"] = 0 ## for the prior\n",
    "counts[\"HAM\"][\"T-RECORDS\"] = 0 ## for the prior\n",
    "counts[\"SPAM\"][\"T-WORDS\"] = 0 ## for the class conditional prob. denom.\n",
    "counts[\"HAM\"][\"T-WORDS\"] = 0 ## for the class conditional prob. denom.\n",
    "IDS = []\n",
    "\n",
    "## loop over count files\n",
    "for filename in countfiles:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        ## loop over the lines\n",
    "        for line in myfile.readlines():\n",
    "            ## clean up data\n",
    "            line = re.sub('\\n','',line)\n",
    "\n",
    "            [ID,countStr] = re.split(\"\\t\",line)\n",
    "            IDcounts = eval(countStr)\n",
    "            SPAM = int(IDcounts[\"SPAM\"])\n",
    "\n",
    "            ## do some work on the prior\n",
    "            if SPAM:\n",
    "                classKey = \"SPAM\"\n",
    "                nclassKey = \"HAM\"\n",
    "            else:\n",
    "                classKey = \"HAM\"\n",
    "                nclassKey = \"SPAM\"\n",
    "            counts[classKey][\"T-RECORDS\"] += 1\n",
    "\n",
    "            ## store the record's word counts for classification later\n",
    "            counts[ID] = IDcounts\n",
    "            IDS.append(ID)\n",
    "\n",
    "            ## build up the class-level word counts for the likelihood function\n",
    "            for word in IDcounts.keys():\n",
    "                if word != \"SPAM\":\n",
    "\n",
    "                    ## store the count\n",
    "                    count = int(IDcounts[word])\n",
    "                    \n",
    "                    ## initialize (with smoothing) in the classes, if necessary\n",
    "                    if word not in counts[classKey].keys():\n",
    "                        counts[classKey][word] = 1 ## +1 for Laplace smoothing\n",
    "                        counts[nclassKey][word] = 1 ## +1 for Laplace smoothing\n",
    "                        counts[classKey][\"T-WORDS\"] += 1 \n",
    "                        counts[nclassKey][\"T-WORDS\"] += 1\n",
    "\n",
    "                    ## add to the word's class counts\n",
    "                    counts[classKey][word] += count\n",
    "                    counts[classKey][\"T-WORDS\"] += count\n",
    "\n",
    "## compute the prior\n",
    "Ps = float(counts[\"SPAM\"][\"T-RECORDS\"])/(float(counts[\"SPAM\"][\"T-RECORDS\"]) + float(counts[\"HAM\"][\"T-RECORDS\"])) \n",
    "Ph = 1-Ps\n",
    "\n",
    "## initialize components of confusion matrix\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "## loop over the records-level data, applying the bayesian filter to each\n",
    "for ID in IDS:\n",
    "\n",
    "    SPAM = int(counts[ID][\"SPAM\"])\n",
    "    lP_ratio = math.log(Ps) - math.log(Ph)\n",
    "\n",
    "    for word in counts[ID].keys():\n",
    "        if word != \"SPAM\":\n",
    "            ## compute the class conditional probabilities\n",
    "            sPro = float(counts[\"SPAM\"][word])/float(counts[\"SPAM\"][\"T-WORDS\"])\n",
    "            hPro = float(counts[\"HAM\"][word])/float(counts[\"SPAM\"][\"T-WORDS\"]) \n",
    "\n",
    "            ## build up the log likelihood ratio\n",
    "            lP_ratio += counts[ID][word]*math.log(sPro)\n",
    "            lP_ratio -= counts[ID][word]*math.log(hPro)\n",
    "\n",
    "    ## compute the posterior distribution\n",
    "    try:\n",
    "        P_ratio = math.exp(lP_ratio)\n",
    "        Psd = (P_ratio) / (P_ratio + 1)                    \n",
    "    except OverflowError:\n",
    "        Psd = 1\n",
    "    Phd = 1 - Psd\n",
    "    if (Psd > Phd):\n",
    "        guess = 1\n",
    "    else:\n",
    "        guess = 0\n",
    "    if guess == SPAM:\n",
    "        if SPAM:\n",
    "            TP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    else:\n",
    "        if SPAM:\n",
    "            FN += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "    ## print out the resulting classification\n",
    "    print ID+\"\\t\"+str(SPAM)+\"\\t\"+str(guess)+\"\\t\"+str(Psd)+\"\\t\"+str(Phd)\n",
    "\n",
    "## report confusion matrix statistics\n",
    "\n",
    "TPR = float(TP) / (float(TP) + float(FN))\n",
    "FPR = float(FP) / (float(FP) + float(TN))\n",
    "ACC = (float(TP) + float(TN)) / (float(TP) + float(TN) + float(FP) + float(FN))\n",
    "print \"\"\n",
    "print \"========== Summary ==========\"\n",
    "print \"TPR = \" + str(TPR)\n",
    "print \"FPR = \" + str(FPR)\n",
    "print \"ACC = \" + str(ACC)\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make scripts executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The call for HW1.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Summary ==========\r\n",
      "TPR = 0.136363636364\r\n",
      "FPR = 0.0357142857143\r\n",
      "ACC = 0.6\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\"; cat enronemail_1h.txt.output | tail -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the learning and classification steps in on Map-Reduce job is acceptable for HW1.\n",
    "\n",
    "Strictly speaking, going forward, we would have a **second map-reduce job** for doing classification. For classification we would strive to load the entire Naive Bayes classifier model into memory of the mapper. The mapper would calculate the Pr(Class|Example) for each class and do the argmax based classification and output the predicted class, actual class (for now; later we could use combiners). The reducer would then gather classification records and compute metrics such as misclassification rate and output the resulting metric values. Note we should use only a single reducer in the Classification Map-Reduce Job here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.4: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.\n",
    "In this part, no new code is necessary, since mapper.py and reducer.py have been written in the generalized fashion.\n",
    "2 flavors of solution are given here: \n",
    "- one without smoothing and log probabilites; \n",
    "- one with smoothing and log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 1.4 - Mapper function\n",
    "#This mapper function works very similarly to the implementation in 1.3. The only difference is that it enables iteration through a list of words (provided as arguments) for flagging for inclusion in the conditional probability calculation.\n",
    "\n",
    " \n",
    "#HW 1.4 - Mapper Function\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].lower().split() \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject and body fields from the line, and combine into one string\n",
    "        words=re.findall(WORD_RE,subject_and_body)\n",
    "        for word in words:\n",
    "            flag=0\n",
    "            if word in findwords:\n",
    "                flag=1\n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1\\t'+str(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW 1.4 - Reducer function\n",
    "#This reducer is almost exactly the same as in Problem 1.3. The only difference is not in the code itself, but in the fact that it receives more than one flagged word from the mapper. Because the flagged words are tracked via a list, the reducer doesn't care how many flagged words it receives. It will incorporate all of them into the conditional probability calculation.\n",
    "#Note: Again, the code for Laplace smoothing and log probability is included as comments, but is not used in the final implementation.\n",
    "\n",
    " \n",
    "#HW 1.4 - Reducer Function\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "#\n",
    "emails={} #Associative array to hold email data\n",
    "words={} #Associative array for word data\n",
    " \n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "flagged_words=[] #list of flagged words to include in conditional probability calculation\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])\n",
    "            word=result[2]\n",
    "            flag=int(result[4])\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0,'flag':flag}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "            \n",
    "            if flag==1 and word not in flagged_words:\n",
    "                flagged_words.append(word)\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    " \n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=1-prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    " \n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - no longer used\n",
    "    #p_spam=log(prior_spam)\n",
    "    #p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "        if word in flagged_words:\n",
    "            try:\n",
    "                #p_spam+=log(words[word]['p_spam']) #Log version - no longer used\n",
    "                p_spam*=words[word]['p_spam']\n",
    "            except ValueError:\n",
    "                pass #This means that words that do not appear in a class will use the class prior\n",
    "            try:\n",
    "                #p_ham+=log(words[word]['p_ham']) #Log version - no longer used\n",
    "                p_ham*=words[word]['p_ham']\n",
    "            except ValueError:\n",
    "                pass\n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.4 - Results\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0011.2003-12-18.GP\t1\t0\n"
     ]
    }
   ],
   "source": [
    "#Run our HW 1.4 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\"\n",
    "!echo \"HW 1.4 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance valium EnlargementWithATypo'\n",
      "Training error: 0.37\n"
     ]
    }
   ],
   "source": [
    "#HW 1.4 - Evaluation code\n",
    "def eval_1_4():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance valium EnlargementWithATypo'\" \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the addition of a few more words improves performance slightly, but not enough to make this an effective model for real spam classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.4: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.\n",
    "\n",
    "**VERSION 2: WITH smoothing and log probabilities** \n",
    "\n",
    "In this part, no new code is necessary, since mapper.py and reducer.py have been written in the generalized fashion.\n",
    "We will continue to use the mapper from 1.2, and the reducer from 1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The call for HW1.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Summary ==========\r\n",
      "TPR = 0.159090909091\r\n",
      "FPR = 0.0357142857143\r\n",
      "ACC = 0.61\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\"; cat enronemail_1h.txt.output | tail -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.5: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "In this part, no new code is necessary, since mapper.py and reducer.py have been written in the generalized fashion.\n",
    "We will continue to use the mapper from 1.2, and the reducer from 1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The call for HW1.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Summary ==========\r\n",
      "TPR = 1.0\r\n",
      "FPR = 0.178571428571\r\n",
      "ACC = 0.9\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"*\"; cat enronemail_1h.txt.output | tail -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW1.6: Benchmark your code with the Python SciKit-Learn implementation of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Error\n",
      "  SkLearn Bernoulli NB:\t\t0.16\n",
      "  SkLearn Multinomial NB:\t0.03\n",
      "  HW1.5 Multinomial NB:\t\t0.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "emails = []\n",
    "classes = []\n",
    "    \n",
    "with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        [ID,SPAM,SUBJECT,CONTENT] = re.split(\"\\t\",line)            \n",
    "        DAT = CONTENT.lower()            \n",
    "        emails.append(DAT)\n",
    "        classes.append(SPAM)\n",
    "    \n",
    "classes = np.array(classes)\n",
    "    \n",
    "# Create features for train and dev data \n",
    "vectorizer = CountVectorizer(min_df=3)\n",
    "trainingData = vectorizer.fit_transform(emails)\n",
    "    \n",
    "print \"\\nTraining Error\"    \n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(trainingData, classes) \n",
    "print \"  SkLearn Bernoulli NB:\\t\\t\", 1-classifier.score(trainingData,classes)\n",
    "    \n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(trainingData, classes) \n",
    "print \"  SkLearn Multinomial NB:\\t\", 1-classifier.score(trainingData,classes)\n",
    "   \n",
    "print \"  HW1.5 Multinomial NB:\\t\\t\", 1-0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 1.6 - Summary of Results\n",
    "\n",
    "| Model                                                                      | Training Error |\n",
    "|----------------------------------------------------------------------------|----------------|\n",
    "| Multinomial NB, Scikit-Learn Implementation                                | 0.03            |\n",
    "| Bernoulli NB, Scikit-Learn Implementation                                  | 0.16           |\n",
    "| Multinomial NB, MapReduce implementation                                   | 0.1           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the observed error varies between the different implementations of multinomial naive Bayes.\n",
    "Overall, both version perform similarly, though differ at the level of feature extraction.\n",
    "Since SkLearn's default feature extractor relys only on the '\\w' regular expression, \n",
    "it includes digits in the definitions of words, while ignoring hyphenations and contractions.\n",
    "This generally results in a larger, more specific vocabulary, given the larger range of acceptable characters\n",
    "(and the introduction of junk pieces of words left by splitting at apostraphes and hyphens),\n",
    "that can overfit and result in the reported lower error, since we are testing on the training set.\n",
    "\n",
    "Note as well that between the two SkLearn implementations of Naive Bayes, the Multinomial calssifier \n",
    "outperforms the Bernoulli classifier. This is likely due to the fact that the Bernoulli classifier\n",
    "ignores frequency of occurrence, making it difficult to compare documents of disparate sizes \n",
    "(which the emails are very easily seen to be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
