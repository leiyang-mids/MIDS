{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#UC Berkeley MIDS DATASCI W261-02, Machine Learning at Scale\n",
    "--------\n",
    "##Assignement #1  (version 2016-01-14)\n",
    "## Submitted by Lei Yang ([leiyang@berkeley.edu](mailto:leiyang@berkeley)), 2016-01-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.0.0*: Define big data. Provide an example of a big data problem in your domain of expertise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.0.0 Answer:</span>\n",
    "**Big-data** problem usually has the characteristics of **[4V's](http://www.ibmbigdatahub.com/infographic/four-vs-big-data)**, namely volume, velocity, variety, and veracity \n",
    "- *volume*: in the range of petabyte at least, where regular single server cannot handle, and must resort to distributed computing\n",
    "- *velocity*: constant streaming data instead of batch data, information is updated in real-time\n",
    "- *variety*: unstructured data from various sources, fusion together for decision making\n",
    "- *veracity*: the uncertainty of data, manifested by poor data quality, incomplete record, invalid value, etc.\n",
    "\n",
    "**Example** from my domain:\n",
    "I work in manufacturing industry, where numerous sensors are mounted on the equipment to collect process data, such as temperature, flow rate, pressure etc. The real-time data is used for process and equiment control, where algorithms and statistics are applied. As the production capacity increases, challenges for data analysis rise in various areas:\n",
    "- *volume*: with more sensors installed on the equipment, higher sampling rate required by customer, longer processing time, bigger production capacity, data volume keeps increasing and is approaching the big-data scale\n",
    "- *velocity*: in the highly automated manufacturing environment, wafer processing is fast, which requires fast data processing for decision making. For fault detection problem, any abnormal phenomenon must be detected promptly before the next wafer starts. For classification problem, accurate prediction is desired to facilitate fault diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.0.1*: In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.0.1 Answer:</span>\n",
    "[reference](https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.1*: Read through the provided control script (pNaiveBayes.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.2*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's define *pNaiveBayes.sh* script first, we only need to do this once since it is the same throughout HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "#echo \"$countfiles\"\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py & reducer.py, and make all scripts executable\n",
    "- *mapper.py* counts the single specified word for the chunk, and output an integer\n",
    "- *reducer.py* collates counts from all chunks, and output the total count of the single specified word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "countword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        for word in line.lower().split()[2:]:\n",
    "            if countword in word:\n",
    "                count += 1\n",
    "print countword + ' ' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for filename in sys.argv[1:]:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            temp = line.split()\n",
    "            word = temp[0]\n",
    "            sum += int(temp[1])\n",
    "print word + ': ' + str(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.2 Results: </span>by checking the ouput file, we know there are 10 counts of word 'assistance'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance: 10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.3*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py:\n",
    "- obtains count for each word from the chunk, for spam and non-spam email separately, \n",
    "- records all counts in a dictionary, \n",
    "- outputs the dictionaries (non-spam count, and spam count), (non)spam counts, and keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keyword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keyword + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define reducer.py:\n",
    "- collapse wrod counts from all chunks\n",
    "- estimate NB model parameters: prior and conditional probabilities\n",
    "- classify messages that contains the keyword\n",
    "- **Note:** for messages that don't contain the keyword, the decision is solely based on prior probability, which will always give non-spam prediction, thus we skip those messages and only focus on those with the specified keyword\n",
    "- output results\n",
    "\n",
    "####Parameter estimation background:\n",
    "Assuming *positional independence*, and with *add-one Laplace smoothing*, the multinomial NB conditional probability $P(t | c)$ can be estimated as:\n",
    "$$\n",
    "\\hat{P}(t\\mid c)=\\frac{T_{ct}+1}{(\\sum_{t^\\prime \\in V}{T_{ct^\\prime}})+B},\n",
    "$$\n",
    "\n",
    "where $B=|V|$ is the number of terms in the vocabulary $V$ (including all text classes), and $T_{ct}$ is the count of word *t* in class *c*. \n",
    "\n",
    "To classify a message, the posterior probability of class $c$ can be calculated as:\n",
    "$$\n",
    "c_{map}=\\arg\\max_{c\\in\\mathbb C}[\\log{\\hat{P}(c)}+\\sum_{1\\leqslant k \\leqslant n_d}{\\log{\\hat{P}(t_k\\mid c)}}],\n",
    "$$\n",
    "where $\\hat{P}(t_k\\mid c)$ is estimated above with *positional independence* assumption as $\\hat{P}(t\\mid c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keyword = counts[4]\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with key word: ' + keyword\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "p_word_s = 1.0*((s_count[keyword] if keyword in s_count else 0) + 1) / (tot_s + B)\n",
    "p_word_n = 1.0*((n_count[keyword] if keyword in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### prior probability: same for every message, since it's determined by training data ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "# print model parameters\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "print 'P(%s|spam) = %f' %(keyword, p_word_s)\n",
    "print 'P(%s|non-spam) = %f' %(keyword, p_word_n)\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'TRUTH \\t CLASS \\t ID'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        n_word = sum([1 if keyword in word else 0 for word in words])\n",
    "        # if the message doesn't contain our keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        #### posterior probability ####\n",
    "        p_s_word = math.log(p_s) + n_word * math.log(p_word_s)\n",
    "        p_n_word = math.log(p_n) + n_word * math.log(p_word_n)\n",
    "        isSpam = True if p_s_word > p_n_word else False        \n",
    "        # print results\n",
    "        print ('spam' if int(msg[1]) else 'ham') + '\\t' + ('spam' if isSpam else 'ham') + '\\t' + msg[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.3 Results: </span>run the NB classifier with keyword 'assistance', the output file are displayed below:\n",
    "- **Model parameters**: \n",
    " - prior \n",
    " - likelihood\n",
    "- **Classification results**: \n",
    " - TRUTH: original label\n",
    " - CLASS: filter result\n",
    " - ID: message ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with key word: assistance\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000227\r\n",
      "P(assistance|non-spam) = 0.000093\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "TRUTH \t CLASS \t ID\r\n",
      "spam\tspam\t0002.2004-08-01.bg\r\n",
      "ham\tspam\t0004.1999-12-10.kaminski\r\n",
      "ham\tspam\t0005.1999-12-12.kaminski\r\n",
      "spam\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0013.2004-08-01.bg\r\n",
      "spam\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "spam\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.4*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of mapper.py remains the same as it still just counts words for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keywords = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keywords + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of reducer.py is modified to consider multiple keywords, which we use dictionaries to represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keywords = counts[4].split()\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with keywords: ' + str(keywords)\n",
    "   \n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "\n",
    "#### prior probability ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "#### conditional probabilities for words ####\n",
    "p_word_s, p_word_n = {}, {}\n",
    "for word in keywords:\n",
    "    p_word_s[word] = 1.0*((s_count[word] if word in s_count else 0) + 1) / (tot_s + B)\n",
    "    p_word_n[word] = 1.0*((n_count[word] if word in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### print model parameters ####\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "for word in keywords:\n",
    "    print 'P(%s|spam) = %f' %(word, p_word_s[word])\n",
    "    print 'P(%s|non-spam) = %f' %(word, p_word_n[word])\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'TRUTH \\t CLASS \\t ID'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        #### initialize posterior probability ####\n",
    "        p_s_word = math.log(p_s)\n",
    "        p_n_word = math.log(p_n)\n",
    "        \n",
    "        #### add likelihood for each keyword ####\n",
    "        n_word = 0\n",
    "        for key in keywords:\n",
    "            n_key = sum([1 if key in word else 0 for word in words])\n",
    "            n_word += n_key\n",
    "            p_s_word += n_key * math.log(p_word_s[key])\n",
    "            p_n_word += n_key * math.log(p_word_n[key])\n",
    "            \n",
    "        # if the message doesn't contain any keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        isSpam = True if p_s_word > p_n_word else False        \n",
    "        # print results\n",
    "        print ('spam' if int(msg[1]) else 'ham') + '\\t' + ('spam' if isSpam else 'ham') + '\\t' + msg[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.4 Results: </span>run the NB classifier with keywords 'assistance', 'valium' and 'enlargementWithATypo', the output file are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with keywords: ['assistance', 'valium', 'enlargementwithatypo']\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000227\r\n",
      "P(assistance|non-spam) = 0.000093\r\n",
      "P(valium|spam) = 0.000038\r\n",
      "P(valium|non-spam) = 0.000047\r\n",
      "P(enlargementwithatypo|spam) = 0.000038\r\n",
      "P(enlargementwithatypo|non-spam) = 0.000047\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "TRUTH \t CLASS \t ID\r\n",
      "spam\tspam\t0002.2004-08-01.bg\r\n",
      "ham\tspam\t0004.1999-12-10.kaminski\r\n",
      "ham\tspam\t0005.1999-12-12.kaminski\r\n",
      "spam\tham\t0009.2003-12-18.gp\r\n",
      "spam\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0013.2004-08-01.bg\r\n",
      "spam\tham\t0016.2003-12-19.gp\r\n",
      "spam\tham\t0017.2004-08-01.bg\r\n",
      "spam\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "spam\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance valium enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.5*: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of mapper.py remains the same as it still just counts words for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "#keywords = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of reducer.py is modified to consider all present words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with all words'\n",
    "   \n",
    "# we now estimate NB parameters for all present words\n",
    "allwords = Set(s_count.keys() + n_count.keys())\n",
    "B = len(allwords)\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "\n",
    "#### prior probability ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "#### conditional probabilities for words ####\n",
    "p_word_s, p_word_n = {}, {}\n",
    "for word in allwords:\n",
    "    p_word_s[word] = 1.0*((s_count[word] if word in s_count else 0) + 1) / (tot_s + B)\n",
    "    p_word_n[word] = 1.0*((n_count[word] if word in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### we won't print model parameters, to save some space ####\n",
    "#### likelihood: dependend on the frequency of current word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'TRUTH \\t CLASS \\t ID'\n",
    "n_correct = 0\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        #### initialize posterior probability ####\n",
    "        p_s_word = math.log(p_s)\n",
    "        p_n_word = math.log(p_n)\n",
    "        \n",
    "        #### add likelihood for each keyword ####        \n",
    "        for key in Set(words):\n",
    "            n_key = sum([1 if key in word else 0 for word in words])\n",
    "            p_s_word += n_key * math.log(p_word_s[key])\n",
    "            p_n_word += n_key * math.log(p_word_n[key])\n",
    "            \n",
    "        isSpam = True if p_s_word > p_n_word else False\n",
    "        n_correct += isSpam == int(msg[1])\n",
    "        # print results\n",
    "        print ('spam' if int(msg[1]) else 'ham') + '\\t' + ('spam' if isSpam else 'ham') + '\\t' + msg[0]\n",
    "\n",
    "print '\\nOur multinomial NB training error: %f' %(1-1.0*n_correct/(nSpam+nNormal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.5 Results: </span>run the NB classifier all present words, the output file are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with all words\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "TRUTH \t CLASS \t ID\r\n",
      "ham\tham\t0001.1999-12-10.farmer\r\n",
      "ham\tham\t0001.1999-12-10.kaminski\r\n",
      "ham\tham\t0001.2000-01-17.beck\r\n",
      "ham\tham\t0001.2000-06-06.lokay\r\n",
      "ham\tham\t0001.2001-02-07.kitchen\r\n",
      "ham\tham\t0001.2001-04-02.williams\r\n",
      "ham\tham\t0002.1999-12-13.farmer\r\n",
      "ham\tham\t0002.2001-02-07.kitchen\r\n",
      "spam\tspam\t0002.2001-05-25.sa_and_hp\r\n",
      "spam\tspam\t0002.2003-12-18.gp\r\n",
      "spam\tspam\t0002.2004-08-01.bg\r\n",
      "ham\tham\t0003.1999-12-10.kaminski\r\n",
      "ham\tham\t0003.1999-12-14.farmer\r\n",
      "ham\tham\t0003.2000-01-17.beck\r\n",
      "ham\tham\t0003.2001-02-08.kitchen\r\n",
      "spam\tspam\t0003.2003-12-18.gp\r\n",
      "spam\tspam\t0003.2004-08-01.bg\r\n",
      "ham\tham\t0004.1999-12-10.kaminski\r\n",
      "ham\tham\t0004.1999-12-14.farmer\r\n",
      "ham\tham\t0004.2001-04-02.williams\r\n",
      "spam\tspam\t0004.2001-06-12.sa_and_hp\r\n",
      "spam\tspam\t0004.2004-08-01.bg\r\n",
      "ham\tham\t0005.1999-12-12.kaminski\r\n",
      "ham\tham\t0005.1999-12-14.farmer\r\n",
      "ham\tham\t0005.2000-06-06.lokay\r\n",
      "ham\tham\t0005.2001-02-08.kitchen\r\n",
      "spam\tspam\t0005.2001-06-23.sa_and_hp\r\n",
      "spam\tspam\t0005.2003-12-18.gp\r\n",
      "ham\tham\t0006.1999-12-13.kaminski\r\n",
      "ham\tspam\t0006.2001-02-08.kitchen\r\n",
      "ham\tham\t0006.2001-04-03.williams\r\n",
      "spam\tspam\t0006.2001-06-25.sa_and_hp\r\n",
      "spam\tspam\t0006.2003-12-18.gp\r\n",
      "spam\tspam\t0006.2004-08-01.bg\r\n",
      "ham\tham\t0007.1999-12-13.kaminski\r\n",
      "ham\tham\t0007.1999-12-14.farmer\r\n",
      "ham\tham\t0007.2000-01-17.beck\r\n",
      "ham\tham\t0007.2001-02-09.kitchen\r\n",
      "spam\tspam\t0007.2003-12-18.gp\r\n",
      "spam\tspam\t0007.2004-08-01.bg\r\n",
      "ham\tham\t0008.2001-02-09.kitchen\r\n",
      "spam\tspam\t0008.2001-06-12.sa_and_hp\r\n",
      "spam\tspam\t0008.2001-06-25.sa_and_hp\r\n",
      "spam\tspam\t0008.2003-12-18.gp\r\n",
      "spam\tspam\t0008.2004-08-01.bg\r\n",
      "ham\tham\t0009.1999-12-13.kaminski\r\n",
      "ham\tham\t0009.1999-12-14.farmer\r\n",
      "ham\tham\t0009.2000-06-07.lokay\r\n",
      "ham\tham\t0009.2001-02-09.kitchen\r\n",
      "spam\tspam\t0009.2001-06-26.sa_and_hp\r\n",
      "spam\tspam\t0009.2003-12-18.gp\r\n",
      "ham\tham\t0010.1999-12-14.farmer\r\n",
      "ham\tham\t0010.1999-12-14.kaminski\r\n",
      "ham\tham\t0010.2001-02-09.kitchen\r\n",
      "spam\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0010.2003-12-18.gp\r\n",
      "spam\tspam\t0010.2004-08-01.bg\r\n",
      "ham\tham\t0011.1999-12-14.farmer\r\n",
      "spam\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0011.2001-06-29.sa_and_hp\r\n",
      "spam\tspam\t0011.2003-12-18.gp\r\n",
      "spam\tspam\t0011.2004-08-01.bg\r\n",
      "ham\tham\t0012.1999-12-14.farmer\r\n",
      "ham\tham\t0012.1999-12-14.kaminski\r\n",
      "ham\tham\t0012.2000-01-17.beck\r\n",
      "ham\tspam\t0012.2000-06-08.lokay\r\n",
      "ham\tham\t0012.2001-02-09.kitchen\r\n",
      "spam\tspam\t0012.2003-12-19.gp\r\n",
      "ham\tham\t0013.1999-12-14.farmer\r\n",
      "ham\tham\t0013.1999-12-14.kaminski\r\n",
      "ham\tham\t0013.2001-04-03.williams\r\n",
      "spam\tspam\t0013.2001-06-30.sa_and_hp\r\n",
      "spam\tspam\t0013.2004-08-01.bg\r\n",
      "ham\tham\t0014.1999-12-14.kaminski\r\n",
      "ham\tham\t0014.1999-12-15.farmer\r\n",
      "ham\tham\t0014.2001-02-12.kitchen\r\n",
      "spam\tspam\t0014.2001-07-04.sa_and_hp\r\n",
      "spam\tspam\t0014.2003-12-19.gp\r\n",
      "spam\tspam\t0014.2004-08-01.bg\r\n",
      "ham\tham\t0015.1999-12-14.kaminski\r\n",
      "ham\tham\t0015.1999-12-15.farmer\r\n",
      "ham\tham\t0015.2000-06-09.lokay\r\n",
      "ham\tham\t0015.2001-02-12.kitchen\r\n",
      "spam\tspam\t0015.2001-07-05.sa_and_hp\r\n",
      "spam\tspam\t0015.2003-12-19.gp\r\n",
      "ham\tham\t0016.1999-12-15.farmer\r\n",
      "ham\tham\t0016.2001-02-12.kitchen\r\n",
      "spam\tspam\t0016.2001-07-05.sa_and_hp\r\n",
      "spam\tspam\t0016.2001-07-06.sa_and_hp\r\n",
      "spam\tspam\t0016.2003-12-19.gp\r\n",
      "spam\tspam\t0016.2004-08-01.bg\r\n",
      "ham\tham\t0017.1999-12-14.kaminski\r\n",
      "ham\tham\t0017.2000-01-17.beck\r\n",
      "ham\tham\t0017.2001-04-03.williams\r\n",
      "spam\tspam\t0017.2003-12-18.gp\r\n",
      "spam\tspam\t0017.2004-08-01.bg\r\n",
      "spam\tspam\t0017.2004-08-02.bg\r\n",
      "ham\tspam\t0018.1999-12-14.kaminski\r\n",
      "spam\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "spam\tspam\t0018.2003-12-18.gp\r\n",
      "\r\n",
      "Classification rate: 0.970000\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"dummy\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW1.6*: Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes\n",
    "- Feature vectorization for the emails\n",
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) \n",
    "- Run the Multinomial Naive Bayes algorithm from **HW1.5**\n",
    "- Report Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUTH \t MNB_HW1.5 \t MNB_SK \t BNB_SK \t ID\n",
      "ham\tham\tham\tham\t0001.1999-12-10.farmer\n",
      "ham\tham\tham\tham\t0001.1999-12-10.kaminski\n",
      "ham\tham\tham\tham\t0001.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0001.2000-06-06.lokay\n",
      "ham\tham\tham\tham\t0001.2001-02-07.kitchen\n",
      "ham\tham\tham\tham\t0001.2001-04-02.williams\n",
      "ham\tham\tham\tham\t0002.1999-12-13.farmer\n",
      "ham\tham\tham\tham\t0002.2001-02-07.kitchen\n",
      "spam\tspam\tspam\tham\t0002.2001-05-25.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0002.2003-12-18.gp\n",
      "spam\tspam\tspam\tham\t0002.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0003.1999-12-10.kaminski\n",
      "ham\tham\tham\tham\t0003.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0003.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0003.2001-02-08.kitchen\n",
      "spam\tspam\tspam\tham\t0003.2003-12-18.gp\n",
      "spam\tspam\tspam\tham\t0003.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0004.1999-12-10.kaminski\n",
      "ham\tham\tham\tham\t0004.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0004.2001-04-02.williams\n",
      "spam\tspam\tspam\tspam\t0004.2001-06-12.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0004.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0005.1999-12-12.kaminski\n",
      "ham\tham\tham\tham\t0005.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0005.2000-06-06.lokay\n",
      "ham\tham\tham\tham\t0005.2001-02-08.kitchen\n",
      "spam\tspam\tspam\tham\t0005.2001-06-23.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0005.2003-12-18.gp\n",
      "ham\tham\tham\tham\t0006.1999-12-13.kaminski\n",
      "ham\tspam\tham\tham\t0006.2001-02-08.kitchen\n",
      "ham\tham\tham\tham\t0006.2001-04-03.williams\n",
      "spam\tspam\tspam\tham\t0006.2001-06-25.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0006.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0006.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0007.1999-12-13.kaminski\n",
      "ham\tham\tham\tham\t0007.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0007.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0007.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0007.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0007.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0008.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0008.2001-06-12.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0008.2001-06-25.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0008.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0008.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0009.1999-12-13.kaminski\n",
      "ham\tham\tham\tham\t0009.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0009.2000-06-07.lokay\n",
      "ham\tham\tham\tham\t0009.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0009.2001-06-26.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0009.2003-12-18.gp\n",
      "ham\tham\tham\tham\t0010.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0010.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0010.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0010.2001-06-28.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0010.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0010.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0011.1999-12-14.farmer\n",
      "spam\tspam\tspam\tspam\t0011.2001-06-28.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0011.2001-06-29.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0011.2003-12-18.gp\n",
      "spam\tspam\tspam\tham\t0011.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0012.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0012.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0012.2000-01-17.beck\n",
      "ham\tspam\tham\tham\t0012.2000-06-08.lokay\n",
      "ham\tham\tham\tham\t0012.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tham\t0012.2003-12-19.gp\n",
      "ham\tham\tham\tham\t0013.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0013.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0013.2001-04-03.williams\n",
      "spam\tspam\tspam\tspam\t0013.2001-06-30.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0013.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0014.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0014.1999-12-15.farmer\n",
      "ham\tham\tham\tham\t0014.2001-02-12.kitchen\n",
      "spam\tspam\tspam\tspam\t0014.2001-07-04.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0014.2003-12-19.gp\n",
      "spam\tspam\tspam\tham\t0014.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0015.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0015.1999-12-15.farmer\n",
      "ham\tham\tham\tham\t0015.2000-06-09.lokay\n",
      "ham\tham\tham\tham\t0015.2001-02-12.kitchen\n",
      "spam\tspam\tspam\tspam\t0015.2001-07-05.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0015.2003-12-19.gp\n",
      "ham\tham\tham\tham\t0016.1999-12-15.farmer\n",
      "ham\tham\tham\tham\t0016.2001-02-12.kitchen\n",
      "spam\tspam\tspam\tspam\t0016.2001-07-05.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0016.2001-07-06.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0016.2003-12-19.gp\n",
      "spam\tspam\tspam\tham\t0016.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0017.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0017.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0017.2001-04-03.williams\n",
      "spam\tspam\tspam\tham\t0017.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0017.2004-08-01.bg\n",
      "spam\tspam\tspam\tspam\t0017.2004-08-02.bg\n",
      "ham\tspam\tham\tham\t0018.1999-12-14.kaminski\n",
      "spam\tspam\tspam\tspam\t0018.2001-07-13.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0018.2003-12-18.gp\n",
      "\n",
      "Our multinomial NB training error: 0.030000\n",
      "SK- multinomial NB training error: 0.000000\n",
      "SK- Bernoulli   NB training error: 0.160000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "with open('enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "train_label = [msg[1] for msg in emails]\n",
    "train_data = [msg[2]+msg[3] if len(msg)==4 else msg[2] for msg in emails]\n",
    "msg_id = [msg[0].lower() for msg in emails]\n",
    "\n",
    "# feature vectorization\n",
    "uniVectorizer = CountVectorizer()\n",
    "dtmTrain = uniVectorizer.fit_transform(train_data) \n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(dtmTrain, train_label)\n",
    "pred_mnb = mnb.predict(dtmTrain)\n",
    "training_error_mnb = 1.0*sum(pred_mnb != train_label) / len(train_label)\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier from sklearn\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(dtmTrain, train_label)\n",
    "pred_bnb = bnb.predict(dtmTrain)\n",
    "training_error_bnb = 1.0*sum(pred_bnb != train_label) / len(train_label)\n",
    "\n",
    "# multinomial Naive Bayes Classifier from HW1.5\n",
    "!./pNaiveBayes.sh 4 \"dummy\"\n",
    "\n",
    "# load results from HW1.5 and generate comparison matrix\n",
    "print 'TRUTH \\t MNB_HW1.5 \\t MNB_SK \\t BNB_SK \\t ID'\n",
    "with open ('enronemail_1h.txt.output', \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        if line.startswith('ham') or line.startswith('spam'):\n",
    "            result = line.split()            \n",
    "            idx = msg_id.index(result[2])\n",
    "            result.insert(2, 'spam' if pred_mnb[idx]=='1' else 'ham')\n",
    "            result.insert(3, 'spam' if pred_bnb[idx]=='1' else 'ham')\n",
    "            print str.join('\\t', result)\n",
    "            \n",
    "        if line.startswith('Our multinomial NB'):\n",
    "            print '\\n' + line.strip('\\n')     \n",
    "\n",
    "print 'SK- multinomial NB training error: %f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB training error: %f' %training_error_bnb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
