{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#3\n",
    "####Lei Yang (leiyang@berkeley.edu)\n",
    "####Due: 2016-02-02, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.0.*\n",
    "####What is a merge sort? Where is it used in Hadoop?\n",
    "Merge sort is to merge two sorted lists into one sorted list.\n",
    "It's used in the shuffling stage of Hadoop where key-value pairs from different mappers are merged into one before hand over to the reducer\n",
    "\n",
    "####How is  a combiner function in the context of Hadoop?\n",
    "Combiner is a local aggregation mechanism, where during each mapper process, aggregation is done to collapse key-value earlier and send intermediate results to the reducer.\n",
    "The operation that can combiner is applicable must be associative and communicative.\n",
    "\n",
    "####Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "In word count for long articles. Combiner can be applied within each mapper when processing on a node.\n",
    "In this way, the number of key-value pairs is dramatically reduced before they are sent to the reducer. Consequently the network communication is reduced as well.\n",
    "\n",
    "####What is the Hadoop shuffle?\n",
    "Shuffle happens after all mapper processes complete and before reducer starts, all key-value pairs are sorted by key, and the same key is guaranteed to be delivered to the same reducer.\n",
    "\n",
    "####What is the Apriori algorithm? Describe an example use in your domain of expertise. Define confidence and lift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-resourcemanager-Leis-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-nodemanager-Leis-MacBook-Pro.local.out\n",
      "16/01/31 12:30:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n",
      "16/01/31 12:30:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/mapred-leiyang-historyserver-Leis-MacBook-Pro.local.out\n",
      "16/01/31 12:30:31 INFO hs.JobHistoryServer: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting JobHistoryServer\n",
      "STARTUP_MSG:   host = leis-macbook-pro.local/192.168.0.12\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.7.1\n",
      "STARTUP_MSG:   classpath = /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/modules/*.jar\n",
      "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z\n",
      "STARTUP_MSG:   java = 1.7.0_79\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.1.* Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc.\n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "**Use the Consumer Complaints  Dataset provide [here](https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0) to complete this question:**\n",
    "\n",
    "- The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    " - Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "**Here’s is the first few lines of the  of the Consumer Complaints  Dataset:**\n",
    "\n",
    "- Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "- 1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "- 1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "- 1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "- 1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "**User-defined Counters**\n",
    "\n",
    "- Now, let’s use Hadoop Counters to identify the number of complaints pertaining to *debt collection*, *mortgage* and *other* categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "- Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.1 Answer:</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper\n",
    "- as the shuffler will do the sorting, mapper just need to emit word with integer as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "for line in sys.stdin:  \n",
    "    # extract the column values\n",
    "    parts = line.strip().split(',')\n",
    "    # product is in second column\n",
    "    prod = parts[1].strip().lower()\n",
    "    # emit product name as key, no need for value as we are only count product name\n",
    "    print \"%s\\t%s\" %(prod, 'na')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # product name\n",
    "    prod = line.split('\\t')[0].strip()\n",
    "    \n",
    "    # compare with what we want to count and adjust the counter\n",
    "    if prod == 'debt collection':\n",
    "        sys.stderr.write(\"reporter:counter:HW3_1,debt,1\\n\")\n",
    "    elif prod == 'mortgage':\n",
    "        sys.stderr.write(\"reporter:counter:HW3_1,mortgage,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:HW3_1,others,1\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop Streaming\n",
    "- add parameter *-D mapred.reduce.tasks=2* to specify number of reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6554278484771760949/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob151457222619226138.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/lei/Consumer_Complaints.csv \\\n",
    "-output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Check counter value\n",
    "\n",
    "![Image 1](HW3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2.*  Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "**For this brief study the Input file will be one record (the next line only):**\n",
    "\n",
    "*foo foo quux labs foo bar quux*\n",
    "\n",
    "- Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Mapper_cnt,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    \n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# increase counter for reducer being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Reducer_cnt,1\\n\")\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # print out count\n",
    "            print '%s\\t%s' %(current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to print the last word count if needed!\n",
    "if current_word == word:    \n",
    "    print '%s\\t%s' %(current_word, current_count)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Write the file and put on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/lei/wordcount.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/lei/wordcount.txt\n",
    "!hdfs dfs -put wordcount.txt /user/lei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar4010586940660359060/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2206749045695806587.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input '/user/lei/wordcount.txt' \\\n",
    "-output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.2 Results:</span>\n",
    "![Image 2](HW3_2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2*  Exploratory analysis on consumer complaint data\n",
    "**Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).**\n",
    "\n",
    "- Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:      \n",
    "    # extract the column values\n",
    "    parts = line.strip().split(',')\n",
    "    # issue is in 4th column\n",
    "    issue = parts[3].strip().lower()\n",
    "    # emit issue as key, and 1 as count\n",
    "    print \"%s,%s\" %(regex.sub('', issue), '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# increase counter for reducer being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Reducer_cnt,1\\n\")\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split(',', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # print out count\n",
    "            print '%s,%s' %(current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to print the last word count if needed!\n",
    "if current_word == word:    \n",
    "    print '%s,%s' %(current_word, current_count)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar1354390700304975242/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2622783107061335072.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input '/user/lei/Consumer_Complaints.csv' \\\n",
    "-output results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.2 Results:</span>\n",
    "we can see that the counter values are consistent with our specification of times for mapper and reducer to be called.\n",
    "![Image 2](HW3_2_2.png)\n",
    "\n",
    "And the issue counts are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account terms and changes,350\t\n",
      "application processing delay,243\t\n",
      "application,8625\t\n",
      "apr or interest rate,3431\t\n",
      "billing disputes,6938\t\n",
      "billing statement,1220\t\n",
      "cant contact lender,221\t\n",
      "closingcancelling account,2795\t\n",
      "collection practices,1003\t\n",
      "convenience checks,75\t\n",
      "credit card protection  debt protection,1343\t\n",
      "credit determination,1490\t\n",
      "customer service  customer relations,1367\t\n",
      "dealing with my lender or servicer,1944\t\n",
      "delinquent account,1061\t\n",
      "deposits and withdrawals,10555\t\n",
      "disclosure verification of debt,5214\t\n",
      "health club,12545\t\n",
      "improper contact or sharing of info,2832\t\n",
      "incorrectmissing disclosures or info,64\t\n",
      "late fee,1797\t\n",
      "loan modification,70487\t\n",
      "loan servicing,36767\t\n",
      "makingreceiving payments,3226\t\n",
      "managing the loan or lease,4560\t\n",
      "money was not available when promised,274\t\n",
      "other fee,1075\t\n",
      "other transaction issues,387\t\n",
      "other,6273\t\n",
      "payoff process,1155\t\n",
      "privacy,240\t\n",
      "repaying your loan,3844\t\n",
      "rewards,1002\t\n",
      "shopping for a line of credit,137\t\n",
      "taking out the loan or lease,1242\t\n",
      "takingthreatening an illegal action,2505\t\n",
      "unable to get credit reportcredit score,4357\t\n",
      "unsolicited issuance of credit card,640\t\n",
      "using a debit or atm card,2422\t\n",
      "account opening,16205\t\n",
      "advertising and marketing,1193\t\n",
      "applied for loandid not receive money,139\t\n",
      "arbitration,168\t\n",
      "balance transfer fee,95\t\n",
      "balance transfer,502\t\n",
      "bankruptcy,222\t\n",
      "cant repay my loan,1647\t\n",
      "cant stop charges to bank account,131\t\n",
      "cash advance fee,104\t\n",
      "cash advance,136\t\n",
      "charged bank acct wrong day or amt,71\t\n",
      "charged fees or interest i didnt expect,807\t\n",
      "collection debt dispute,904\t\n",
      "communication tactics,6920\t\n",
      "contd attempts collect debt not owed,11848\t\n",
      "credit decision  underwriting,2774\t\n",
      "credit line increasedecrease,1149\t\n",
      "credit monitoring or identity protection,1453\t\n",
      "credit reporting companys investigation,4858\t\n",
      "credit reporting,1701\t\n",
      "false statements or representation,2508\t\n",
      "forbearance  workout plans,350\t\n",
      "fraud or scam,566\t\n",
      "getting a loan,291\t\n",
      "identity theft  fraud  embezzlement,3276\t\n",
      "improper use of my credit report,1477\t\n",
      "incorrect information on credit report,29069\t\n",
      "issue,1\t\n",
      "managing the line of credit,446\t\n",
      "other service issues,151\t\n",
      "overlimit fee,127\t\n",
      "payment to acct not credited,92\t\n",
      "problems caused by my funds being low,5663\t\n",
      "problems when you are unable to pay,3821\t\n",
      "received a loan i didnt apply for,118\t\n",
      "sale of account,139\t\n",
      "settlement process and costs,4350\t\n",
      "shopping for a loan or lease,535\t\n",
      "transaction issue,1098\t\n",
      "wrong amount charged or received,98\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/leiyang/results/part-0000* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2*  Exploratory analysis on consumer complaint data\n",
    "**Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).**\n",
    "\n",
    "- Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions of mapper and reducer don't need to change in this case, we can just use the reducer as a standalone combiner, specified by Hadoop-streaming parameter (-combiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar4851254519125269812/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob1661741782688936946.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input '/user/lei/Consumer_Complaints.csv' \\\n",
    "-output results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.2 Results:</span>\n",
    "\n",
    "![Image 2](HW3_2_3.png)\n",
    "We can see that the reducer.py was called 8 times during map step as **combiner**, and 2 times during reduce step as **reducer**. And the issue counts from two reducers are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account terms and changes,350\t\r\n",
      "application processing delay,243\t\r\n",
      "application,8625\t\r\n",
      "apr or interest rate,3431\t\r\n",
      "billing disputes,6938\t\r\n",
      "billing statement,1220\t\r\n",
      "cant contact lender,221\t\r\n",
      "closingcancelling account,2795\t\r\n",
      "collection practices,1003\t\r\n",
      "convenience checks,75\t\r\n",
      "credit card protection  debt protection,1343\t\r\n",
      "credit determination,1490\t\r\n",
      "customer service  customer relations,1367\t\r\n",
      "dealing with my lender or servicer,1944\t\r\n",
      "delinquent account,1061\t\r\n",
      "deposits and withdrawals,10555\t\r\n",
      "disclosure verification of debt,5214\t\r\n",
      "health club,12545\t\r\n",
      "improper contact or sharing of info,2832\t\r\n",
      "incorrectmissing disclosures or info,64\t\r\n",
      "late fee,1797\t\r\n",
      "loan modification,70487\t\r\n",
      "loan servicing,36767\t\r\n",
      "makingreceiving payments,3226\t\r\n",
      "managing the loan or lease,4560\t\r\n",
      "money was not available when promised,274\t\r\n",
      "other fee,1075\t\r\n",
      "other transaction issues,387\t\r\n",
      "other,6273\t\r\n",
      "payoff process,1155\t\r\n",
      "privacy,240\t\r\n",
      "repaying your loan,3844\t\r\n",
      "rewards,1002\t\r\n",
      "shopping for a line of credit,137\t\r\n",
      "taking out the loan or lease,1242\t\r\n",
      "takingthreatening an illegal action,2505\t\r\n",
      "unable to get credit reportcredit score,4357\t\r\n",
      "unsolicited issuance of credit card,640\t\r\n",
      "using a debit or atm card,2422\t\r\n",
      "account opening,16205\t\r\n",
      "advertising and marketing,1193\t\r\n",
      "applied for loandid not receive money,139\t\r\n",
      "arbitration,168\t\r\n",
      "balance transfer fee,95\t\r\n",
      "balance transfer,502\t\r\n",
      "bankruptcy,222\t\r\n",
      "cant repay my loan,1647\t\r\n",
      "cant stop charges to bank account,131\t\r\n",
      "cash advance fee,104\t\r\n",
      "cash advance,136\t\r\n",
      "charged bank acct wrong day or amt,71\t\r\n",
      "charged fees or interest i didnt expect,807\t\r\n",
      "collection debt dispute,904\t\r\n",
      "communication tactics,6920\t\r\n",
      "contd attempts collect debt not owed,11848\t\r\n",
      "credit decision  underwriting,2774\t\r\n",
      "credit line increasedecrease,1149\t\r\n",
      "credit monitoring or identity protection,1453\t\r\n",
      "credit reporting companys investigation,4858\t\r\n",
      "credit reporting,1701\t\r\n",
      "false statements or representation,2508\t\r\n",
      "forbearance  workout plans,350\t\r\n",
      "fraud or scam,566\t\r\n",
      "getting a loan,291\t\r\n",
      "identity theft  fraud  embezzlement,3276\t\r\n",
      "improper use of my credit report,1477\t\r\n",
      "incorrect information on credit report,29069\t\r\n",
      "issue,1\t\r\n",
      "managing the line of credit,446\t\r\n",
      "other service issues,151\t\r\n",
      "overlimit fee,127\t\r\n",
      "payment to acct not credited,92\t\r\n",
      "problems caused by my funds being low,5663\t\r\n",
      "problems when you are unable to pay,3821\t\r\n",
      "received a loan i didnt apply for,118\t\r\n",
      "sale of account,139\t\r\n",
      "settlement process and costs,4350\t\r\n",
      "shopping for a loan or lease,535\t\r\n",
      "transaction issue,1098\t\r\n",
      "wrong amount charged or received,98\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/leiyang/results/part-0000* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2*  Exploratory analysis on consumer complaint data\n",
    "\n",
    "- Using a single reducer: What are the top 50 most frequent terms in your word count analysis? \n",
    "- Present the top 50 terms and their frequency and their relative frequency. \n",
    "- If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).\n",
    "\n",
    "**Notes:**\n",
    "- for a single reducer (job) to get list of relative frequencies, we need to implement **order inversion** to get total count first.\n",
    "- **mapper** will emit **'dummy_sort_key, issue_name / \\*, count'**, as it is impossible to sort count with secondary sorting if we use the issue name as partitioner option.\n",
    "- we need to sort numerically of the count, and in the mean time guarantee the emits for total calculation **(key, \\*, count)** arrive first, thus we define *-inf* as the dummy sort key for those emits, as other counts are always postive.\n",
    "- **reducer** will get total count first, then joint count for each word, and finally relative frequency. It needs to be a generic process such that if the combiner is not called, the final results would still be correct.\n",
    "- specify secondary sort on issue name\n",
    "\n",
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:      \n",
    "    # get issue count and name\n",
    "    issue, count = line.strip().split(',')\n",
    "    # emit issue as key, and 1 as count\n",
    "    print \"%s,%s,%s\" %(count, issue, count)\n",
    "    # for order inversion, to calculate total count\n",
    "    print '%s,%s,%s' %('-inf', '*', count)\n",
    "    \n",
    "# temp test for issue tie-break\n",
    "#print '%s,%s,%s' %(1, 'zzz', 3)\n",
    "#print '%s,%s,%s' %(1, 'oko', 3)\n",
    "#print '%s,%s,%s' %(1, 'ccc', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "\n",
    "# buffer for top and bottom\n",
    "n_bottom, n_top = 10, 50\n",
    "bottom, top = [], []\n",
    "n_total = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    dummy, issue, count = line.strip().split(',', 2)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # get total count\n",
    "    if '*' == issue:\n",
    "        n_total += count        \n",
    "        continue\n",
    "    \n",
    "    # calculate relative frequency\n",
    "    rf = 1.0*count/n_total\n",
    "    \n",
    "    # buffer top and bottom\n",
    "    if len(bottom) < n_bottom:\n",
    "        bottom.append([issue, count, rf])\n",
    "                \n",
    "    if len(top) < n_top:\n",
    "        top.append([issue, count, rf])\n",
    "    else:\n",
    "        top = top[1:] + [[issue, count, rf]]\n",
    "        \n",
    "# print results:\n",
    "top.reverse()\n",
    "print '\\ntop %d issues:' %n_top\n",
    "for rec in top:\n",
    "    print '%.2f%%\\t%d\\t%s' %(100*rec[2], rec[1], rec[0])\n",
    "\n",
    "print '\\nbottom %d issues:' %n_bottom\n",
    "for rec in bottom:\n",
    "    print '%.2f%%\\t%d\\t%s' %(100*rec[2], rec[1], rec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results2\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar3079316103673299489/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2021439456043946546.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# assuming count results are available\n",
    "!hdfs dfs -rm -r results2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D map.output.key.value.fields.spec=0-1:0- \\\n",
    "-D mapred.text.key.comparator.options='-k1,1n -k2,2' \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/leiyang/results/part-0000* \\\n",
    "-output results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The sorted top and bottom issues are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\r\n",
      "top 50 issues:\t\r\n",
      "22.53%\t70487\tloan modification\r\n",
      "11.75%\t36767\tloan servicing\r\n",
      "9.29%\t29069\tincorrect information on credit report\r\n",
      "5.18%\t16205\taccount opening\r\n",
      "4.01%\t12545\thealth club\r\n",
      "3.79%\t11848\tcontd attempts collect debt not owed\r\n",
      "3.37%\t10555\tdeposits and withdrawals\r\n",
      "2.76%\t8625\tapplication\r\n",
      "2.22%\t6938\tbilling disputes\r\n",
      "2.21%\t6920\tcommunication tactics\r\n",
      "2.00%\t6273\tother\r\n",
      "1.81%\t5663\tproblems caused by my funds being low\r\n",
      "1.67%\t5214\tdisclosure verification of debt\r\n",
      "1.55%\t4858\tcredit reporting companys investigation\r\n",
      "1.46%\t4560\tmanaging the loan or lease\r\n",
      "1.39%\t4357\tunable to get credit reportcredit score\r\n",
      "1.39%\t4350\tsettlement process and costs\r\n",
      "1.23%\t3844\trepaying your loan\r\n",
      "1.22%\t3821\tproblems when you are unable to pay\r\n",
      "1.10%\t3431\tapr or interest rate\r\n",
      "1.05%\t3276\tidentity theft  fraud  embezzlement\r\n",
      "1.03%\t3226\tmakingreceiving payments\r\n",
      "0.91%\t2832\timproper contact or sharing of info\r\n",
      "0.89%\t2795\tclosingcancelling account\r\n",
      "0.89%\t2774\tcredit decision  underwriting\r\n",
      "0.80%\t2508\tfalse statements or representation\r\n",
      "0.80%\t2505\ttakingthreatening an illegal action\r\n",
      "0.77%\t2422\tusing a debit or atm card\r\n",
      "0.62%\t1944\tdealing with my lender or servicer\r\n",
      "0.57%\t1797\tlate fee\r\n",
      "0.54%\t1701\tcredit reporting\r\n",
      "0.53%\t1647\tcant repay my loan\r\n",
      "0.48%\t1490\tcredit determination\r\n",
      "0.47%\t1477\timproper use of my credit report\r\n",
      "0.46%\t1453\tcredit monitoring or identity protection\r\n",
      "0.44%\t1367\tcustomer service  customer relations\r\n",
      "0.43%\t1343\tcredit card protection  debt protection\r\n",
      "0.40%\t1242\ttaking out the loan or lease\r\n",
      "0.39%\t1220\tbilling statement\r\n",
      "0.38%\t1193\tadvertising and marketing\r\n",
      "0.37%\t1155\tpayoff process\r\n",
      "0.37%\t1149\tcredit line increasedecrease\r\n",
      "0.35%\t1098\ttransaction issue\r\n",
      "0.34%\t1075\tother fee\r\n",
      "0.34%\t1061\tdelinquent account\r\n",
      "0.32%\t1003\tcollection practices\r\n",
      "0.32%\t1002\trewards\r\n",
      "0.29%\t904\tcollection debt dispute\r\n",
      "0.26%\t807\tcharged fees or interest i didnt expect\r\n",
      "0.20%\t640\tunsolicited issuance of credit card\r\n",
      "\t\r\n",
      "bottom 10 issues:\t\r\n",
      "0.00%\t1\tissue\r\n",
      "0.02%\t64\tincorrectmissing disclosures or info\r\n",
      "0.02%\t71\tcharged bank acct wrong day or amt\r\n",
      "0.02%\t75\tconvenience checks\r\n",
      "0.03%\t92\tpayment to acct not credited\r\n",
      "0.03%\t95\tbalance transfer fee\r\n",
      "0.03%\t98\twrong amount charged or received\r\n",
      "0.03%\t104\tcash advance fee\r\n",
      "0.04%\t118\treceived a loan i didnt apply for\r\n",
      "0.04%\t127\toverlimit fee\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/leiyang/results2/part-0000* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*3.2.1 OPTIONAL - * Using 2 reducers: \n",
    "- What are the top 50 most frequent terms in your word count analysis? \n",
    "- Present the top 50 terms and their frequency and their relative frequency. \n",
    "- If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.3.* Shopping Cart Analysis\n",
    "Product Recommendations: \n",
    "- The action or practice of selling additional products or services\n",
    "to existing customers is called cross-selling. \n",
    "- Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers.\n",
    "- One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset [here](https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0):\n",
    "\n",
    "- Each line in this dataset represents a browsing session of a customer.\n",
    "- On each line, each string of 8 characters represents the id of an item browsed during that session.\n",
    "- The items are separated by spaces.\n",
    "\n",
    "- Here are the first few lines of the ProductPurchaseData\n",
    " - FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\n",
    " - GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192\n",
    " - ELE17451 GRO73461 DAI22896 SNA99873 FRO86643\n",
    " - ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465\n",
    " - ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444\n",
    "\n",
    "**Do some exploratory data analysis of this dataset.**\n",
    "\n",
    "- How many unique items are available from this supplier?\n",
    "- **Using a single reducer:**\n",
    " - Report your findings such as number of unique products; largest basket; \n",
    " - Report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.\n",
    "\n",
    "###Mapper\n",
    "- use **order inversion** for reletive frequency, for each word emit $(*, 1, size)$ and $(w_i, 1, size)$\n",
    "- where $size$ is the basket size for each new session, otherwise zero to minimize data transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products\n",
    "    products = line.strip().split(' ')\n",
    "    size = len(products)\n",
    "    if size==0:\n",
    "        continue\n",
    "    for i in range(size):        \n",
    "        # emit dummy key for total count\n",
    "        print '%s,%s,%s' %('*', 1, size if i==0 else 0)\n",
    "        # emit word key\n",
    "        print '%s,%s,%s' %(products[i], 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer\n",
    "- obtain unique products based on $(w_i, 1, size)$ emit.\n",
    "- get the top 50 items with buffer and compare approach, to avoid a second job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,Reducer_cnt,1\\n\")\n",
    "\n",
    "n_total = 0\n",
    "n_top = 50\n",
    "n_unique = 0\n",
    "max_size = 0\n",
    "\n",
    "top_count = np.zeros(n_top)\n",
    "top_prod = [[] for i in range(n_top)]\n",
    "\n",
    "current_prod = None\n",
    "current_count = 0\n",
    "\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    #print line.strip()\n",
    "    prod, count, size = line.strip().split(',', 2)\n",
    "    \n",
    "    # skip bad counts\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # if it's dummy key, get total\n",
    "    if '*'==prod:\n",
    "        n_total += count\n",
    "        max_size = max(max_size, int(size))\n",
    "        continue\n",
    "        \n",
    "    # count unique and get frequency\n",
    "    if current_prod == prod:\n",
    "        current_count += count\n",
    "    else:\n",
    "        # one product just finishes streaming\n",
    "        if current_prod:\n",
    "            # increase unique prod count\n",
    "            n_unique += 1\n",
    "            # get relative freq\n",
    "            rf = 1.0*current_count/n_total\n",
    "            # compare with top list\n",
    "            id_min = np.argmin(top_count)\n",
    "            # if it's bigger than the smallerest, replace it\n",
    "            if top_count[id_min] < current_count:\n",
    "                top_count[id_min] = current_count\n",
    "                top_prod[id_min] = [current_prod, current_count, rf]\n",
    "        \n",
    "        # reset for new prod\n",
    "        current_prod = prod\n",
    "        current_count = count\n",
    "\n",
    "print '\\nmax basket size: %d' %max_size\n",
    "print '\\ntotal product browsing count: %d' %n_total\n",
    "print '\\nnumber of unique product: %d' %n_unique\n",
    "print '\\ntop %d product: ' %n_top\n",
    "# TODO: use numpy.lexsort for secondary product sort\n",
    "for i in np.argsort(top_count)[::-1]:\n",
    "    print '%.4f%%\\t%s\\t%s' %(100*top_prod[i][2], top_prod[i][1], top_prod[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MapReducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 13:12:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 13:12:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results\n",
      "16/01/31 13:12:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar8115166402243256169/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob1044106929107147758.jar tmpDir=null\n",
      "16/01/31 13:12:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 13:12:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 13:12:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 13:12:12 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/01/31 13:12:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/31 13:12:12 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 13:12:12 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 13:12:12 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/01/31 13:12:12 INFO Configuration.deprecation: map.output.key.value.fields.spec is deprecated. Instead, use mapreduce.fieldsel.map.output.key.value.fields.spec\n",
      "16/01/31 13:12:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 13:12:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454261413856_0005\n",
      "16/01/31 13:12:12 INFO impl.YarnClientImpl: Submitted application application_1454261413856_0005\n",
      "16/01/31 13:12:12 INFO mapreduce.Job: The url to track the job: http://Leis-MacBook-Pro.local:8088/proxy/application_1454261413856_0005/\n",
      "16/01/31 13:12:12 INFO mapreduce.Job: Running job: job_1454261413856_0005\n",
      "16/01/31 13:12:30 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "16/01/31 13:12:31 INFO mapreduce.Job: Job job_1454261413856_0005 running in uber mode : false\n",
      "16/01/31 13:12:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 13:12:31 INFO mapreduce.Job: Job job_1454261413856_0005 completed successfully\n",
      "16/01/31 13:12:31 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9541110\n",
      "\t\tFILE: Number of bytes written=19560589\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462974\n",
      "\t\tHDFS: Number of bytes written=1218\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13254\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5237\n",
      "\t\tTotal time spent by all map tasks (ms)=13254\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5237\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13254\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5237\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13572096\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5362688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=761648\n",
      "\t\tMap output bytes=8017808\n",
      "\t\tMap output materialized bytes=9541122\n",
      "\t\tInput split bytes=318\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=73151\n",
      "\t\tReduce shuffle bytes=9541122\n",
      "\t\tReduce input records=761648\n",
      "\t\tReduce output records=58\n",
      "\t\tSpilled Records=1523296\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=202\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=760741888\n",
      "\tHW3_3\n",
      "\t\tMapper_cnt=3\n",
      "\t\tReducer_cnt=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462656\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1218\n",
      "16/01/31 13:12:31 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D map.output.key.value.fields.spec=0:1- \\\n",
    "-D mapred.text.key.comparator.options='-k1,1' \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/lei/ProductPurchaseData.txt \\\n",
    "-output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 13:12:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "max basket size: 37\t\n",
      "\t\n",
      "total product browsing count: 380824\t\n",
      "\t\n",
      "number of unique product: 12591\t\n",
      "\t\n",
      "top 50 product: \t\n",
      "1.7507%\t6667\tDAI62779\n",
      "1.0191%\t3881\tFRO40251\n",
      "1.0175%\t3875\tELE17451\n",
      "0.9458%\t3602\tGRO73461\n",
      "0.7993%\t3044\tSNA80324\n",
      "0.7486%\t2851\tELE32164\n",
      "0.7184%\t2736\tDAI75645\n",
      "0.6447%\t2455\tSNA45677\n",
      "0.6118%\t2330\tFRO31317\n",
      "0.6021%\t2293\tDAI85309\n",
      "0.6019%\t2292\tELE26917\n",
      "0.5864%\t2233\tFRO80039\n",
      "0.5554%\t2115\tGRO21487\n",
      "0.5470%\t2083\tSNA99873\n",
      "0.5262%\t2004\tGRO59710\n",
      "0.5042%\t1920\tGRO71621\n",
      "0.5036%\t1918\tFRO85978\n",
      "0.4832%\t1840\tGRO30386\n",
      "0.4769%\t1816\tELE74009\n",
      "0.4685%\t1784\tGRO56726\n",
      "0.4656%\t1773\tDAI63921\n",
      "0.4611%\t1756\tGRO46854\n",
      "0.4498%\t1713\tELE66600\n",
      "0.4496%\t1712\tDAI83733\n",
      "0.4469%\t1702\tFRO32293\n",
      "0.4456%\t1697\tELE66810\n",
      "0.4322%\t1646\tSNA55762\n",
      "0.4272%\t1627\tDAI22177\n",
      "0.4020%\t1531\tFRO78087\n",
      "0.3981%\t1516\tELE99737\n",
      "0.3910%\t1489\tGRO94758\n",
      "0.3910%\t1489\tELE34057\n",
      "0.3771%\t1436\tFRO35904\n",
      "0.3729%\t1420\tFRO53271\n",
      "0.3695%\t1407\tSNA93860\n",
      "0.3650%\t1390\tSNA90094\n",
      "0.3550%\t1352\tGRO38814\n",
      "0.3532%\t1345\tELE56788\n",
      "0.3469%\t1321\tGRO61133\n",
      "0.3456%\t1316\tELE74482\n",
      "0.3456%\t1316\tDAI88807\n",
      "0.3443%\t1311\tELE59935\n",
      "0.3401%\t1295\tSNA96271\n",
      "0.3387%\t1290\tDAI43223\n",
      "0.3385%\t1289\tELE91337\n",
      "0.3348%\t1275\tGRO15017\n",
      "0.3311%\t1261\tDAI31081\n",
      "0.3204%\t1220\tGRO81087\n",
      "0.3201%\t1219\tDAI22896\n",
      "0.3188%\t1214\tGRO85051\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*3.3.1 OPTIONAL* - Using 2 reducers:  \n",
    "- Report your findings such as number of unique products; largest basket; \n",
    "- Report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.\n",
    "\n",
    "**Notes:**\n",
    "- the challenge is from total calculation since we have multiple reducers, as only one will get the ***** key and be able to calculate the marginal.\n",
    "- possible solution: \n",
    " - hold the reducer(s) which don't have total info, pending the availability of n_total, which may be shared via counter of the reducer that has ***** key?\n",
    " - define another dummy key and make sure each reducer has one for total calculation, seems not efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.4.* (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "- Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. \n",
    "- Write a map-reduce program to find products which are frequently browsed together. \n",
    "- Fix the support count (cooccurence count) to s = 100\n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent),\n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2.\n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.\n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2):\n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right),\n",
    "and break ties in support (between pairs, if any exist)\n",
    "by taking the first ones in lexicographically increasing order.\n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.\n",
    "\n",
    "\n",
    "###Mapper\n",
    "- using **pair** pattern with **order inversion**, emit $(w_i,w_j,1,0)$ for all pairs, and $(w_i,*,1,size)$ for each product, from each session (row).\n",
    "- the fourth field of every emit is used to indicate basket size for every session (row), size is postive for *only* one emit, and zero for rest of the emit, so we minimize data transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products\n",
    "    products = line.strip().split(' ')\n",
    "    \n",
    "    # get pairs of product\n",
    "    pairs = [[a[i],a[j]] for i in range(n) for j in range(i+1,n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer\n",
    "- count unique product based on $(w_i, *, 1, size)$ emits\n",
    "- get largest basket from streaming of of $size$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (training)\n",
    "- Differ from previous one, in that we need to separate counts for spam and ham\n",
    "- key~value pair: word~[1, flag for ham/spam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_t.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%d\\t%s\\t%s' % (word, 1, isSpam, msgID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (training)\n",
    "Assuming *positional independence*, and without smoothing, the multinomial NB conditional probability $P(t | c)$ can be estimated as:\n",
    "$$\n",
    "\\hat{P}(t\\mid c)=\\frac{T_{ct}+0}{(\\sum_{t^\\prime \\in V}{T_{ct^\\prime}})+B},\n",
    "$$\n",
    "\n",
    "where $B=|V|$ is the number of terms in the vocabulary $V$ (including all text classes), and $T_{ct}$ is the count of word *t* in class *c*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (classification)\n",
    "- key~value pair: msgID~[probability, flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probability from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"prob/part-00000\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip bad message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1                \n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isSpam, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (classification)\n",
    "To classify a message, the posterior probability of class $c$ can be calculated as:\n",
    "$$\n",
    "c_{map}=\\arg\\max_{c\\in\\mathbb C}[\\log{\\hat{P}(c)}+\\sum_{1\\leqslant k \\leqslant n_d}{\\log{\\hat{P}(t_k\\mid c)}}],\n",
    "$$\n",
    "where $\\hat{P}(t_k\\mid c)$ is estimated above with *positional independence* assumption as $\\hat{P}(t\\mid c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_c.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "current_msg = None\n",
    "current_prob = [0, 0] \n",
    "current_truth = 0\n",
    "msgID = None\n",
    "n_error = 0\n",
    "n_msg = 0\n",
    "n_zero = [0, 0]\n",
    "\n",
    "print '%s\\t%s\\t%s' %('TRUTH', 'PREDICTION', 'EMAIL ID')\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    #print line\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper_c.py\n",
    "    msgID, p0, p1, isSpam, pr0, pr1 = line.split('\\t')\n",
    "    prob = [float(p0), float(p1)]\n",
    "    \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:        \n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_msg == msgID:        \n",
    "        current_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], current_prob], 0)\n",
    "    else:\n",
    "        if current_msg:\n",
    "            # count finish for current word, predict and print result\n",
    "            pred = np.argmax(current_prob)\n",
    "            n_error += pred != current_truth\n",
    "            n_msg += 1\n",
    "            n_zero[current_truth] += float('-inf') in current_prob\n",
    "            print '%s\\t%s\\t%s' %(current_truth, pred, current_msg)\n",
    "                    \n",
    "        # initialize new count for new word\n",
    "        prior = [math.log(float(pr0)), math.log(float(pr1))]\n",
    "        current_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], prior], 0)\n",
    "        current_msg = msgID\n",
    "        current_truth = isSpam\n",
    "\n",
    "# do not forget to print the last msg result if needed!\n",
    "if current_msg == msgID:\n",
    "    pred = np.argmax(current_prob)\n",
    "    n_error += pred != isSpam\n",
    "    n_msg += 1\n",
    "    n_zero[current_truth] += float('-inf') in current_prob\n",
    "    print '%s\\t%s\\t%s' %(current_truth, pred, msgID)\n",
    "    \n",
    "# calculate the overall error rate\n",
    "print 'Error rate: %.4f' %(1.0*n_error/n_msg)\n",
    "print 'Number of messages with zero probability: spam(%d), ham(%d)' %(n_zero[1], n_zero[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the jobs with Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "Deleted prob\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r results\n",
    "!hdfs dfs -rm -r prob\n",
    "# run training job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper mapper_t.py -reducer reducer_t.py -input /user/lei/enronemail_1h.txt -output prob\n",
    "# run classification job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper mapper_c.py -reducer reducer_c.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.3 Results: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUTH\tPREDICTION\tEMAIL ID\r\n",
      "0\t0\t0001.1999-12-10.farmer\r\n",
      "0\t0\t0001.1999-12-10.kaminski\r\n",
      "0\t0\t0001.2000-01-17.beck\r\n",
      "0\t0\t0001.2000-06-06.lokay\r\n",
      "0\t0\t0001.2001-02-07.kitchen\r\n",
      "0\t0\t0001.2001-04-02.williams\r\n",
      "0\t0\t0002.1999-12-13.farmer\r\n",
      "0\t0\t0002.2001-02-07.kitchen\r\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\r\n",
      "1\t1\t0002.2003-12-18.GP\r\n",
      "1\t1\t0002.2004-08-01.BG\r\n",
      "0\t0\t0003.1999-12-10.kaminski\r\n",
      "0\t0\t0003.1999-12-14.farmer\r\n",
      "0\t0\t0003.2000-01-17.beck\r\n",
      "0\t0\t0003.2001-02-08.kitchen\r\n",
      "1\t1\t0003.2003-12-18.GP\r\n",
      "1\t1\t0003.2004-08-01.BG\r\n",
      "0\t0\t0004.1999-12-10.kaminski\r\n",
      "0\t0\t0004.1999-12-14.farmer\r\n",
      "0\t0\t0004.2001-04-02.williams\r\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0004.2004-08-01.BG\r\n",
      "0\t0\t0005.1999-12-12.kaminski\r\n",
      "0\t0\t0005.1999-12-14.farmer\r\n",
      "0\t0\t0005.2000-06-06.lokay\r\n",
      "0\t0\t0005.2001-02-08.kitchen\r\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\r\n",
      "1\t1\t0005.2003-12-18.GP\r\n",
      "0\t0\t0006.1999-12-13.kaminski\r\n",
      "0\t0\t0006.2001-02-08.kitchen\r\n",
      "0\t0\t0006.2001-04-03.williams\r\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0006.2003-12-18.GP\r\n",
      "1\t1\t0006.2004-08-01.BG\r\n",
      "0\t0\t0007.1999-12-13.kaminski\r\n",
      "0\t0\t0007.1999-12-14.farmer\r\n",
      "0\t0\t0007.2000-01-17.beck\r\n",
      "0\t0\t0007.2001-02-09.kitchen\r\n",
      "1\t1\t0007.2003-12-18.GP\r\n",
      "1\t1\t0007.2004-08-01.BG\r\n",
      "0\t0\t0008.2001-02-09.kitchen\r\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0008.2003-12-18.GP\r\n",
      "1\t1\t0008.2004-08-01.BG\r\n",
      "0\t0\t0009.1999-12-13.kaminski\r\n",
      "0\t0\t0009.1999-12-14.farmer\r\n",
      "0\t0\t0009.2000-06-07.lokay\r\n",
      "0\t0\t0009.2001-02-09.kitchen\r\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\r\n",
      "1\t1\t0009.2003-12-18.GP\r\n",
      "0\t0\t0010.1999-12-14.farmer\r\n",
      "0\t0\t0010.1999-12-14.kaminski\r\n",
      "0\t0\t0010.2001-02-09.kitchen\r\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0010.2003-12-18.GP\r\n",
      "1\t1\t0010.2004-08-01.BG\r\n",
      "0\t0\t0011.1999-12-14.farmer\r\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\r\n",
      "1\t1\t0011.2003-12-18.GP\r\n",
      "1\t1\t0011.2004-08-01.BG\r\n",
      "0\t0\t0012.1999-12-14.farmer\r\n",
      "0\t0\t0012.1999-12-14.kaminski\r\n",
      "0\t0\t0012.2000-01-17.beck\r\n",
      "0\t0\t0012.2000-06-08.lokay\r\n",
      "0\t0\t0012.2001-02-09.kitchen\r\n",
      "1\t1\t0012.2003-12-19.GP\r\n",
      "0\t0\t0013.1999-12-14.farmer\r\n",
      "0\t0\t0013.1999-12-14.kaminski\r\n",
      "0\t0\t0013.2001-04-03.williams\r\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\r\n",
      "1\t1\t0013.2004-08-01.BG\r\n",
      "0\t0\t0014.1999-12-14.kaminski\r\n",
      "0\t0\t0014.1999-12-15.farmer\r\n",
      "0\t0\t0014.2001-02-12.kitchen\r\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\r\n",
      "1\t1\t0014.2003-12-19.GP\r\n",
      "1\t1\t0014.2004-08-01.BG\r\n",
      "0\t0\t0015.1999-12-14.kaminski\r\n",
      "0\t0\t0015.1999-12-15.farmer\r\n",
      "0\t0\t0015.2000-06-09.lokay\r\n",
      "0\t0\t0015.2001-02-12.kitchen\r\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0015.2003-12-19.GP\r\n",
      "0\t0\t0016.1999-12-15.farmer\r\n",
      "0\t0\t0016.2001-02-12.kitchen\r\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\r\n",
      "1\t1\t0016.2003-12-19.GP\r\n",
      "1\t1\t0016.2004-08-01.BG\r\n",
      "0\t0\t0017.1999-12-14.kaminski\r\n",
      "0\t0\t0017.2000-01-17.beck\r\n",
      "0\t0\t0017.2001-04-03.williams\r\n",
      "1\t1\t0017.2003-12-18.GP\r\n",
      "1\t1\t0017.2004-08-01.BG\r\n",
      "1\t1\t0017.2004-08-02.BG\r\n",
      "0\t0\t0018.1999-12-14.kaminski\r\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\r\n",
      "1\t1\t0018.2003-12-18.GP\r\n",
      "Error rate: 0.0000\t\r\n",
      "Number of messages with zero probability: spam(44), ham(56)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Discussion on posterior probability:\n",
    "- the NB posterior probability is: \n",
    "$$\n",
    "P(Y=y_k\\mid X_1,X_2, ...,X_n)=\\frac{P(Y=y_k)\\prod_i{P(X_i\\mid Y=y_k)}}{\\sum_j{P(Y=y_j)\\prod_i{P(X_i\\mid Y=y_j)}}}\n",
    "$$\n",
    "- from the results we can see for each message, there are words that only belong to one category\n",
    " - in this case the conditional probability of those words for the opposite category, $P(X_i\\mid Y=y_j)$, are zero   \n",
    " - and this will make one term (with respect to $j$) on the denominator become zero, \n",
    " - and thus the posterior probability is 1 for the category that contains all the words from the email, and 0 for the other category (hisogram below)\n",
    "- this fact illustrates that NB classifier is *over-confident*, the training error in this case is small, but the validation error can be big\n",
    "- and this brings the need for smoothing, such that the posterior probability of one class won't be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERVLCEREVOz/A7U4JKjP14qJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105e95c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram for posterio probability\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 2\n",
    "# 44 spam and 56 ham all has posterior probability of 1\n",
    "spam = (0, 44)\n",
    "ham = (0, 56)\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, spam, width, color='r')\n",
    "p2 = plt.bar(ind, ham, width, color='y',bottom=spam,)\n",
    "\n",
    "plt.ylabel('N')\n",
    "plt.title('Posterior Probability')\n",
    "plt.xticks(ind + width/2., ('0', '1'))\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.legend((p1[0], p2[0]), ('spam', 'ham'), loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.4* Repeat HW2.3 with the following modification: \n",
    "- use Laplace plus-one smoothing,\n",
    "- compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "- **we only need to change the reducer in training step, the rest remains the same as previous**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (training)\n",
    "- minor change: assign 1 to the smooth_factor (instead of 0 in HW2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 1 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the jobs with Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "Deleted prob\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r results\n",
    "!hdfs dfs -rm -r prob\n",
    "# run training job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_t.py -reducer reducer_t.py -input /user/lei/enronemail_1h.txt -output prob\n",
    "# run classification job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_c.py -reducer reducer_c.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.4 Results: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUTH\tPREDICTION\tEMAIL ID\r\n",
      "0\t0\t0001.1999-12-10.farmer\r\n",
      "0\t0\t0001.1999-12-10.kaminski\r\n",
      "0\t0\t0001.2000-01-17.beck\r\n",
      "0\t0\t0001.2000-06-06.lokay\r\n",
      "0\t0\t0001.2001-02-07.kitchen\r\n",
      "0\t0\t0001.2001-04-02.williams\r\n",
      "0\t0\t0002.1999-12-13.farmer\r\n",
      "0\t0\t0002.2001-02-07.kitchen\r\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\r\n",
      "1\t1\t0002.2003-12-18.GP\r\n",
      "1\t1\t0002.2004-08-01.BG\r\n",
      "0\t0\t0003.1999-12-10.kaminski\r\n",
      "0\t0\t0003.1999-12-14.farmer\r\n",
      "0\t0\t0003.2000-01-17.beck\r\n",
      "0\t0\t0003.2001-02-08.kitchen\r\n",
      "1\t1\t0003.2003-12-18.GP\r\n",
      "1\t1\t0003.2004-08-01.BG\r\n",
      "0\t0\t0004.1999-12-10.kaminski\r\n",
      "0\t0\t0004.1999-12-14.farmer\r\n",
      "0\t0\t0004.2001-04-02.williams\r\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0004.2004-08-01.BG\r\n",
      "0\t0\t0005.1999-12-12.kaminski\r\n",
      "0\t0\t0005.1999-12-14.farmer\r\n",
      "0\t0\t0005.2000-06-06.lokay\r\n",
      "0\t0\t0005.2001-02-08.kitchen\r\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\r\n",
      "1\t1\t0005.2003-12-18.GP\r\n",
      "0\t0\t0006.1999-12-13.kaminski\r\n",
      "0\t0\t0006.2001-02-08.kitchen\r\n",
      "0\t0\t0006.2001-04-03.williams\r\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0006.2003-12-18.GP\r\n",
      "1\t1\t0006.2004-08-01.BG\r\n",
      "0\t0\t0007.1999-12-13.kaminski\r\n",
      "0\t0\t0007.1999-12-14.farmer\r\n",
      "0\t0\t0007.2000-01-17.beck\r\n",
      "0\t0\t0007.2001-02-09.kitchen\r\n",
      "1\t1\t0007.2003-12-18.GP\r\n",
      "1\t1\t0007.2004-08-01.BG\r\n",
      "0\t0\t0008.2001-02-09.kitchen\r\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0008.2003-12-18.GP\r\n",
      "1\t1\t0008.2004-08-01.BG\r\n",
      "0\t0\t0009.1999-12-13.kaminski\r\n",
      "0\t0\t0009.1999-12-14.farmer\r\n",
      "0\t0\t0009.2000-06-07.lokay\r\n",
      "0\t0\t0009.2001-02-09.kitchen\r\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\r\n",
      "1\t1\t0009.2003-12-18.GP\r\n",
      "0\t0\t0010.1999-12-14.farmer\r\n",
      "0\t0\t0010.1999-12-14.kaminski\r\n",
      "0\t0\t0010.2001-02-09.kitchen\r\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0010.2003-12-18.GP\r\n",
      "1\t1\t0010.2004-08-01.BG\r\n",
      "0\t0\t0011.1999-12-14.farmer\r\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\r\n",
      "1\t1\t0011.2003-12-18.GP\r\n",
      "1\t1\t0011.2004-08-01.BG\r\n",
      "0\t0\t0012.1999-12-14.farmer\r\n",
      "0\t0\t0012.1999-12-14.kaminski\r\n",
      "0\t0\t0012.2000-01-17.beck\r\n",
      "0\t0\t0012.2000-06-08.lokay\r\n",
      "0\t0\t0012.2001-02-09.kitchen\r\n",
      "1\t1\t0012.2003-12-19.GP\r\n",
      "0\t0\t0013.1999-12-14.farmer\r\n",
      "0\t0\t0013.1999-12-14.kaminski\r\n",
      "0\t0\t0013.2001-04-03.williams\r\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\r\n",
      "1\t1\t0013.2004-08-01.BG\r\n",
      "0\t0\t0014.1999-12-14.kaminski\r\n",
      "0\t0\t0014.1999-12-15.farmer\r\n",
      "0\t0\t0014.2001-02-12.kitchen\r\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\r\n",
      "1\t1\t0014.2003-12-19.GP\r\n",
      "1\t1\t0014.2004-08-01.BG\r\n",
      "0\t0\t0015.1999-12-14.kaminski\r\n",
      "0\t0\t0015.1999-12-15.farmer\r\n",
      "0\t0\t0015.2000-06-09.lokay\r\n",
      "0\t0\t0015.2001-02-12.kitchen\r\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0015.2003-12-19.GP\r\n",
      "0\t0\t0016.1999-12-15.farmer\r\n",
      "0\t0\t0016.2001-02-12.kitchen\r\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\r\n",
      "1\t1\t0016.2003-12-19.GP\r\n",
      "1\t1\t0016.2004-08-01.BG\r\n",
      "0\t0\t0017.1999-12-14.kaminski\r\n",
      "0\t0\t0017.2000-01-17.beck\r\n",
      "0\t0\t0017.2001-04-03.williams\r\n",
      "1\t1\t0017.2003-12-18.GP\r\n",
      "1\t0\t0017.2004-08-01.BG\r\n",
      "1\t1\t0017.2004-08-02.BG\r\n",
      "0\t1\t0018.1999-12-14.kaminski\r\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\r\n",
      "1\t1\t0018.2003-12-18.GP\r\n",
      "Error rate: 0.0200\t\r\n",
      "Number of messages with zero probability: spam(0), ham(0)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW2.4 Discussion:\n",
    "- smoothing essentially add noise to the data\n",
    "- and thus introduce some misclassification on the training set\n",
    "- but it can have better results on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.5.* Repeat HW2.4. \n",
    "- This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. \n",
    "- How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:\n",
    "- **we only need to change the reducer in training step and the mapper for classification, the rest remains the same as previous**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (training)\n",
    "- minor change with previous: at the end not emitting word~probability pair with less than 3 word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    # only emit probability when the count (spam and ham together) is no less than 3\n",
    "    if sum(wordcount[key]) >= 3:\n",
    "        print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (classification)\n",
    "- change: at the end only emit word which has probability from training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probability from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"prob/part-00000\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip bad message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1    \n",
    "        if word in prob:\n",
    "            print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isSpam, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the jobs with Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "Deleted prob\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r results\n",
    "!hdfs dfs -rm -r prob\n",
    "# run training job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_t.py -reducer reducer_t.py -input /user/lei/enronemail_1h.txt -output prob\n",
    "# run classification job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_c.py -reducer reducer_c.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.5 Results: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUTH\tPREDICTION\tEMAIL ID\r\n",
      "0\t0\t0001.1999-12-10.farmer\r\n",
      "0\t0\t0001.1999-12-10.kaminski\r\n",
      "0\t0\t0001.2000-01-17.beck\r\n",
      "0\t0\t0001.2000-06-06.lokay\r\n",
      "0\t0\t0001.2001-02-07.kitchen\r\n",
      "0\t0\t0001.2001-04-02.williams\r\n",
      "0\t0\t0002.1999-12-13.farmer\r\n",
      "0\t0\t0002.2001-02-07.kitchen\r\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\r\n",
      "1\t1\t0002.2003-12-18.GP\r\n",
      "1\t1\t0002.2004-08-01.BG\r\n",
      "0\t0\t0003.1999-12-10.kaminski\r\n",
      "0\t0\t0003.1999-12-14.farmer\r\n",
      "0\t0\t0003.2000-01-17.beck\r\n",
      "0\t0\t0003.2001-02-08.kitchen\r\n",
      "1\t1\t0003.2003-12-18.GP\r\n",
      "1\t1\t0003.2004-08-01.BG\r\n",
      "0\t0\t0004.1999-12-10.kaminski\r\n",
      "0\t0\t0004.1999-12-14.farmer\r\n",
      "0\t0\t0004.2001-04-02.williams\r\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0004.2004-08-01.BG\r\n",
      "0\t0\t0005.1999-12-12.kaminski\r\n",
      "0\t0\t0005.1999-12-14.farmer\r\n",
      "0\t0\t0005.2000-06-06.lokay\r\n",
      "0\t0\t0005.2001-02-08.kitchen\r\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\r\n",
      "1\t1\t0005.2003-12-18.GP\r\n",
      "0\t0\t0006.1999-12-13.kaminski\r\n",
      "0\t0\t0006.2001-02-08.kitchen\r\n",
      "0\t0\t0006.2001-04-03.williams\r\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0006.2003-12-18.GP\r\n",
      "1\t1\t0006.2004-08-01.BG\r\n",
      "0\t0\t0007.1999-12-13.kaminski\r\n",
      "0\t0\t0007.1999-12-14.farmer\r\n",
      "0\t0\t0007.2000-01-17.beck\r\n",
      "0\t0\t0007.2001-02-09.kitchen\r\n",
      "1\t1\t0007.2003-12-18.GP\r\n",
      "1\t1\t0007.2004-08-01.BG\r\n",
      "0\t0\t0008.2001-02-09.kitchen\r\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0008.2003-12-18.GP\r\n",
      "1\t1\t0008.2004-08-01.BG\r\n",
      "0\t0\t0009.1999-12-13.kaminski\r\n",
      "0\t0\t0009.1999-12-14.farmer\r\n",
      "0\t0\t0009.2000-06-07.lokay\r\n",
      "0\t0\t0009.2001-02-09.kitchen\r\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\r\n",
      "1\t1\t0009.2003-12-18.GP\r\n",
      "0\t0\t0010.1999-12-14.farmer\r\n",
      "0\t0\t0010.1999-12-14.kaminski\r\n",
      "0\t0\t0010.2001-02-09.kitchen\r\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\r\n",
      "1\t0\t0010.2003-12-18.GP\r\n",
      "1\t1\t0010.2004-08-01.BG\r\n",
      "0\t0\t0011.1999-12-14.farmer\r\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\r\n",
      "1\t1\t0011.2003-12-18.GP\r\n",
      "1\t1\t0011.2004-08-01.BG\r\n",
      "0\t0\t0012.1999-12-14.farmer\r\n",
      "0\t0\t0012.1999-12-14.kaminski\r\n",
      "0\t0\t0012.2000-01-17.beck\r\n",
      "0\t0\t0012.2000-06-08.lokay\r\n",
      "0\t0\t0012.2001-02-09.kitchen\r\n",
      "1\t1\t0012.2003-12-19.GP\r\n",
      "0\t0\t0013.1999-12-14.farmer\r\n",
      "0\t0\t0013.1999-12-14.kaminski\r\n",
      "0\t0\t0013.2001-04-03.williams\r\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\r\n",
      "1\t1\t0013.2004-08-01.BG\r\n",
      "0\t0\t0014.1999-12-14.kaminski\r\n",
      "0\t0\t0014.1999-12-15.farmer\r\n",
      "0\t0\t0014.2001-02-12.kitchen\r\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\r\n",
      "1\t1\t0014.2003-12-19.GP\r\n",
      "1\t1\t0014.2004-08-01.BG\r\n",
      "0\t0\t0015.1999-12-14.kaminski\r\n",
      "0\t0\t0015.1999-12-15.farmer\r\n",
      "0\t0\t0015.2000-06-09.lokay\r\n",
      "0\t0\t0015.2001-02-12.kitchen\r\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0015.2003-12-19.GP\r\n",
      "0\t0\t0016.1999-12-15.farmer\r\n",
      "0\t0\t0016.2001-02-12.kitchen\r\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\r\n",
      "1\t1\t0016.2003-12-19.GP\r\n",
      "1\t1\t0016.2004-08-01.BG\r\n",
      "0\t0\t0017.1999-12-14.kaminski\r\n",
      "0\t0\t0017.2000-01-17.beck\r\n",
      "0\t0\t0017.2001-04-03.williams\r\n",
      "1\t1\t0017.2003-12-18.GP\r\n",
      "1\t1\t0017.2004-08-01.BG\r\n",
      "1\t1\t0017.2004-08-02.BG\r\n",
      "0\t0\t0018.1999-12-14.kaminski\r\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\r\n",
      "1\t1\t0018.2003-12-18.GP\r\n",
      "Error rate: 0.0100\t\r\n",
      "Number of messages with zero probability: spam(43), ham(54)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW2.5 Discussion:\n",
    "- similar to smoothing, ignoring small frequency words may eliminate some that only belong to one category\n",
    "- and to reduce the chance of binary (1-0) posterior probability for prediction\n",
    "- training error increases a bit, not surprisingly, but may benefit prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.6.* Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "- Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK- multinomial NB training error: 0.0000\n",
      "SK- Bernoulli   NB training error: 0.1600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "with open('enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "train_label = [msg[1] for msg in emails]\n",
    "train_data = [msg[2]+msg[3] if len(msg)==4 else msg[2] for msg in emails]\n",
    "msg_id = [msg[0].lower() for msg in emails]\n",
    "\n",
    "# feature vectorization\n",
    "uniVectorizer = CountVectorizer()\n",
    "dtmTrain = uniVectorizer.fit_transform(train_data) \n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(dtmTrain, train_label)\n",
    "pred_mnb = mnb.predict(dtmTrain)\n",
    "training_error_mnb = 1.0*sum(pred_mnb != train_label) / len(train_label)\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier from sklearn\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(dtmTrain, train_label)\n",
    "pred_bnb = bnb.predict(dtmTrain)\n",
    "training_error_bnb = 1.0*sum(pred_bnb != train_label) / len(train_label)\n",
    "\n",
    "print 'SK- multinomial NB training error: %.4f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB training error: %.4f' %training_error_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###*HW 2.6.1. OPTIONAL* (note this exercise is a stretch HW and optional)\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "- Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "- Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.6.1 Results: </span>See execution above, performance summary:\n",
    "|   |Training Error   |   \n",
    "|---|:---:|\n",
    "| SK-Learn Multinomial NB | 0% |\n",
    "| SK-Learn Bernoulli NB  | 16% |\n",
    "| Ad-hoc Multinomial NB (HW2.5)| 1%  |\n",
    "\n",
    "###HW2.6 Discussion:\n",
    "- Bernoulli NB classifier focuses on the appearance of word, while Multinomial NB classifier emphasizes on the frequency of the word\n",
    "- thus the performance difference between the two can be caused by the bias in training data: there are words that appear way more frequently in one category than the other. In this case multinomial will perform better.\n",
    "- I'd choose Bernoulli classifier, as for SPAM detection, the appearance of keywords is more important than the frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.7. OPTIONAL* (note this exercise is a stretch HW and optional)\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "- Line 1 contains the subject\n",
    "- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. \n",
    "- Please pay attend to funky characters and tabs. \n",
    "- Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). \n",
    "- Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train-Enron-1.txt was generated!\n"
     ]
    }
   ],
   "source": [
    "# assuming the folder is at the same place of this notebook\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# function to consolidate one folder\n",
    "def readmail(path, handler, isSpam):\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    for name in files:\n",
    "        id = '.'.join(name.split('.')[:-2])\n",
    "        f = open(path+name, 'r')\n",
    "        mail = f.read().split('\\n', 1)\n",
    "        subject = mail[0].strip('Subject:').strip()\n",
    "        body = ' '.join(mail[1].split()).strip()\n",
    "        row = '%s\\t%s\\t%s\\t%s' %(id, isSpam, subject, 'NA' if not body else body)\n",
    "        f.close()\n",
    "        handler.write(row + '\\n')\n",
    "\n",
    "# read two folder separately\n",
    "text_file = open(\"train-Enron-1.txt\", \"w\")\n",
    "readmail('./enron1-Training-Data-RAW/ham/', text_file, '0')\n",
    "readmail('./enron1-Training-Data-RAW/spam/', text_file, '1')\n",
    "text_file.close()\n",
    "print 'File train-Enron-1.txt was generated!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Upload file to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/leiyang/train-Enron-1.txt\n",
      "Done, train-Enron-1.txt is uploaded to hdfs.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/leiyang/train-Enron-1.txt\n",
    "!hdfs dfs -put train-Enron-1.txt /user/leiyang/\n",
    "print 'Done, train-Enron-1.txt is uploaded to hdfs.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.8. OPTIONAL*\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    "- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). \n",
    "- Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). \n",
    "- Drop tokens with a frequency of less than three (3).\n",
    "- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. \n",
    "- Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). \n",
    "- How do we treat tokens in the test set that do not appear in the training set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_t.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%d\\t%s\\t%s' % (word, 1, isSpam, msgID)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 1 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:    \n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    if wordcount[key] >= 3:\n",
    "        print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probability from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"prob/part-00000\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip bad message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1    \n",
    "        if word in prob:\n",
    "            print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isSpam, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_c.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "current_msg = None\n",
    "current_prob = [0, 0] \n",
    "current_truth = 0\n",
    "msgID = None\n",
    "n_error = 0\n",
    "n_msg = 0\n",
    "n_zero = [0, 0]\n",
    "\n",
    "print '%s\\t%s\\t%s' %('TRUTH', 'PREDICTION', 'EMAIL ID')\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:    \n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper_c.py\n",
    "    msgID, p0, p1, isSpam, pr0, pr1 = line.split('\\t')\n",
    "    prob = [float(p0), float(p1)]\n",
    "    \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:        \n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_msg == msgID:        \n",
    "        current_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], current_prob], 0)\n",
    "    else:\n",
    "        if current_msg:\n",
    "            # count finish for current word, predict and print result\n",
    "            pred = np.argmax(current_prob)\n",
    "            n_error += pred != current_truth\n",
    "            n_msg += 1\n",
    "            n_zero[current_truth] += float('-inf') in current_prob\n",
    "            print '%s\\t%s\\t%s' %(current_truth, pred, current_msg)\n",
    "                    \n",
    "        # initialize new count for new word\n",
    "        prior = [math.log(float(pr0)), math.log(float(pr1))]\n",
    "        current_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], prior], 0)\n",
    "        current_msg = msgID\n",
    "        current_truth = isSpam\n",
    "\n",
    "# do not forget to print the last msg result if needed!\n",
    "if current_msg == msgID:\n",
    "    pred = np.argmax(current_prob)\n",
    "    n_error += pred != isSpam\n",
    "    n_msg += 1\n",
    "    n_zero[current_truth] += float('-inf') in current_prob\n",
    "    print '%s\\t%s\\t%s' %(current_truth, pred, msgID)\n",
    "    \n",
    "# calculate the overall error rate\n",
    "print 'Error rate: %.4f' %(1.0*n_error/n_msg)\n",
    "print 'Number of messages with zero probability: spam(%d), ham(%d)' %(n_zero[1], n_zero[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the jobs with Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "Deleted prob\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r results\n",
    "!hdfs dfs -rm -r prob\n",
    "# run training job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_t.py -reducer reducer_t.py -input train-Enron-1.txt -output prob\n",
    "# run classification job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_c.py -reducer reducer_c.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.8 Results: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUTH\tPREDICTION\tEMAIL ID\r\n",
      "0\t1\t0001.1999-12-10.farmer\r\n",
      "0\t0\t0001.1999-12-10.kaminski\r\n",
      "0\t0\t0001.2000-01-17.beck\r\n",
      "0\t0\t0001.2000-06-06.lokay\r\n",
      "0\t0\t0001.2001-02-07.kitchen\r\n",
      "0\t0\t0001.2001-04-02.williams\r\n",
      "0\t0\t0002.1999-12-13.farmer\r\n",
      "0\t0\t0002.2001-02-07.kitchen\r\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\r\n",
      "1\t1\t0002.2003-12-18.GP\r\n",
      "1\t1\t0002.2004-08-01.BG\r\n",
      "0\t0\t0003.1999-12-10.kaminski\r\n",
      "0\t0\t0003.1999-12-14.farmer\r\n",
      "0\t0\t0003.2000-01-17.beck\r\n",
      "0\t0\t0003.2001-02-08.kitchen\r\n",
      "1\t1\t0003.2003-12-18.GP\r\n",
      "1\t1\t0003.2004-08-01.BG\r\n",
      "0\t0\t0004.1999-12-10.kaminski\r\n",
      "0\t0\t0004.1999-12-14.farmer\r\n",
      "0\t0\t0004.2001-04-02.williams\r\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0004.2004-08-01.BG\r\n",
      "0\t0\t0005.1999-12-12.kaminski\r\n",
      "0\t0\t0005.1999-12-14.farmer\r\n",
      "0\t0\t0005.2000-06-06.lokay\r\n",
      "0\t0\t0005.2001-02-08.kitchen\r\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\r\n",
      "1\t1\t0005.2003-12-18.GP\r\n",
      "0\t0\t0006.1999-12-13.kaminski\r\n",
      "0\t0\t0006.2001-02-08.kitchen\r\n",
      "0\t0\t0006.2001-04-03.williams\r\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0006.2003-12-18.GP\r\n",
      "1\t1\t0006.2004-08-01.BG\r\n",
      "0\t0\t0007.1999-12-13.kaminski\r\n",
      "0\t0\t0007.1999-12-14.farmer\r\n",
      "0\t0\t0007.2000-01-17.beck\r\n",
      "0\t0\t0007.2001-02-09.kitchen\r\n",
      "1\t1\t0007.2003-12-18.GP\r\n",
      "1\t1\t0007.2004-08-01.BG\r\n",
      "0\t0\t0008.2001-02-09.kitchen\r\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0008.2003-12-18.GP\r\n",
      "1\t1\t0008.2004-08-01.BG\r\n",
      "0\t0\t0009.1999-12-13.kaminski\r\n",
      "0\t0\t0009.1999-12-14.farmer\r\n",
      "0\t0\t0009.2000-06-07.lokay\r\n",
      "0\t0\t0009.2001-02-09.kitchen\r\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\r\n",
      "1\t1\t0009.2003-12-18.GP\r\n",
      "0\t0\t0010.1999-12-14.farmer\r\n",
      "0\t0\t0010.1999-12-14.kaminski\r\n",
      "0\t0\t0010.2001-02-09.kitchen\r\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0010.2003-12-18.GP\r\n",
      "1\t1\t0010.2004-08-01.BG\r\n",
      "0\t0\t0011.1999-12-14.farmer\r\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\r\n",
      "1\t1\t0011.2003-12-18.GP\r\n",
      "1\t1\t0011.2004-08-01.BG\r\n",
      "0\t0\t0012.1999-12-14.farmer\r\n",
      "0\t0\t0012.1999-12-14.kaminski\r\n",
      "0\t0\t0012.2000-01-17.beck\r\n",
      "0\t1\t0012.2000-06-08.lokay\r\n",
      "0\t0\t0012.2001-02-09.kitchen\r\n",
      "1\t1\t0012.2003-12-19.GP\r\n",
      "0\t0\t0013.1999-12-14.farmer\r\n",
      "0\t0\t0013.1999-12-14.kaminski\r\n",
      "0\t0\t0013.2001-04-03.williams\r\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\r\n",
      "1\t1\t0013.2004-08-01.BG\r\n",
      "0\t0\t0014.1999-12-14.kaminski\r\n",
      "0\t0\t0014.1999-12-15.farmer\r\n",
      "0\t0\t0014.2001-02-12.kitchen\r\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\r\n",
      "1\t1\t0014.2003-12-19.GP\r\n",
      "1\t1\t0014.2004-08-01.BG\r\n",
      "0\t0\t0015.1999-12-14.kaminski\r\n",
      "0\t0\t0015.1999-12-15.farmer\r\n",
      "0\t0\t0015.2000-06-09.lokay\r\n",
      "0\t0\t0015.2001-02-12.kitchen\r\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0015.2003-12-19.GP\r\n",
      "0\t0\t0016.1999-12-15.farmer\r\n",
      "0\t0\t0016.2001-02-12.kitchen\r\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\r\n",
      "1\t1\t0016.2003-12-19.GP\r\n",
      "1\t1\t0016.2004-08-01.BG\r\n",
      "0\t0\t0017.1999-12-14.kaminski\r\n",
      "0\t0\t0017.2000-01-17.beck\r\n",
      "0\t0\t0017.2001-04-03.williams\r\n",
      "1\t1\t0017.2003-12-18.GP\r\n",
      "1\t1\t0017.2004-08-01.BG\r\n",
      "1\t1\t0017.2004-08-02.BG\r\n",
      "0\t0\t0018.1999-12-14.kaminski\r\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\r\n",
      "1\t1\t0018.2003-12-18.GP\r\n",
      "Error rate: 0.0200\t\r\n",
      "Number of messages with zero probability: spam(0), ham(0)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW2.8 Discussion:\n",
    "- error rate 2%, with more training data, we are able to achieve good prediction accuracy\n",
    "- when a word from text set is not found in the training set, we simply skip it, because we have no knowledge about what this word indicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.8.1. OPTIONAL*\n",
    "- Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "- Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK- multinomial NB training error: 0.02\n",
      "SK- Bernoulli   NB training error: 0.19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv, re, string\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "f = open('train-Enron-1.txt', 'r')\n",
    "txt = f.read().strip()\n",
    "f.close()\n",
    "emails = txt.split('\\n')\n",
    "train_label = [msg.strip().split('\\t', 2)[1] for msg in emails]\n",
    "train_data = [msg.strip().split('\\t', 2)[-1] for msg in emails]\n",
    "# get rid of the funky characters\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "train_data = [' '.join(regex.sub(' ', msg).split()).decode('latin-1') for msg in train_data]\n",
    "\n",
    "# read test data\n",
    "with open('enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "test_label = [msg[1] for msg in emails]\n",
    "test_data = [msg[2]+msg[3] if len(msg)==4 else msg[2] for msg in emails]\n",
    "\n",
    "# feature vectorization\n",
    "uniVectorizer = CountVectorizer()\n",
    "dtmTrain = uniVectorizer.fit_transform(train_data) \n",
    "dtmTest = uniVectorizer.transform(test_data)\n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(dtmTrain, train_label)\n",
    "pred_mnb = mnb.predict(dtmTest)\n",
    "training_error_mnb = 1.0*sum(pred_mnb != test_label) / len(test_label)\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier from sklearn\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(dtmTrain, train_label)\n",
    "pred_bnb = bnb.predict(dtmTest)\n",
    "training_error_bnb = 1.0*sum(pred_bnb != test_label) / len(test_label)\n",
    "\n",
    "print 'SK- multinomial NB training error: %.2f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB training error: %.2f' %training_error_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW2.8.1 Discussion:\n",
    "|   |Training Error   |   \n",
    "|---|:---:|\n",
    "| SK-Learn Multinomial NB | 2% |\n",
    "| SK-Learn Bernoulli NB  | 19% |\n",
    "| Ad-hoc Multinomial NB (HW2.5)| 1%  |\n",
    "\n",
    "- Bernoulli has much worse performance than Multinomial NB\n",
    "- The multinomial classifier has comparable performance with our own implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping historyserver\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ stop historyserver "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
