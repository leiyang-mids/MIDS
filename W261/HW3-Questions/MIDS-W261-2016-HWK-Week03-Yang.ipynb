{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#3\n",
    "####Lei Yang (leiyang@berkeley.edu)\n",
    "####Due: 2016-02-02, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.0.* Q&A\n",
    "\n",
    "####What is a merge sort? Where is it used in Hadoop?\n",
    "Merge sort is a sorting algorithm which quickly combines two sorted lists into a single list of items. Merge sort benefits from distributable in its least efficient step, which is the sorting of the child lists. The merging of child lists into a single sorted list is done in linear time. Merge sorting is used in the shuffle stage of Hadoop to rearrange keys prior to sending them to the reducer. Key-value pairs from different mappers are sorted at their mappers, and then distributed across the reducers in a sorted form.\n",
    "\n",
    "####How is a combiner function used in the context of Hadoop?\n",
    "Combiners are used for local aggregation during the mapper processes of Hadoop. They are run when the incomplete output from the mapper becomes too large to fit within memory and \"spills over\" onto disk. The combiner is responsible for shrinking the data back down so that the mapper can run faster by keeping data in memory and so that the network operations in the partitioner are kept to a bare minimum. Depending on the size and scope of the problem, Hadoop will run combiners any number of times including zero with no input from the user. For this reason, it is critical that the combiner is able to receive records in the format of the mapper's output and emit data in the same format. The combining operation must also be associative and commutative so that the variable number of runs will not affect the result.\n",
    "\n",
    "####Give an example where it can be used and justify why it should be used in the context of this problem\n",
    "Combiners can be used in long word-count operations. A typical mapper output for a word-count problem will be greater than the size of the document since it emits each individual word and the number associated with it. Transferring this data across the network can drastically reduce the performance of this operation, as well as making the subsequent sorting operation take much longer. Adding a combiner can reduce the size of the mapper output from being tied to the size of the document to being tied to the size of the vocabulary. \n",
    "\n",
    "####What is the Hadoop shuffle?\n",
    "Shuffle happens after all mapper processes complete and before reducer starts, all key-value pairs are sorted by key, and the same key is guaranteed to be delivered to the same reducer.\n",
    "\n",
    "####What is the Apriori algorithm? Describe an example use in your domain of expertise. Define confidence and lift.\n",
    "- Aprior algorithm is used to find frequent itemsets, each iteration has two scans of data and a filtering in between:\n",
    " 1. generate a condidate set $C_k$ for itemsets of size $k$, based on the output of previous iteration $L_{k-1}$\n",
    " 2. remove all members from the set whose support is less than the user specified threshold $s_i$\n",
    " 3. generate the final set $L_k$ for frequent itemset of size $k$, based on output after filtering\n",
    " \n",
    "- For example, to find itemsets of size $k$ from a basket set, the process is:\n",
    " 1. count all single words from all baskets, output $C_1$\n",
    " 2. remove all words with support below threshold, output $L_1$\n",
    " 3. using $L_1$, generate candidate set for frequent pair set $C_2$\n",
    " 5. remove all pairs with support below threshold, get $L_2$\n",
    " 6. using $L_2$, generate candidate set for frequent triple set $C_3$\n",
    " 7. remove all triples with support below threshold, get $L_3$\n",
    "\n",
    "- Confidence is the relative frequency of an association rule, for example:\n",
    " - if the count of triple (a,b,c) is $n$\n",
    " - and the count of pair (a,b) is $m$\n",
    " - the confidence of rule (a,b) => c is $\\frac{n}{m}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-resourcemanager-Leis-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-nodemanager-Leis-MacBook-Pro.local.out\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n",
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/mapred-leiyang-historyserver-Leis-MacBook-Pro.local.out\n",
      "16/02/03 18:14:56 INFO hs.JobHistoryServer: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting JobHistoryServer\n",
      "STARTUP_MSG:   host = leis-macbook-pro.local/192.168.0.12\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.7.1\n",
      "STARTUP_MSG:   classpath = /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/modules/*.jar\n",
      "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z\n",
      "STARTUP_MSG:   java = 1.7.0_79\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.1.* Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc.\n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "**Use the Consumer Complaints  Dataset provide [here](https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0) to complete this question:**\n",
    "\n",
    "- The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    " - Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "**Here’s is the first few lines of the  of the Consumer Complaints  Dataset:**\n",
    "\n",
    "- Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "- 1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "- 1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "- 1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "- 1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "**User-defined Counters**\n",
    "\n",
    "- Now, let’s use Hadoop Counters to identify the number of complaints pertaining to *debt collection*, *mortgage* and *other* categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "- Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.1 Answer:</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper\n",
    "- as the shuffler will do the sorting, mapper just need to emit word with integer as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "for line in sys.stdin:  \n",
    "    # extract the column values\n",
    "    parts = line.strip().split(',')\n",
    "    # product is in second column\n",
    "    prod = parts[1].strip().lower()\n",
    "    # emit product name as key, no need for value as we are only count product name\n",
    "    print \"%s\\t%s\" %(prod, 'na')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # product name\n",
    "    prod = line.split('\\t')[0].strip()\n",
    "    \n",
    "    # compare with what we want to count and adjust the counter\n",
    "    if prod == 'debt collection':\n",
    "        sys.stderr.write(\"reporter:counter:HW3_1,debt,1\\n\")\n",
    "    elif prod == 'mortgage':\n",
    "        sys.stderr.write(\"reporter:counter:HW3_1,mortgage,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:HW3_1,others,1\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop Streaming\n",
    "- add parameter *-D mapred.reduce.tasks=2* to specify number of reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar3709839099986008331/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3447758072235333814.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/lei/Consumer_Complaints.csv \\\n",
    "-output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Check counter value\n",
    "\n",
    "![Image 1](HW3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2.*  Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "**For this brief study the Input file will be one record (the next line only):**\n",
    "\n",
    "*foo foo quux labs foo bar quux*\n",
    "\n",
    "- Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Mapper_cnt,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    \n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# increase counter for reducer being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Reducer_cnt,1\\n\")\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # print out count\n",
    "            print '%s\\t%s' %(current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to print the last word count if needed!\n",
    "if current_word == word:    \n",
    "    print '%s\\t%s' %(current_word, current_count)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Write the file and put on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/lei/wordcount.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/lei/wordcount.txt\n",
    "!hdfs dfs -put wordcount.txt /user/lei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar7556678054711269180/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3086524328830560864.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input '/user/lei/wordcount.txt' \\\n",
    "-output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.2 Results:</span>\n",
    "![Image 2](HW3_2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2*  Exploratory analysis on consumer complaint data\n",
    "**Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).**\n",
    "\n",
    "- Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:      \n",
    "    # extract the column values\n",
    "    parts = line.strip().split(',')\n",
    "    # issue is in 4th column\n",
    "    issue = parts[3].strip().lower()\n",
    "    # emit issue as key, and 1 as count\n",
    "    print \"%s,%s\" %(regex.sub('', issue), '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# increase counter for reducer being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Reducer_cnt,1\\n\")\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split(',', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # print out count\n",
    "            print '%s,%s' %(current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to print the last word count if needed!\n",
    "if current_word == word:    \n",
    "    print '%s,%s' %(current_word, current_count)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar5125320870294825413/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2595898526152944027.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input '/user/lei/Consumer_Complaints.csv' \\\n",
    "-output results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.2 Results:</span>\n",
    "we can see that the counter values are consistent with our specification of times for mapper and reducer to be called.\n",
    "![Image 2](HW3_2_2.png)\n",
    "\n",
    "And the issue counts are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account terms and changes,350\t\r\n",
      "application processing delay,243\t\r\n",
      "application,8625\t\r\n",
      "apr or interest rate,3431\t\r\n",
      "billing disputes,6938\t\r\n",
      "billing statement,1220\t\r\n",
      "cant contact lender,221\t\r\n",
      "closingcancelling account,2795\t\r\n",
      "collection practices,1003\t\r\n",
      "convenience checks,75\t\r\n",
      "credit card protection  debt protection,1343\t\r\n",
      "credit determination,1490\t\r\n",
      "customer service  customer relations,1367\t\r\n",
      "dealing with my lender or servicer,1944\t\r\n",
      "delinquent account,1061\t\r\n",
      "deposits and withdrawals,10555\t\r\n",
      "disclosure verification of debt,5214\t\r\n",
      "health club,12545\t\r\n",
      "improper contact or sharing of info,2832\t\r\n",
      "incorrectmissing disclosures or info,64\t\r\n",
      "late fee,1797\t\r\n",
      "loan modification,70487\t\r\n",
      "loan servicing,36767\t\r\n",
      "makingreceiving payments,3226\t\r\n",
      "managing the loan or lease,4560\t\r\n",
      "money was not available when promised,274\t\r\n",
      "other fee,1075\t\r\n",
      "other transaction issues,387\t\r\n",
      "other,6273\t\r\n",
      "payoff process,1155\t\r\n",
      "privacy,240\t\r\n",
      "repaying your loan,3844\t\r\n",
      "rewards,1002\t\r\n",
      "shopping for a line of credit,137\t\r\n",
      "taking out the loan or lease,1242\t\r\n",
      "takingthreatening an illegal action,2505\t\r\n",
      "unable to get credit reportcredit score,4357\t\r\n",
      "unsolicited issuance of credit card,640\t\r\n",
      "using a debit or atm card,2422\t\r\n",
      "account opening,16205\t\r\n",
      "advertising and marketing,1193\t\r\n",
      "applied for loandid not receive money,139\t\r\n",
      "arbitration,168\t\r\n",
      "balance transfer fee,95\t\r\n",
      "balance transfer,502\t\r\n",
      "bankruptcy,222\t\r\n",
      "cant repay my loan,1647\t\r\n",
      "cant stop charges to bank account,131\t\r\n",
      "cash advance fee,104\t\r\n",
      "cash advance,136\t\r\n",
      "charged bank acct wrong day or amt,71\t\r\n",
      "charged fees or interest i didnt expect,807\t\r\n",
      "collection debt dispute,904\t\r\n",
      "communication tactics,6920\t\r\n",
      "contd attempts collect debt not owed,11848\t\r\n",
      "credit decision  underwriting,2774\t\r\n",
      "credit line increasedecrease,1149\t\r\n",
      "credit monitoring or identity protection,1453\t\r\n",
      "credit reporting companys investigation,4858\t\r\n",
      "credit reporting,1701\t\r\n",
      "false statements or representation,2508\t\r\n",
      "forbearance  workout plans,350\t\r\n",
      "fraud or scam,566\t\r\n",
      "getting a loan,291\t\r\n",
      "identity theft  fraud  embezzlement,3276\t\r\n",
      "improper use of my credit report,1477\t\r\n",
      "incorrect information on credit report,29069\t\r\n",
      "issue,1\t\r\n",
      "managing the line of credit,446\t\r\n",
      "other service issues,151\t\r\n",
      "overlimit fee,127\t\r\n",
      "payment to acct not credited,92\t\r\n",
      "problems caused by my funds being low,5663\t\r\n",
      "problems when you are unable to pay,3821\t\r\n",
      "received a loan i didnt apply for,118\t\r\n",
      "sale of account,139\t\r\n",
      "settlement process and costs,4350\t\r\n",
      "shopping for a loan or lease,535\t\r\n",
      "transaction issue,1098\t\r\n",
      "wrong amount charged or received,98\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/leiyang/results/part-0000* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2*  Exploratory analysis on consumer complaint data\n",
    "**Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).**\n",
    "\n",
    "- Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions of mapper and reducer don't need to change in this case, we can just use the reducer as a standalone combiner, specified by Hadoop-streaming parameter (-combiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar2110887529466242773/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob4709300790161981780.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input '/user/lei/Consumer_Complaints.csv' \\\n",
    "-output results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW3.2 Results:</span>\n",
    "We can see that the reducer.py was called 8 times during map step as **combiner**, and 2 times during reduce step as **reducer**. \n",
    "![Image 2](HW3_2_3.png)\n",
    "And the issue counts from two reducers are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account terms and changes,350\t\r\n",
      "application processing delay,243\t\r\n",
      "application,8625\t\r\n",
      "apr or interest rate,3431\t\r\n",
      "billing disputes,6938\t\r\n",
      "billing statement,1220\t\r\n",
      "cant contact lender,221\t\r\n",
      "closingcancelling account,2795\t\r\n",
      "collection practices,1003\t\r\n",
      "convenience checks,75\t\r\n",
      "credit card protection  debt protection,1343\t\r\n",
      "credit determination,1490\t\r\n",
      "customer service  customer relations,1367\t\r\n",
      "dealing with my lender or servicer,1944\t\r\n",
      "delinquent account,1061\t\r\n",
      "deposits and withdrawals,10555\t\r\n",
      "disclosure verification of debt,5214\t\r\n",
      "health club,12545\t\r\n",
      "improper contact or sharing of info,2832\t\r\n",
      "incorrectmissing disclosures or info,64\t\r\n",
      "late fee,1797\t\r\n",
      "loan modification,70487\t\r\n",
      "loan servicing,36767\t\r\n",
      "makingreceiving payments,3226\t\r\n",
      "managing the loan or lease,4560\t\r\n",
      "money was not available when promised,274\t\r\n",
      "other fee,1075\t\r\n",
      "other transaction issues,387\t\r\n",
      "other,6273\t\r\n",
      "payoff process,1155\t\r\n",
      "privacy,240\t\r\n",
      "repaying your loan,3844\t\r\n",
      "rewards,1002\t\r\n",
      "shopping for a line of credit,137\t\r\n",
      "taking out the loan or lease,1242\t\r\n",
      "takingthreatening an illegal action,2505\t\r\n",
      "unable to get credit reportcredit score,4357\t\r\n",
      "unsolicited issuance of credit card,640\t\r\n",
      "using a debit or atm card,2422\t\r\n",
      "account opening,16205\t\r\n",
      "advertising and marketing,1193\t\r\n",
      "applied for loandid not receive money,139\t\r\n",
      "arbitration,168\t\r\n",
      "balance transfer fee,95\t\r\n",
      "balance transfer,502\t\r\n",
      "bankruptcy,222\t\r\n",
      "cant repay my loan,1647\t\r\n",
      "cant stop charges to bank account,131\t\r\n",
      "cash advance fee,104\t\r\n",
      "cash advance,136\t\r\n",
      "charged bank acct wrong day or amt,71\t\r\n",
      "charged fees or interest i didnt expect,807\t\r\n",
      "collection debt dispute,904\t\r\n",
      "communication tactics,6920\t\r\n",
      "contd attempts collect debt not owed,11848\t\r\n",
      "credit decision  underwriting,2774\t\r\n",
      "credit line increasedecrease,1149\t\r\n",
      "credit monitoring or identity protection,1453\t\r\n",
      "credit reporting companys investigation,4858\t\r\n",
      "credit reporting,1701\t\r\n",
      "false statements or representation,2508\t\r\n",
      "forbearance  workout plans,350\t\r\n",
      "fraud or scam,566\t\r\n",
      "getting a loan,291\t\r\n",
      "identity theft  fraud  embezzlement,3276\t\r\n",
      "improper use of my credit report,1477\t\r\n",
      "incorrect information on credit report,29069\t\r\n",
      "issue,1\t\r\n",
      "managing the line of credit,446\t\r\n",
      "other service issues,151\t\r\n",
      "overlimit fee,127\t\r\n",
      "payment to acct not credited,92\t\r\n",
      "problems caused by my funds being low,5663\t\r\n",
      "problems when you are unable to pay,3821\t\r\n",
      "received a loan i didnt apply for,118\t\r\n",
      "sale of account,139\t\r\n",
      "settlement process and costs,4350\t\r\n",
      "shopping for a loan or lease,535\t\r\n",
      "transaction issue,1098\t\r\n",
      "wrong amount charged or received,98\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/leiyang/results/part-0000* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.2*  Exploratory analysis on consumer complaint data\n",
    "\n",
    "- Using a single reducer: What are the top 50 most frequent terms in your word count analysis? \n",
    "- Present the top 50 terms and their frequency and their relative frequency. \n",
    "- If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).\n",
    "\n",
    "**Notes:**\n",
    "- for a single reducer (job) to get list of relative frequencies, we need to implement **order inversion** to get total count first.\n",
    "- **mapper** will emit **'dummy_sort_key, issue_name / \\*, count'**, as it is impossible to sort count with secondary sorting if we use the issue name as partitioner option.\n",
    "- we need to sort numerically of the count, and in the mean time guarantee the emits for total calculation **(key, \\*, count)** arrive first, thus we define *-inf* as the dummy sort key for those emits, as other counts are always postive.\n",
    "- **reducer** will get total count first, then joint count for each word, and finally relative frequency. It needs to be a generic process such that if the combiner is not called, the final results would still be correct.\n",
    "- specify secondary sort on issue name\n",
    "\n",
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_2,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:      \n",
    "    # get issue count and name\n",
    "    issue, count = line.strip().split(',')\n",
    "    # emit issue as key, and 1 as count\n",
    "    print \"%s,%s,%s\" %(count, issue, count)\n",
    "    # for order inversion, to calculate total count\n",
    "    print '%s,%s,%s' %('-3', '*', count)\n",
    "    \n",
    "# test for tie-break\n",
    "#print '%s,%s,%s' %(1, 'zzz', 3)\n",
    "#print '%s,%s,%s' %(1, 'oko', 3)\n",
    "#print '%s,%s,%s' %(1, 'ccc', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "\n",
    "# buffer for top and bottom\n",
    "n_bottom, n_top = 10, 50\n",
    "bottom, top = [], []\n",
    "n_total = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    dummy, issue, count = line.strip().split(',', 2)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # get total count\n",
    "    if '*' == issue:\n",
    "        n_total += count        \n",
    "        continue\n",
    "    \n",
    "    # calculate relative frequency\n",
    "    rf = 1.0*count/n_total\n",
    "    \n",
    "    # buffer top and bottom\n",
    "    if len(bottom) < n_bottom:\n",
    "        bottom.append([issue, count, rf])\n",
    "                \n",
    "    if len(top) < n_top:\n",
    "        top.append([issue, count, rf])\n",
    "    else:\n",
    "        top = top[1:] + [[issue, count, rf]]\n",
    "        \n",
    "# print results:\n",
    "top.reverse()\n",
    "print '\\ntop %d issues:' %n_top\n",
    "for rec in top:\n",
    "    print '%.2f%%\\t%d\\t%s' %(100*rec[2], rec[1], rec[0])\n",
    "\n",
    "print '\\nbottom %d issues:' %n_bottom\n",
    "for rec in bottom:\n",
    "    print '%.2f%%\\t%d\\t%s' %(100*rec[2], rec[1], rec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results2\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar1115560675774446028/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3811758649004660161.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# assuming count results are available\n",
    "!hdfs dfs -rm -r results2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D map.output.key.value.fields.spec=0-1:0- \\\n",
    "-D mapred.text.key.comparator.options='-k1,1n -k2,2' \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/leiyang/results/part-0000* \\\n",
    "-output results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The sorted top and bottom issues are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\r\n",
      "top 50 issues:\t\r\n",
      "22.53%\t70487\tloan modification\r\n",
      "11.75%\t36767\tloan servicing\r\n",
      "9.29%\t29069\tincorrect information on credit report\r\n",
      "5.18%\t16205\taccount opening\r\n",
      "4.01%\t12545\thealth club\r\n",
      "3.79%\t11848\tcontd attempts collect debt not owed\r\n",
      "3.37%\t10555\tdeposits and withdrawals\r\n",
      "2.76%\t8625\tapplication\r\n",
      "2.22%\t6938\tbilling disputes\r\n",
      "2.21%\t6920\tcommunication tactics\r\n",
      "2.00%\t6273\tother\r\n",
      "1.81%\t5663\tproblems caused by my funds being low\r\n",
      "1.67%\t5214\tdisclosure verification of debt\r\n",
      "1.55%\t4858\tcredit reporting companys investigation\r\n",
      "1.46%\t4560\tmanaging the loan or lease\r\n",
      "1.39%\t4357\tunable to get credit reportcredit score\r\n",
      "1.39%\t4350\tsettlement process and costs\r\n",
      "1.23%\t3844\trepaying your loan\r\n",
      "1.22%\t3821\tproblems when you are unable to pay\r\n",
      "1.10%\t3431\tapr or interest rate\r\n",
      "1.05%\t3276\tidentity theft  fraud  embezzlement\r\n",
      "1.03%\t3226\tmakingreceiving payments\r\n",
      "0.91%\t2832\timproper contact or sharing of info\r\n",
      "0.89%\t2795\tclosingcancelling account\r\n",
      "0.89%\t2774\tcredit decision  underwriting\r\n",
      "0.80%\t2508\tfalse statements or representation\r\n",
      "0.80%\t2505\ttakingthreatening an illegal action\r\n",
      "0.77%\t2422\tusing a debit or atm card\r\n",
      "0.62%\t1944\tdealing with my lender or servicer\r\n",
      "0.57%\t1797\tlate fee\r\n",
      "0.54%\t1701\tcredit reporting\r\n",
      "0.53%\t1647\tcant repay my loan\r\n",
      "0.48%\t1490\tcredit determination\r\n",
      "0.47%\t1477\timproper use of my credit report\r\n",
      "0.46%\t1453\tcredit monitoring or identity protection\r\n",
      "0.44%\t1367\tcustomer service  customer relations\r\n",
      "0.43%\t1343\tcredit card protection  debt protection\r\n",
      "0.40%\t1242\ttaking out the loan or lease\r\n",
      "0.39%\t1220\tbilling statement\r\n",
      "0.38%\t1193\tadvertising and marketing\r\n",
      "0.37%\t1155\tpayoff process\r\n",
      "0.37%\t1149\tcredit line increasedecrease\r\n",
      "0.35%\t1098\ttransaction issue\r\n",
      "0.34%\t1075\tother fee\r\n",
      "0.34%\t1061\tdelinquent account\r\n",
      "0.32%\t1003\tcollection practices\r\n",
      "0.32%\t1002\trewards\r\n",
      "0.29%\t904\tcollection debt dispute\r\n",
      "0.26%\t807\tcharged fees or interest i didnt expect\r\n",
      "0.20%\t640\tunsolicited issuance of credit card\r\n",
      "\t\r\n",
      "bottom 10 issues:\t\r\n",
      "0.00%\t1\tissue\r\n",
      "0.02%\t64\tincorrectmissing disclosures or info\r\n",
      "0.02%\t71\tcharged bank acct wrong day or amt\r\n",
      "0.02%\t75\tconvenience checks\r\n",
      "0.03%\t92\tpayment to acct not credited\r\n",
      "0.03%\t95\tbalance transfer fee\r\n",
      "0.03%\t98\twrong amount charged or received\r\n",
      "0.03%\t104\tcash advance fee\r\n",
      "0.04%\t118\treceived a loan i didnt apply for\r\n",
      "0.04%\t127\toverlimit fee\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/leiyang/results2/part-0000* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*3.2.1 OPTIONAL - * Using 2 reducers: \n",
    "- What are the top 50 most frequent terms in your word count analysis? \n",
    "- Present the top 50 terms and their frequency and their relative frequency. \n",
    "- If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.3.* Shopping Cart Analysis\n",
    "Product Recommendations: \n",
    "- The action or practice of selling additional products or services\n",
    "to existing customers is called cross-selling. \n",
    "- Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers.\n",
    "- One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset [here](https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0):\n",
    "\n",
    "- Each line in this dataset represents a browsing session of a customer.\n",
    "- On each line, each string of 8 characters represents the id of an item browsed during that session.\n",
    "- The items are separated by spaces.\n",
    "\n",
    "- Here are the first few lines of the ProductPurchaseData\n",
    " - FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\n",
    " - GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192\n",
    " - ELE17451 GRO73461 DAI22896 SNA99873 FRO86643\n",
    " - ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465\n",
    " - ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444\n",
    "\n",
    "**Do some exploratory data analysis of this dataset.**\n",
    "\n",
    "- How many unique items are available from this supplier?\n",
    "- **Using a single reducer:**\n",
    " - Report your findings such as number of unique products; largest basket; \n",
    " - Report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.\n",
    "\n",
    "###Mapper - pair count\n",
    "- where $size$ is the basket size for each new session, otherwise zero to minimize data transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products\n",
    "    products = line.strip().split(' ')\n",
    "    size = len(products)\n",
    "    if size==0:\n",
    "        continue\n",
    "    for i in range(size):                \n",
    "        # emit word key\n",
    "        print '%s,%s,%s' %(products[i], 1, size if i==0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer - pair count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,Reducer_cnt,1\\n\")\n",
    "\n",
    "max_size = 0\n",
    "current_prod = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get mapper output\n",
    "    prod, count, size = line.strip().split(',', 2)\n",
    "    \n",
    "    # skip bad counts\n",
    "    try:\n",
    "        count = int(count)\n",
    "        size = int(size)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # handle basket size\n",
    "    max_size = max(max_size, size)\n",
    "        \n",
    "    # count unique and get frequency\n",
    "    if current_prod == prod:\n",
    "        current_count += count\n",
    "    else:\n",
    "        # one product just finishes streaming\n",
    "        if current_prod:            \n",
    "            # emit product count\n",
    "            print '%s,%s' %(current_prod, current_count)            \n",
    "                    \n",
    "        # reset for new prod\n",
    "        current_prod = prod\n",
    "        current_count = count\n",
    "\n",
    "#print 'max basket size: %d' %max_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper - relative frequency & sort\n",
    "- use **order inversion** for reletive frequency, for each word emit $(dummy\\_sort, *, count)$ and $(dummy\\_sort, product, count)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_3s,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:      \n",
    "    # get product and count\n",
    "    prod, count = line.strip().split(',')\n",
    "    # emit prod as key, and count\n",
    "    print \"%s,%s,%s\" %(count, prod, count)\n",
    "    # for order inversion, to calculate total count\n",
    "    print '%d,%s,%s' %(1e+10, '*', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer - relative frequency & sort\n",
    "- get the top 50 pairs with most count\n",
    "- obtain unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,Reducer_cnt,1\\n\")\n",
    "\n",
    "n_total = 0\n",
    "n_top = 50\n",
    "n_unique = 0\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    \n",
    "    dummy, product, count = line.strip().split(',', 2)\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "       \n",
    "    # handle total\n",
    "    if product == '*':\n",
    "        n_total += count\n",
    "        continue\n",
    "    \n",
    "    # get relative frequency\n",
    "    n_unique += 1\n",
    "    if n_unique <= n_top:\n",
    "        print '%s\\t%s\\t%.4f%%' %(product, count, 100.0*count/n_total)\n",
    "    \n",
    "print 'total browsing items: %d' %n_total\n",
    "print 'unique product: %d' %n_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MapReducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6113682566909254738/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2500948326253885932.jar tmpDir=null\n",
      "Deleted results2\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar2055398774848871732/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob4358016637415128494.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# job 1 - pair count\n",
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D map.output.key.value.fields.spec=0:1- \\\n",
    "-D mapred.text.key.comparator.options='-k1,1' \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/leiyang/ProductPurchaseData.txt \\\n",
    "-output results\n",
    "\n",
    "# job 2 - relative frequency & sort with order inversion\n",
    "!hdfs dfs -rm -r results2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D map.output.key.value.fields.spec=0-1:0- \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2' \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper_s.py,reducer_s.py \\\n",
    "-mapper mapper_s.py \\\n",
    "-reducer reducer_s.py \\\n",
    "-input /user/leiyang/results/part-0000* \\\n",
    "-output results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779\t6667\t1.7507%\r\n",
      "FRO40251\t3881\t1.0191%\r\n",
      "ELE17451\t3875\t1.0175%\r\n",
      "GRO73461\t3602\t0.9458%\r\n",
      "SNA80324\t3044\t0.7993%\r\n",
      "ELE32164\t2851\t0.7486%\r\n",
      "DAI75645\t2736\t0.7184%\r\n",
      "SNA45677\t2455\t0.6447%\r\n",
      "FRO31317\t2330\t0.6118%\r\n",
      "DAI85309\t2293\t0.6021%\r\n",
      "ELE26917\t2292\t0.6019%\r\n",
      "FRO80039\t2233\t0.5864%\r\n",
      "GRO21487\t2115\t0.5554%\r\n",
      "SNA99873\t2083\t0.5470%\r\n",
      "GRO59710\t2004\t0.5262%\r\n",
      "GRO71621\t1920\t0.5042%\r\n",
      "FRO85978\t1918\t0.5036%\r\n",
      "GRO30386\t1840\t0.4832%\r\n",
      "ELE74009\t1816\t0.4769%\r\n",
      "GRO56726\t1784\t0.4685%\r\n",
      "DAI63921\t1773\t0.4656%\r\n",
      "GRO46854\t1756\t0.4611%\r\n",
      "ELE66600\t1713\t0.4498%\r\n",
      "DAI83733\t1712\t0.4496%\r\n",
      "FRO32293\t1702\t0.4469%\r\n",
      "ELE66810\t1697\t0.4456%\r\n",
      "SNA55762\t1646\t0.4322%\r\n",
      "DAI22177\t1627\t0.4272%\r\n",
      "FRO78087\t1531\t0.4020%\r\n",
      "ELE99737\t1516\t0.3981%\r\n",
      "ELE34057\t1489\t0.3910%\r\n",
      "GRO94758\t1489\t0.3910%\r\n",
      "FRO35904\t1436\t0.3771%\r\n",
      "FRO53271\t1420\t0.3729%\r\n",
      "SNA93860\t1407\t0.3695%\r\n",
      "SNA90094\t1390\t0.3650%\r\n",
      "GRO38814\t1352\t0.3550%\r\n",
      "ELE56788\t1345\t0.3532%\r\n",
      "GRO61133\t1321\t0.3469%\r\n",
      "DAI88807\t1316\t0.3456%\r\n",
      "ELE74482\t1316\t0.3456%\r\n",
      "ELE59935\t1311\t0.3443%\r\n",
      "SNA96271\t1295\t0.3401%\r\n",
      "DAI43223\t1290\t0.3387%\r\n",
      "ELE91337\t1289\t0.3385%\r\n",
      "GRO15017\t1275\t0.3348%\r\n",
      "DAI31081\t1261\t0.3311%\r\n",
      "GRO81087\t1220\t0.3204%\r\n",
      "DAI22896\t1219\t0.3201%\r\n",
      "GRO85051\t1214\t0.3188%\r\n",
      "total browsing items: 380823\t\r\n",
      "unique product: 12591\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results2/part-0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*3.3.1 OPTIONAL* - Using 2 reducers:  \n",
    "- Report your findings such as number of unique products; largest basket; \n",
    "- Report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.\n",
    "\n",
    "**Notes:**\n",
    "- the challenge is from total calculation since we have multiple reducers, as only one will get the ***** key and be able to calculate the marginal.\n",
    "- possible solution: \n",
    " - write a customer partitioner, emit two dummy pairs, dispatch one for each reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.4.* (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "- Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. \n",
    "- Write a map-reduce program to find products which are frequently browsed together. \n",
    "- Fix the support count (cooccurence count) to s = 100\n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent),\n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "**List the top 50 product pairs with corresponding support count (aka frequency)**, and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2.\n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.\n",
    "\n",
    "<img src=\"Pairs.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2):\n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right),\n",
    "and break ties in support (between pairs, if any exist)\n",
    "by taking the first ones in lexicographically increasing order.\n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    " \n",
    "|Spec | Value|\n",
    "|---|:---:|\n",
    "| Computer | single |\n",
    "| OS  | OS X El Capitan |\n",
    "| Processor | 2.2 GHz Intel Core i7  |\n",
    "| Memory | 16 GB 1600 MHz DDR3|\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.\n",
    "\n",
    "\n",
    "###Mapper\n",
    "- for each session (row), use pair pattern with **order inversion**, emit $((w_i\\_w_j),1)$ for all pairs, and one $(*,1)$ for the session (for total session count).\n",
    "- the fourth field of every emit is used to indicate basket size for every session (row), size is postive for *only* one emit, and zero for rest of the emit, so we minimize data transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,Mapper_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products from the session\n",
    "    products = line.strip().split(' ')\n",
    "    size = len(products)\n",
    "    if size==0:\n",
    "        continue\n",
    "    \n",
    "    # sort products the pair is lexicographically sound\n",
    "    products.sort()\n",
    "    \n",
    "    # get pairs of products\n",
    "    pairs = [[products[i], products[j]] for i in range(size) for j in range(i+1, size)]\n",
    "    \n",
    "    # emit dummy record\n",
    "    print '%s,%s' %('*', 1)\n",
    "    \n",
    "    # emit product pairs\n",
    "    for pair in pairs:\n",
    "        print '%s_%s,%s' %(pair[0], pair[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Combiner\n",
    "- local aggregation for count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for reducer being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,Combiner_cnt,1\\n\")\n",
    "\n",
    "current_pair = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:       \n",
    "    # get all products from the session\n",
    "    pair, count = line.strip().split(',', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # accumulate counts for whatever keys it receives\n",
    "    if current_pair == pair:\n",
    "        current_count += count\n",
    "    else:\n",
    "        # previous pair finishes streaming, emit results\n",
    "        if current_pair:            \n",
    "            print '%s,%s' %(current_pair, current_count)\n",
    "        # reset new pair\n",
    "        current_pair = pair\n",
    "        current_count = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer\n",
    "- count number of basket based on $(*, 1)$ emits\n",
    "- get suport and relative frequency for each pair in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for reducer being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,Reducer_cnt,1\\n\")\n",
    "\n",
    "n_basket = 0\n",
    "min_support = 100\n",
    "current_pair = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:       \n",
    "    # get all products from the session\n",
    "    pair, count = line.strip().split(',', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # get total sessions/baskets\n",
    "    if pair == '*':\n",
    "        n_basket += count\n",
    "        continue\n",
    "        \n",
    "    # get pair count\n",
    "    if current_pair == pair:\n",
    "        current_count += count\n",
    "    else:\n",
    "        # previous pair finishes streaming\n",
    "        if current_pair and current_count > min_support:\n",
    "            # get relative freq\n",
    "            rf = 100.0*current_count/n_basket\n",
    "            # emit\n",
    "            print '%s,%s,%.4f%%' %(current_pair, current_count, rf)\n",
    "        # reset new pair\n",
    "        current_pair = pair\n",
    "        current_count = count\n",
    "\n",
    "#print '\\ntotal basket: %d' %n_basket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper for sort (or use identity mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,Mapper_s_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # just emit\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer for sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,Reducer_s_cnt,1\\n\")\n",
    "\n",
    "n_out = 0\n",
    "n_top = 50\n",
    "\n",
    "print 'top %d pairs: ' %n_top\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # parse mapper output  \n",
    "    pair, count, rf = line.strip().split(',', 2)\n",
    "    n_out += 1\n",
    "    if n_out <= n_top:\n",
    "        w1, w2 = pair.split('_')\n",
    "        print '%s\\t%s\\t%s\\t%s' %(w1, w2, count, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MapReducing without combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6313500269707918499/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6247307708596164046.jar tmpDir=null\n",
      "Deleted results2\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar2314263346417839441/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob201592152313437354.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# job 1 - count\n",
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D map.output.key.value.fields.spec=0:1- \\\n",
    "-D mapred.text.key.comparator.options='-k1,1' \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/leiyang/ProductPurchaseData.txt \\\n",
    "-output results\n",
    "\n",
    "# job 2 - sort relative frequency\n",
    "!hdfs dfs -rm -r results2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=',' \\\n",
    "-D map.output.key.value.fields.spec=0-1:2- \\\n",
    "-D mapred.text.key.comparator.options='-k2,2nr -k1,1' \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper_s.py,reducer_s.py \\\n",
    "-mapper mapper_s.py \\\n",
    "-reducer reducer_s.py \\\n",
    "-input /user/leiyang/results/part-0000* \\\n",
    "-output results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW3.4 results without combiner\n",
    "- 3 mappers, 1 reducer\n",
    "\n",
    "<img src=\"HW3_4.counter.png\" alt=\"Drawing\" style=\"width: 880px;\"/>\n",
    "<img src=\"HW3_4.time.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 pairs: \t\r\n",
      "DAI62779\tELE17451\t1592\t5.1188%\r\n",
      "FRO40251\tSNA80324\t1412\t4.5400%\r\n",
      "DAI75645\tFRO40251\t1254\t4.0320%\r\n",
      "FRO40251\tGRO85051\t1213\t3.9002%\r\n",
      "DAI62779\tGRO73461\t1139\t3.6623%\r\n",
      "DAI75645\tSNA80324\t1130\t3.6333%\r\n",
      "DAI62779\tFRO40251\t1070\t3.4404%\r\n",
      "DAI62779\tSNA80324\t923\t2.9678%\r\n",
      "DAI62779\tDAI85309\t918\t2.9517%\r\n",
      "ELE32164\tGRO59710\t911\t2.9292%\r\n",
      "DAI62779\tDAI75645\t882\t2.8359%\r\n",
      "FRO40251\tGRO73461\t882\t2.8359%\r\n",
      "DAI62779\tELE92920\t877\t2.8198%\r\n",
      "FRO40251\tFRO92469\t835\t2.6848%\r\n",
      "DAI62779\tELE32164\t832\t2.6752%\r\n",
      "DAI75645\tGRO73461\t712\t2.2893%\r\n",
      "DAI43223\tELE32164\t711\t2.2861%\r\n",
      "DAI62779\tGRO30386\t709\t2.2797%\r\n",
      "ELE17451\tFRO40251\t697\t2.2411%\r\n",
      "DAI85309\tELE99737\t659\t2.1189%\r\n",
      "DAI62779\tELE26917\t650\t2.0900%\r\n",
      "GRO21487\tGRO73461\t631\t2.0289%\r\n",
      "DAI62779\tSNA45677\t604\t1.9421%\r\n",
      "ELE17451\tSNA80324\t597\t1.9196%\r\n",
      "DAI62779\tGRO71621\t595\t1.9131%\r\n",
      "DAI62779\tSNA55762\t593\t1.9067%\r\n",
      "DAI62779\tDAI83733\t586\t1.8842%\r\n",
      "ELE17451\tGRO73461\t580\t1.8649%\r\n",
      "GRO73461\tSNA80324\t562\t1.8070%\r\n",
      "DAI62779\tGRO59710\t561\t1.8038%\r\n",
      "DAI62779\tFRO80039\t550\t1.7684%\r\n",
      "DAI75645\tELE17451\t547\t1.7588%\r\n",
      "DAI62779\tSNA93860\t537\t1.7266%\r\n",
      "DAI55148\tDAI62779\t526\t1.6913%\r\n",
      "DAI43223\tGRO59710\t512\t1.6462%\r\n",
      "ELE17451\tELE32164\t511\t1.6430%\r\n",
      "DAI62779\tSNA18336\t506\t1.6270%\r\n",
      "ELE32164\tGRO73461\t486\t1.5627%\r\n",
      "DAI62779\tFRO78087\t482\t1.5498%\r\n",
      "DAI85309\tELE17451\t482\t1.5498%\r\n",
      "DAI62779\tGRO94758\t479\t1.5401%\r\n",
      "DAI62779\tGRO21487\t471\t1.5144%\r\n",
      "GRO85051\tSNA80324\t471\t1.5144%\r\n",
      "ELE17451\tGRO30386\t468\t1.5048%\r\n",
      "FRO85978\tSNA95666\t463\t1.4887%\r\n",
      "DAI62779\tFRO19221\t462\t1.4855%\r\n",
      "DAI62779\tGRO46854\t461\t1.4823%\r\n",
      "DAI43223\tDAI62779\t459\t1.4758%\r\n",
      "ELE92920\tSNA18336\t455\t1.4630%\r\n",
      "DAI88079\tFRO40251\t446\t1.4340%\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results2/part-0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MapReducing with combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar3295224449230340302/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob7434701589508937802.jar tmpDir=null\n",
      "Deleted results2\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar943447964334990827/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob5402224175670776010.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# job 1 - add combiner below\n",
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D map.output.key.value.fields.spec=0:1- \\\n",
    "-D mapred.text.key.comparator.options='-k1,1' \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper.py,reducer.py,combiner.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner combiner.py \\\n",
    "-input /user/leiyang/ProductPurchaseData.txt \\\n",
    "-output results\n",
    "\n",
    "# job 2 - sort relative frequency\n",
    "!hdfs dfs -rm -r results2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=',' \\\n",
    "-D map.output.key.value.fields.spec=0-1:2- \\\n",
    "-D mapred.text.key.comparator.options='-k2,2nr -k1,1' \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper_s.py,reducer_s.py \\\n",
    "-mapper mapper_s.py \\\n",
    "-reducer reducer_s.py \\\n",
    "-input /user/leiyang/results/part-0000* \\\n",
    "-output results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW3.4 Results with combiner\n",
    "- 3 mappers, 1 reducer\n",
    "- the combiner was called 1 time by each map process, total 3 times\n",
    "\n",
    "<img src=\"HW3_4.combiner.counter.png\" alt=\"Drawing\" style=\"width: 880px;\"/>\n",
    "<img src=\"HW3_4.combiner.time.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 pairs: \t\r\n",
      "DAI62779\tELE17451\t1592\t5.1188%\r\n",
      "FRO40251\tSNA80324\t1412\t4.5400%\r\n",
      "DAI75645\tFRO40251\t1254\t4.0320%\r\n",
      "FRO40251\tGRO85051\t1213\t3.9002%\r\n",
      "DAI62779\tGRO73461\t1139\t3.6623%\r\n",
      "DAI75645\tSNA80324\t1130\t3.6333%\r\n",
      "DAI62779\tFRO40251\t1070\t3.4404%\r\n",
      "DAI62779\tSNA80324\t923\t2.9678%\r\n",
      "DAI62779\tDAI85309\t918\t2.9517%\r\n",
      "ELE32164\tGRO59710\t911\t2.9292%\r\n",
      "DAI62779\tDAI75645\t882\t2.8359%\r\n",
      "FRO40251\tGRO73461\t882\t2.8359%\r\n",
      "DAI62779\tELE92920\t877\t2.8198%\r\n",
      "FRO40251\tFRO92469\t835\t2.6848%\r\n",
      "DAI62779\tELE32164\t832\t2.6752%\r\n",
      "DAI75645\tGRO73461\t712\t2.2893%\r\n",
      "DAI43223\tELE32164\t711\t2.2861%\r\n",
      "DAI62779\tGRO30386\t709\t2.2797%\r\n",
      "ELE17451\tFRO40251\t697\t2.2411%\r\n",
      "DAI85309\tELE99737\t659\t2.1189%\r\n",
      "DAI62779\tELE26917\t650\t2.0900%\r\n",
      "GRO21487\tGRO73461\t631\t2.0289%\r\n",
      "DAI62779\tSNA45677\t604\t1.9421%\r\n",
      "ELE17451\tSNA80324\t597\t1.9196%\r\n",
      "DAI62779\tGRO71621\t595\t1.9131%\r\n",
      "DAI62779\tSNA55762\t593\t1.9067%\r\n",
      "DAI62779\tDAI83733\t586\t1.8842%\r\n",
      "ELE17451\tGRO73461\t580\t1.8649%\r\n",
      "GRO73461\tSNA80324\t562\t1.8070%\r\n",
      "DAI62779\tGRO59710\t561\t1.8038%\r\n",
      "DAI62779\tFRO80039\t550\t1.7684%\r\n",
      "DAI75645\tELE17451\t547\t1.7588%\r\n",
      "DAI62779\tSNA93860\t537\t1.7266%\r\n",
      "DAI55148\tDAI62779\t526\t1.6913%\r\n",
      "DAI43223\tGRO59710\t512\t1.6462%\r\n",
      "ELE17451\tELE32164\t511\t1.6430%\r\n",
      "DAI62779\tSNA18336\t506\t1.6270%\r\n",
      "ELE32164\tGRO73461\t486\t1.5627%\r\n",
      "DAI62779\tFRO78087\t482\t1.5498%\r\n",
      "DAI85309\tELE17451\t482\t1.5498%\r\n",
      "DAI62779\tGRO94758\t479\t1.5401%\r\n",
      "DAI62779\tGRO21487\t471\t1.5144%\r\n",
      "GRO85051\tSNA80324\t471\t1.5144%\r\n",
      "ELE17451\tGRO30386\t468\t1.5048%\r\n",
      "FRO85978\tSNA95666\t463\t1.4887%\r\n",
      "DAI62779\tFRO19221\t462\t1.4855%\r\n",
      "DAI62779\tGRO46854\t461\t1.4823%\r\n",
      "DAI43223\tDAI62779\t459\t1.4758%\r\n",
      "ELE92920\tSNA18336\t455\t1.4630%\r\n",
      "DAI88079\tFRO40251\t446\t1.4340%\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results2/part-0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.5*: Stripes\n",
    "- Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "- Report  the compute times for stripes job versus the Pairs job. \n",
    "- Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "- Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. \n",
    "- Discuss the differences in these counts between the Pairs and Stripes jobs\n",
    "\n",
    "<img src=\"Stripes.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "###Mapper\n",
    "- build associative array for each session, and do local in-memory aggregation\n",
    "- for the associative array, we implement the rule that *any key will only have words that alphabetically behind it in the associative array*, to have unique pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_5,Mapper_cnt,1\\n\")\n",
    "\n",
    "# composite associative array\n",
    "H = {}\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products from the session\n",
    "    products = line.strip().split(' ')\n",
    "    size = len(products)\n",
    "    if size==0:\n",
    "        continue\n",
    "    \n",
    "    # sort products so the pair is lexicographically sound\n",
    "    products.sort()\n",
    "    \n",
    "    # get pairs of products\n",
    "    pairs = [[products[i], products[j]] for i in range(size) for j in range(i+1, size)]\n",
    "    \n",
    "    # emit dummy record\n",
    "    print '%s\\t%s' %('*', 1)\n",
    "    \n",
    "    # build associative arrays\n",
    "    for w1, w2 in pairs:\n",
    "        # each pair is lexicographically in order        \n",
    "        if w1 not in H:\n",
    "            # if w1 is new, add an associative array for it\n",
    "            H[w1] = {}\n",
    "            H[w1][w2] = 1            \n",
    "        elif w2 not in H[w1]:\n",
    "            # w1 is not new, but it doesn't have key for w2\n",
    "            H[w1][w2] = 1\n",
    "        else:\n",
    "            # both are there, increase it\n",
    "            H[w1][w2] += 1\n",
    "        \n",
    "# emit associative arrays\n",
    "for h in H:\n",
    "    print '%s\\t%s' %(h, str(H[h]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer\n",
    "- element-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "# function to combine associative array\n",
    "def elementSum(H1, H2):    \n",
    "    # make sure H1 is the long one\n",
    "    if len(H1)<len(H2):\n",
    "        H0 = H2\n",
    "        H2 = H1\n",
    "        H1 = H0\n",
    "    # merge shorter one H2 into longer one H1\n",
    "    for h in H2:\n",
    "        if h not in H1:\n",
    "            H1[h] = H2[h]\n",
    "        else:\n",
    "            H1[h] += H2[h]        \n",
    "    # return\n",
    "    return H1\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_5,Reducer_cnt,1\\n\")\n",
    "\n",
    "min_support = 100\n",
    "current_word = None\n",
    "current_aArray = None\n",
    "n_total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # parse out keyword and the associative array\n",
    "    word, aArray = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # get total basket\n",
    "    if word == '*':\n",
    "        n_total += int(aArray)\n",
    "        continue\n",
    "    \n",
    "    # get array into variable\n",
    "    cmdStr = 'aArray = ' + aArray\n",
    "    exec cmdStr\n",
    "        \n",
    "    # merge the associative array\n",
    "    if current_word == word:\n",
    "        current_aArray = elementSum(current_aArray, aArray)           \n",
    "    else:\n",
    "        # finish one word merge\n",
    "        if current_word:\n",
    "            # get the top pairs with heap\n",
    "            for p in current_aArray:\n",
    "                if current_aArray[p] > min_support:                    \n",
    "                    # get relative freq\n",
    "                    rf = 100.0*current_aArray[p]/n_total\n",
    "                    print '%s,%s,%s,%.4f%%' %(current_word, p, current_aArray[p], rf)\n",
    "        # reset for a new word\n",
    "        current_word = word\n",
    "        current_aArray = aArray\n",
    "\n",
    "#print '\\ntotal basket: %d' %n_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper to sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_5,Mapper_s_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # just emit\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer to sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_5,Reducer_s_cnt,1\\n\")\n",
    "\n",
    "n_out = 0\n",
    "n_top = 50\n",
    "\n",
    "print 'top %d pairs: ' %n_top\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # parse mapper output  \n",
    "    n_out += 1\n",
    "    if n_out <= n_top:        \n",
    "        print line.strip().replace(',', '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MapReducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar3782570145270895412/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob870881278814839645.jar tmpDir=null\n",
      "Deleted results2\n",
      "packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar5736039512355960039/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob3937477619845631898.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# job 1 - count\n",
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper.py,reducer.py,combiner.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/leiyang/ProductPurchaseData.txt \\\n",
    "-output results\n",
    "\n",
    "# job 2 - sort relative frequency\n",
    "!hdfs dfs -rm -r results2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=',' \\\n",
    "-D map.output.key.value.fields.spec=0-2:3- \\\n",
    "-D mapred.text.key.comparator.options='-k3,3nr -k1,1 -k2,2' \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper_s.py,reducer_s.py \\\n",
    "-mapper mapper_s.py \\\n",
    "-reducer reducer_s.py \\\n",
    "-input /user/leiyang/results/part-0000* \\\n",
    "-output results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW3.5 Results\n",
    "- 3 mappers, 1 reducer\n",
    "- the combiner was called 1 time by each map process, total 3 times\n",
    "- with the same configure, the execution time is reduced to 15 sec.  from 23 sec. of pair approach, about **33%** improvement\n",
    "\n",
    "<img src=\"HW3_5.counter.png\" alt=\"Drawing\" style=\"width: 880px;\"/>\n",
    "<img src=\"HW3_5.time.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 pairs: \t\r\n",
      "DAI62779\tELE17451\t1592\t5.1188%\r\n",
      "FRO40251\tSNA80324\t1412\t4.5400%\r\n",
      "DAI75645\tFRO40251\t1254\t4.0320%\r\n",
      "FRO40251\tGRO85051\t1213\t3.9002%\r\n",
      "DAI62779\tGRO73461\t1139\t3.6623%\r\n",
      "DAI75645\tSNA80324\t1130\t3.6333%\r\n",
      "DAI62779\tFRO40251\t1070\t3.4404%\r\n",
      "DAI62779\tSNA80324\t923\t2.9678%\r\n",
      "DAI62779\tDAI85309\t918\t2.9517%\r\n",
      "ELE32164\tGRO59710\t911\t2.9292%\r\n",
      "DAI62779\tDAI75645\t882\t2.8359%\r\n",
      "FRO40251\tGRO73461\t882\t2.8359%\r\n",
      "DAI62779\tELE92920\t877\t2.8198%\r\n",
      "FRO40251\tFRO92469\t835\t2.6848%\r\n",
      "DAI62779\tELE32164\t832\t2.6752%\r\n",
      "DAI75645\tGRO73461\t712\t2.2893%\r\n",
      "DAI43223\tELE32164\t711\t2.2861%\r\n",
      "DAI62779\tGRO30386\t709\t2.2797%\r\n",
      "ELE17451\tFRO40251\t697\t2.2411%\r\n",
      "DAI85309\tELE99737\t659\t2.1189%\r\n",
      "DAI62779\tELE26917\t650\t2.0900%\r\n",
      "GRO21487\tGRO73461\t631\t2.0289%\r\n",
      "DAI62779\tSNA45677\t604\t1.9421%\r\n",
      "ELE17451\tSNA80324\t597\t1.9196%\r\n",
      "DAI62779\tGRO71621\t595\t1.9131%\r\n",
      "DAI62779\tSNA55762\t593\t1.9067%\r\n",
      "DAI62779\tDAI83733\t586\t1.8842%\r\n",
      "ELE17451\tGRO73461\t580\t1.8649%\r\n",
      "GRO73461\tSNA80324\t562\t1.8070%\r\n",
      "DAI62779\tGRO59710\t561\t1.8038%\r\n",
      "DAI62779\tFRO80039\t550\t1.7684%\r\n",
      "DAI75645\tELE17451\t547\t1.7588%\r\n",
      "DAI62779\tSNA93860\t537\t1.7266%\r\n",
      "DAI55148\tDAI62779\t526\t1.6913%\r\n",
      "DAI43223\tGRO59710\t512\t1.6462%\r\n",
      "ELE17451\tELE32164\t511\t1.6430%\r\n",
      "DAI62779\tSNA18336\t506\t1.6270%\r\n",
      "ELE32164\tGRO73461\t486\t1.5627%\r\n",
      "DAI62779\tFRO78087\t482\t1.5498%\r\n",
      "DAI85309\tELE17451\t482\t1.5498%\r\n",
      "DAI62779\tGRO94758\t479\t1.5401%\r\n",
      "DAI62779\tGRO21487\t471\t1.5144%\r\n",
      "GRO85051\tSNA80324\t471\t1.5144%\r\n",
      "ELE17451\tGRO30386\t468\t1.5048%\r\n",
      "FRO85978\tSNA95666\t463\t1.4887%\r\n",
      "DAI62779\tFRO19221\t462\t1.4855%\r\n",
      "DAI62779\tGRO46854\t461\t1.4823%\r\n",
      "DAI43223\tDAI62779\t459\t1.4758%\r\n",
      "ELE92920\tSNA18336\t455\t1.4630%\r\n",
      "DAI88079\tFRO40251\t446\t1.4340%\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results2/part-0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###OPTIONAL: all HW below this are optional\n",
    "\n",
    "** Preliminary information **\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "** Apriori background information **\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction.\n",
    "\n",
    "###*HW3.6*\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise and what kind of . Define confidence and lift.\n",
    "\n",
    "NOTE:\n",
    "For the remaining homework use the online browsing behavior dataset located at (same dataset as used above):\n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer.\n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session.\n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData:\n",
    "\n",
    "- FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\n",
    "- GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192\n",
    "- ELE17451 GRO73461 DAI22896 SNA99873 FRO86643\n",
    "- ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465\n",
    "- ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Answer:\n",
    "- Aprior algorithm is used to find frequent itemsets, each iteration has two scans of data and a filtering in between:\n",
    " 1. generate a condidate set $C_k$ for itemsets of size $k$, based on the output of previous iteration $L_{k-1}$\n",
    " 2. remove all members from the set whose support is less than the user specified threshold $s_i$\n",
    " 3. generate the final set $L_k$ for frequent itemset of size $k$, based on output after filtering\n",
    " \n",
    " \n",
    "- For example, to find itemsets of size $k$ from a basket set, the process is:\n",
    " 1. count all single words from all baskets, output $C_1$\n",
    " 2. remove all words with support below threshold, output $L_1$\n",
    " 3. using $L_1$, generate candidate set for frequent pair set $C_2$\n",
    " 5. remove all pairs with support below threshold, get $L_2$\n",
    " 6. using $L_2$, generate candidate set for frequent triple set $C_3$\n",
    " 7. remove all triples with support below threshold, get $L_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.7.* Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services\n",
    "to existing customers is called cross-selling. Giving product recommendation is\n",
    "one of the examples of cross-selling that are frequently used by online retailers.\n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website\n",
    "\n",
    "- Write a program using the A-priori algorithm to find products which are frequently browsed together. \n",
    "- Fix the support to s = 100 (i.e. product sets need to occur together at least 100 times to be considered frequent)\n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. A rule is of the form:\n",
    "\n",
    "- (item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    "\n",
    "- (item1, item5) ⇒ item2, supportCount ,support, confidence\n",
    "\n",
    "**Implementation Notes:**\n",
    "- each MapReduce job perform one round of APrior processing:\n",
    " - mapper: construct candidate set $C_k$\n",
    " - reducer: filter $C_k$ to get frequent item set $L_k$\n",
    "- to find itemsets of size 3, we will need 3 jobs\n",
    "\n",
    "\n",
    "###Mapper 1: get $C_1$\n",
    "- emit singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_7,Mapper_1_cnt,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get words and emit\n",
    "    for prod in line.strip().split(' '):\n",
    "        print '%s\\t%d' %(prod, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer 1: get $L_1$\n",
    "- only emit words whose frequency is above the support threshold (100)\n",
    "- can be used as **combiner** too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_7,Reducer_1_cnt,1\\n\")\n",
    "\n",
    "current_prod = None\n",
    "current_count = 0\n",
    "min_support = 100\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get k-v pair\n",
    "    prod, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # get count\n",
    "    if current_prod == prod:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_prod and current_count > min_support:\n",
    "            # emit prod above min support\n",
    "            print '%s\\t%d' %(current_prod, current_count)\n",
    "        # reset\n",
    "        current_prod = prod\n",
    "        current_count = count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper 2: get $C_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_2.py\n",
    "#!/usr/bin/python\n",
    "import sys, subprocess \n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_7,Mapper_2_cnt,1\\n\")\n",
    "\n",
    "singleton = []\n",
    "cat = subprocess.Popen(['hdfs', 'dfs', '-cat', 'results1/part-0*'], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "    singleton.append(line.strip().split('\\t')[0])\n",
    "\n",
    "# read the input data\n",
    "for line in sys.stdin:   \n",
    "    \n",
    "    line = line.strip()\n",
    "        \n",
    "    # get words for each session\n",
    "    prod = line.strip().split(' ')\n",
    "        \n",
    "    # keep product from singleton set only\n",
    "    products = [val for val in prod if val in singleton]\n",
    "    products.sort()\n",
    "    \n",
    "    # get pairs to emit\n",
    "    size = len(products)\n",
    "    pairs = [products[i] + '_' + products[j] for i in range(size) for j in range(i+1, size)]\n",
    "    for p in pairs:\n",
    "        print '%s\\t%d' %(p, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer 2: get $L_2$ \n",
    "- same as Reducer 1, since we have identical k-v format (%s\\t%d) from the mapper\n",
    "- can also be used as combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### same as reducer_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Mapper 3: get $C_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.py\n",
    "#!/usr/bin/python\n",
    "import sys, subprocess \n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_7,Mapper_3_cnt,1\\n\")\n",
    "\n",
    "# load the frequent freqPairs given by Job 2\n",
    "freqPair = []\n",
    "cat = subprocess.Popen(['hdfs', 'dfs', '-cat', 'results2/part-0*'], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "    freqPair.append(line.strip().split('\\t')[0])\n",
    "    \n",
    "# still read frequent freqPairs first, then session data to generate triples\n",
    "for line in sys.stdin:   \n",
    "    \n",
    "    line = line.strip()\n",
    "            \n",
    "    # get products from each session\n",
    "    prod = line.split(' ')\n",
    "    prod.sort()\n",
    "    n = len(prod)\n",
    "    \n",
    "    # generate freqPairs and triples from the session, in the format of a_b and a_b_c, alphabetically sorted\n",
    "    triples = [[prod[i],prod[j],prod[k]] for i in range(n) for j in range(i+1,n) for k in range(i+2,n)]\n",
    "    pairs = [prod[i]+'_'+prod[j] for i in range(n) for j in range(i+1,n)]\n",
    "\n",
    "    # processing pairs\n",
    "    for pair in pairs:\n",
    "        # if the pair is in freqPair, emit a dummy key a_b_*\n",
    "        if pair in freqPair:\n",
    "            print '%s_*\\t%d' %(pair, 1)\n",
    "\n",
    "    # processing triples\n",
    "    for tri in triples:\n",
    "        # from each triple a_b_c: check if the 3 child-pairs (a_b, b_c, a_c) are in the freqPair set\n",
    "        if tri[0]+'_'+tri[1] in freqPair and tri[1]+'_'+tri[2] in freqPair and tri[0]+'_'+tri[2] in freqPair:\n",
    "            # if so, emit the triple a_b_c            \n",
    "            print '%s_%s_%s\\t%d' %(tri[0], tri[1], tri[2], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Reducer 3: get $L_3$\n",
    "- use order inversion to get confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# increase counter for mapper being called\n",
    "sys.stderr.write(\"reporter:counter:HW3_7,Reducer_3_cnt,1\\n\")\n",
    "\n",
    "current_prod = None\n",
    "current_dummy = None\n",
    "current_count = 0\n",
    "min_support = 100\n",
    "marginal = 0\n",
    "\n",
    "for line in sys.stdin:   \n",
    "        \n",
    "    # get k-v freqPair\n",
    "    prod, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # handle marginal with dummy key\n",
    "    if '*' == prod[-1]:        \n",
    "        if current_dummy == prod:\n",
    "            # accumulate marginal\n",
    "            marginal += count\n",
    "        else:\n",
    "            # reset marginal for new dummy key\n",
    "            current_dummy = prod\n",
    "            marginal = count\n",
    "        continue\n",
    "\n",
    "    # processing triple and emit rules\n",
    "    if current_prod == prod:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_prod and current_count > min_support:\n",
    "            # for debug, check if current dummy matches current triple\n",
    "            if current_prod[:-8] != current_dummy[:-1]:\n",
    "                print 'WARNING: mismatch between %s and %s(%d)' %(current_prod, current_dummy, marginal)\n",
    "            else:\n",
    "                # emit triples for the rule\n",
    "                w1,w2,w3 = current_prod.split('_')\n",
    "                conf = 100.0*current_count/marginal\n",
    "                print '(%s, %s) => %s, %d, %d, %.2f%%' %(w1, w2, w3, current_count, marginal, conf)\n",
    "            \n",
    "        # reset for new triple\n",
    "        current_prod = prod\n",
    "        current_count = count\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MapReducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results1\r\n"
     ]
    }
   ],
   "source": [
    "# job 1 - get L_1 for frequent singletons\n",
    "!hdfs dfs -rm -r results1\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper_1.py,reducer_1.py \\\n",
    "-mapper mapper_1.py \\\n",
    "-reducer reducer_1.py \\\n",
    "-combiner reducer_1.py \\\n",
    "-input /user/leiyang/ProductPurchaseData.txt \\\n",
    "-output results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results2\r\n"
     ]
    }
   ],
   "source": [
    "# job 2 - get L_2 for frequent pairs\n",
    "!hdfs dfs -rm -r results2\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper_2.py,reducer_1.py \\\n",
    "-mapper mapper_2.py \\\n",
    "-reducer reducer_1.py \\\n",
    "-input /user/leiyang/ProductPurchaseData.txt \\\n",
    "-output results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results3\r\n"
     ]
    }
   ],
   "source": [
    "# job 3 - get L_3 and calculate association rules\n",
    "!hdfs dfs -rm -r results3\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mapper_3.py,reducer_3.py \\\n",
    "-mapper mapper_3.py \\\n",
    "-reducer reducer_3.py \\\n",
    "-input /user/leiyang/ProductPurchaseData.txt \\\n",
    "-output results3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DAI22896, DAI62779) => GRO73461, 101, 297, 34.01%\t\r\n",
      "WARNING: mismatch between DAI23334_DAI62779_ELE92920 and DAI31081_DAI43223_*(123)\t\r\n",
      "(DAI31081, DAI62779) => ELE17451, 103, 364, 28.30%\t\r\n",
      "(DAI31081, DAI75645) => FRO40251, 122, 206, 59.22%\t\r\n",
      "(DAI31081, ELE32164) => GRO59710, 112, 312, 35.90%\t\r\n",
      "(DAI31081, FRO40251) => GRO85051, 102, 280, 36.43%\t\r\n",
      "WARNING: mismatch between DAI31081_FRO40251_SNA80324 and DAI31081_FRO53271_*(161)\t\r\n",
      "(DAI42083, DAI62779) => DAI92600, 105, 117, 89.74%\t\r\n",
      "(DAI42083, DAI92600) => ELE17451, 117, 256, 45.70%\t\r\n",
      "(DAI42493, DAI62779) => ELE17451, 112, 309, 36.25%\t\r\n",
      "(DAI42493, DAI62779) => ELE92920, 112, 309, 36.25%\t\r\n",
      "(DAI42493, DAI62779) => SNA18336, 109, 309, 35.28%\t\r\n",
      "(DAI43223, DAI62779) => ELE17451, 227, 459, 49.46%\t\r\n",
      "(DAI43223, DAI62779) => ELE32164, 287, 459, 62.53%\t\r\n",
      "(DAI43223, DAI62779) => GRO59710, 205, 459, 44.66%\t\r\n",
      "(DAI43223, ELE17451) => ELE32164, 206, 326, 63.19%\t\r\n",
      "(DAI43223, ELE17451) => GRO59710, 156, 326, 47.85%\t\r\n",
      "(DAI43223, ELE32164) => GRO59710, 287, 711, 40.37%\t\r\n",
      "(DAI43223, ELE32164) => GRO73461, 111, 711, 15.61%\t\r\n",
      "(DAI55148, DAI62779) => DAI75645, 163, 526, 30.99%\t\r\n",
      "(DAI55148, DAI62779) => ELE17451, 150, 526, 28.52%\t\r\n",
      "(DAI55148, DAI62779) => FRO40251, 189, 526, 35.93%\t\r\n",
      "WARNING: mismatch between DAI55148_DAI62779_SNA80324 and DAI55148_DAI75645_*(299)\t\r\n",
      "(DAI55148, DAI75645) => ELE17451, 106, 299, 35.45%\t\r\n",
      "(DAI55148, DAI75645) => FRO40251, 120, 299, 40.13%\t\r\n",
      "WARNING: mismatch between DAI55148_DAI75645_SNA80324 and DAI55148_DAI85309_*(131)\t\r\n",
      "(DAI55148, ELE17451) => FRO40251, 120, 305, 39.34%\t\r\n",
      "WARNING: mismatch between DAI55148_ELE17451_SNA80324 and DAI55148_ELE32164_*(134)\t\r\n",
      "(DAI55148, FRO40251) => FRO92469, 105, 343, 30.61%\t\r\n",
      "WARNING: mismatch between DAI55148_FRO40251_SNA80324 and DAI55148_FRO92469_*(108)\t\r\n",
      "(DAI55911, FRO40251) => GRO85051, 133, 232, 57.33%\t\r\n",
      "(DAI62779, DAI75645) => DAI85309, 103, 882, 11.68%\t\r\n",
      "(DAI62779, DAI75645) => ELE17451, 328, 882, 37.19%\t\r\n",
      "(DAI62779, DAI75645) => ELE20847, 115, 882, 13.04%\t\r\n",
      "(DAI62779, DAI75645) => ELE26917, 101, 882, 11.45%\t\r\n",
      "(DAI62779, DAI75645) => ELE92920, 130, 882, 14.74%\t\r\n",
      "(DAI62779, DAI75645) => FRO40251, 412, 882, 46.71%\t\r\n",
      "(DAI62779, DAI75645) => GRO30386, 137, 882, 15.53%\t\r\n",
      "(DAI62779, DAI75645) => GRO73461, 261, 882, 29.59%\t\r\n",
      "(DAI62779, DAI75645) => GRO85051, 154, 882, 17.46%\t\r\n",
      "(DAI62779, DAI75645) => SNA80324, 421, 882, 47.73%\t\r\n",
      "(DAI62779, DAI83733) => DAI85309, 138, 586, 23.55%\t\r\n",
      "(DAI62779, DAI83733) => ELE17451, 147, 586, 25.09%\t\r\n",
      "(DAI62779, DAI83733) => ELE92920, 103, 586, 17.58%\t\r\n",
      "(DAI62779, DAI85309) => ELE17451, 339, 918, 36.93%\t\r\n",
      "(DAI62779, DAI85309) => ELE32164, 141, 918, 15.36%\t\r\n",
      "(DAI62779, DAI85309) => ELE92920, 191, 918, 20.81%\t\r\n",
      "(DAI62779, DAI85309) => ELE99737, 272, 918, 29.63%\t\r\n",
      "(DAI62779, DAI85309) => FRO40251, 127, 918, 13.83%\t\r\n",
      "(DAI62779, DAI85309) => GRO46854, 110, 918, 11.98%\t\r\n",
      "(DAI62779, DAI85309) => GRO71621, 116, 918, 12.64%\t\r\n",
      "(DAI62779, DAI85309) => GRO73461, 179, 918, 19.50%\t\r\n",
      "(DAI62779, DAI85309) => SNA18336, 151, 918, 16.45%\t\r\n",
      "(DAI62779, DAI85309) => SNA45677, 118, 918, 12.85%\t\r\n",
      "(DAI62779, DAI85309) => SNA55762, 116, 918, 12.64%\t\r\n",
      "(DAI62779, DAI88079) => FRO40251, 117, 117, 100.00%\t\r\n",
      "(DAI62779, DAI88807) => SNA72163, 104, 261, 39.85%\t\r\n",
      "(DAI62779, DAI91290) => ELE17451, 109, 353, 30.88%\t\r\n",
      "(DAI62779, ELE17451) => ELE26917, 160, 1592, 10.05%\t\r\n",
      "(DAI62779, ELE17451) => ELE32164, 277, 1592, 17.40%\t\r\n",
      "(DAI62779, ELE17451) => ELE56788, 107, 1592, 6.72%\t\r\n",
      "(DAI62779, ELE17451) => ELE74009, 112, 1592, 7.04%\t\r\n",
      "(DAI62779, ELE17451) => ELE92920, 345, 1592, 21.67%\t\r\n",
      "(DAI62779, ELE17451) => FRO31317, 106, 1592, 6.66%\t\r\n",
      "(DAI62779, ELE17451) => FRO40251, 406, 1592, 25.50%\t\r\n",
      "(DAI62779, ELE17451) => FRO78087, 121, 1592, 7.60%\t\r\n",
      "(DAI62779, ELE17451) => FRO80039, 130, 1592, 8.17%\t\r\n",
      "(DAI62779, ELE17451) => GRO15017, 111, 1592, 6.97%\t\r\n",
      "(DAI62779, ELE17451) => GRO30386, 218, 1592, 13.69%\t\r\n",
      "(DAI62779, ELE17451) => GRO46854, 109, 1592, 6.85%\t\r\n",
      "(DAI62779, ELE17451) => GRO59710, 213, 1592, 13.38%\t\r\n",
      "(DAI62779, ELE17451) => GRO71621, 159, 1592, 9.99%\t\r\n",
      "(DAI62779, ELE17451) => GRO73461, 245, 1592, 15.39%\t\r\n",
      "(DAI62779, ELE17451) => GRO81087, 160, 1592, 10.05%\t\r\n",
      "(DAI62779, ELE17451) => GRO85051, 178, 1592, 11.18%\t\r\n",
      "(DAI62779, ELE17451) => GRO94758, 180, 1592, 11.31%\t\r\n",
      "(DAI62779, ELE17451) => SNA18336, 244, 1592, 15.33%\t\r\n",
      "(DAI62779, ELE17451) => SNA38068, 118, 1592, 7.41%\t\r\n",
      "(DAI62779, ELE17451) => SNA45677, 158, 1592, 9.92%\t\r\n",
      "(DAI62779, ELE17451) => SNA55762, 157, 1592, 9.86%\t\r\n",
      "(DAI62779, ELE17451) => SNA59903, 202, 1592, 12.69%\t\r\n",
      "(DAI62779, ELE17451) => SNA72163, 107, 1592, 6.72%\t\r\n",
      "(DAI62779, ELE17451) => SNA80324, 417, 1592, 26.19%\t\r\n",
      "(DAI62779, ELE17451) => SNA90094, 103, 1592, 6.47%\t\r\n",
      "WARNING: mismatch between DAI62779_ELE17451_SNA96271 and DAI62779_ELE20398_*(113)\t\r\n",
      "(DAI62779, ELE20847) => FRO40251, 148, 275, 53.82%\t\r\n",
      "(DAI62779, ELE20847) => SNA80324, 153, 275, 55.64%\t\r\n",
      "WARNING: mismatch between DAI62779_ELE21353_FRO19221 and DAI62779_ELE24630_*(132)\t\r\n",
      "(DAI62779, ELE26917) => FRO40251, 109, 650, 16.77%\t\r\n",
      "(DAI62779, ELE32164) => ELE92920, 165, 832, 19.83%\t\r\n",
      "(DAI62779, ELE32164) => GRO30386, 118, 832, 14.18%\t\r\n",
      "(DAI62779, ELE32164) => GRO59710, 301, 832, 36.18%\t\r\n",
      "(DAI62779, ELE32164) => GRO73461, 131, 832, 15.75%\t\r\n",
      "(DAI62779, ELE59028) => FRO85978, 146, 370, 39.46%\t\r\n",
      "WARNING: mismatch between DAI62779_ELE59028_SNA93860 and DAI62779_ELE59935_*(351)\t\r\n",
      "(DAI62779, ELE74009) => ELE92920, 105, 432, 24.31%\t\r\n",
      "(DAI62779, ELE78169) => GRO94758, 109, 213, 51.17%\t\r\n",
      "(DAI62779, ELE92920) => FRO40251, 152, 877, 17.33%\t\r\n",
      "(DAI62779, ELE92920) => GRO15017, 143, 877, 16.31%\t\r\n",
      "(DAI62779, ELE92920) => GRO59710, 116, 877, 13.23%\t\r\n",
      "(DAI62779, ELE92920) => GRO81087, 134, 877, 15.28%\t\r\n",
      "(DAI62779, ELE92920) => SNA18336, 432, 877, 49.26%\t\r\n",
      "(DAI62779, FRO19221) => GRO73461, 142, 462, 30.74%\t\r\n",
      "(DAI62779, FRO19221) => SNA53220, 131, 462, 28.35%\t\r\n",
      "WARNING: mismatch between DAI62779_FRO19221_SNA93860 and DAI62779_FRO24098_*(133)\t\r\n",
      "(DAI62779, FRO40251) => FRO92469, 238, 1070, 22.24%\t\r\n",
      "(DAI62779, FRO40251) => GRO30386, 114, 1070, 10.65%\t\r\n",
      "(DAI62779, FRO40251) => GRO71621, 102, 1070, 9.53%\t\r\n",
      "(DAI62779, FRO40251) => GRO73461, 315, 1070, 29.44%\t\r\n",
      "(DAI62779, FRO40251) => GRO85051, 381, 1070, 35.61%\t\r\n",
      "(DAI62779, FRO40251) => SNA18336, 102, 1070, 9.53%\t\r\n",
      "(DAI62779, FRO40251) => SNA80324, 476, 1070, 44.49%\t\r\n",
      "(DAI62779, FRO85978) => SNA93860, 156, 434, 35.94%\t\r\n",
      "WARNING: mismatch between DAI62779_FRO85978_SNA95666 and DAI62779_FRO92261_*(223)\t\r\n",
      "WARNING: mismatch between DAI62779_FRO92469_SNA80324 and DAI62779_FRO98184_*(118)\t\r\n",
      "(DAI62779, GRO15017) => SNA18336, 105, 391, 26.85%\t\r\n",
      "(DAI62779, GRO21487) => GRO73461, 173, 471, 36.73%\t\r\n",
      "(DAI62779, GRO30386) => GRO59710, 101, 709, 14.25%\t\r\n",
      "(DAI62779, GRO30386) => GRO73461, 186, 709, 26.23%\t\r\n",
      "(DAI62779, GRO30386) => SNA80324, 136, 709, 19.18%\t\r\n",
      "(DAI62779, GRO38814) => GRO73461, 154, 389, 39.59%\t\r\n",
      "(DAI62779, GRO46854) => GRO73461, 135, 461, 29.28%\t\r\n",
      "(DAI62779, GRO71621) => GRO73461, 153, 595, 25.71%\t\r\n",
      "(DAI62779, GRO73461) => SNA45677, 112, 1139, 9.83%\t\r\n",
      "(DAI62779, GRO73461) => SNA55762, 109, 1139, 9.57%\t\r\n",
      "(DAI62779, GRO73461) => SNA80324, 198, 1139, 17.38%\t\r\n",
      "WARNING: mismatch between DAI62779_GRO73461_SNA96271 and DAI62779_GRO81087_*(396)\t\r\n",
      "WARNING: mismatch between DAI62779_GRO85051_SNA80324 and DAI62779_GRO88324_*(237)\t\r\n",
      "(DAI62779, GRO94758) => SNA45677, 116, 479, 24.22%\t\r\n",
      "WARNING: mismatch between DAI62779_GRO94758_SNA80324 and DAI62779_GRO99222_*(237)\t\r\n",
      "WARNING: mismatch between DAI62779_SNA45677_SNA96271 and DAI62779_SNA53220_*(248)\t\r\n",
      "WARNING: mismatch between DAI62779_SNA53220_SNA93860 and DAI62779_SNA55762_*(593)\t\r\n",
      "WARNING: mismatch between DAI62779_SNA59903_SNA72163 and DAI62779_SNA72163_*(279)\t\r\n",
      "(DAI75645, DAI85309) => FRO40251, 103, 212, 48.58%\t\r\n",
      "(DAI75645, DAI88079) => FRO40251, 148, 149, 99.33%\t\r\n",
      "(DAI75645, ELE17451) => FRO40251, 292, 547, 53.38%\t\r\n",
      "(DAI75645, ELE17451) => GRO73461, 121, 547, 22.12%\t\r\n",
      "(DAI75645, ELE17451) => SNA80324, 300, 547, 54.84%\t\r\n",
      "(DAI75645, ELE20847) => FRO40251, 153, 306, 50.00%\t\r\n",
      "(DAI75645, ELE20847) => SNA80324, 164, 306, 53.59%\t\r\n",
      "(DAI75645, ELE26917) => FRO40251, 128, 278, 46.04%\t\r\n",
      "(DAI75645, ELE26917) => SNA80324, 130, 278, 46.76%\t\r\n",
      "(DAI75645, ELE74009) => FRO40251, 139, 286, 48.60%\t\r\n",
      "(DAI75645, ELE74009) => SNA80324, 106, 286, 37.06%\t\r\n",
      "(DAI75645, FRO40251) => FRO53271, 123, 1254, 9.81%\t\r\n",
      "(DAI75645, FRO40251) => FRO92469, 251, 1254, 20.02%\t\r\n",
      "(DAI75645, FRO40251) => GRO21487, 107, 1254, 8.53%\t\r\n",
      "(DAI75645, FRO40251) => GRO30386, 109, 1254, 8.69%\t\r\n",
      "(DAI75645, FRO40251) => GRO38814, 117, 1254, 9.33%\t\r\n",
      "(DAI75645, FRO40251) => GRO71621, 112, 1254, 8.93%\t\r\n",
      "(DAI75645, FRO40251) => GRO73461, 293, 1254, 23.37%\t\r\n",
      "(DAI75645, FRO40251) => GRO81087, 112, 1254, 8.93%\t\r\n",
      "(DAI75645, FRO40251) => GRO85051, 395, 1254, 31.50%\t\r\n",
      "(DAI75645, FRO40251) => GRO94758, 118, 1254, 9.41%\t\r\n",
      "(DAI75645, FRO40251) => SNA45677, 120, 1254, 9.57%\t\r\n",
      "(DAI75645, FRO40251) => SNA55762, 131, 1254, 10.45%\t\r\n",
      "(DAI75645, FRO40251) => SNA80324, 550, 1254, 43.86%\t\r\n",
      "WARNING: mismatch between DAI75645_FRO47962_GRO73461 and DAI75645_FRO53271_*(210)\t\r\n",
      "WARNING: mismatch between DAI75645_FRO92469_SNA80324 and DAI75645_GRO15017_*(228)\t\r\n",
      "(DAI75645, GRO21487) => GRO73461, 114, 213, 53.52%\t\r\n",
      "(DAI75645, GRO30386) => SNA80324, 131, 239, 54.81%\t\r\n",
      "(DAI75645, GRO38814) => GRO73461, 101, 244, 41.39%\t\r\n",
      "WARNING: mismatch between DAI75645_GRO38814_SNA80324 and DAI75645_GRO44993_*(120)\t\r\n",
      "(DAI75645, GRO46854) => GRO73461, 101, 190, 53.16%\t\r\n",
      "(DAI75645, GRO73461) => SNA80324, 230, 712, 32.30%\t\r\n",
      "WARNING: mismatch between DAI75645_GRO81087_SNA80324 and DAI75645_GRO85051_*(395)\t\r\n",
      "WARNING: mismatch between DAI75645_GRO85051_SNA80324 and DAI75645_GRO94758_*(203)\t\r\n",
      "WARNING: mismatch between DAI75645_GRO94758_SNA80324 and DAI75645_SNA38068_*(124)\t\r\n",
      "(DAI75645, SNA45677) => SNA80324, 111, 245, 45.31%\t\r\n",
      "WARNING: mismatch between DAI75645_SNA55762_SNA80324 and DAI75645_SNA72163_*(131)\t\r\n",
      "(DAI85309, ELE17451) => ELE92920, 137, 482, 28.42%\t\r\n",
      "(DAI85309, ELE17451) => SNA18336, 119, 482, 24.69%\t\r\n",
      "(DAI85309, ELE92920) => SNA18336, 139, 201, 69.15%\t\r\n",
      "(DAI85309, ELE99737) => FRO19221, 111, 659, 16.84%\t\r\n",
      "(DAI85309, ELE99737) => GRO94758, 102, 659, 15.48%\t\r\n",
      "(DAI85309, ELE99737) => SNA45677, 105, 659, 15.93%\t\r\n",
      "(DAI88079, ELE17451) => FRO40251, 123, 124, 99.19%\t\r\n",
      "(DAI88079, FRO40251) => GRO73461, 144, 446, 32.29%\t\r\n",
      "WARNING: mismatch between DAI88079_FRO40251_SNA80324 and DAI88079_GRO73461_*(145)\t\r\n",
      "(DAI88807, ELE17451) => SNA59903, 120, 291, 41.24%\t\r\n",
      "(DAI88807, ELE17451) => SNA72163, 105, 291, 36.08%\t\r\n",
      "(DAI88807, GRO73461) => SNA72163, 110, 313, 35.14%\t\r\n",
      "WARNING: mismatch between DAI88807_SNA59903_SNA72163 and DAI88807_SNA72163_*(394)\t\r\n",
      "(ELE17451, ELE32164) => GRO59710, 202, 511, 39.53%\t\r\n",
      "(ELE17451, ELE92920) => SNA18336, 228, 384, 59.38%\t\r\n",
      "(ELE17451, FRO40251) => FRO92469, 162, 697, 23.24%\t\r\n",
      "(ELE17451, FRO40251) => GRO73461, 159, 697, 22.81%\t\r\n",
      "(ELE17451, FRO40251) => GRO85051, 217, 697, 31.13%\t\r\n",
      "(ELE17451, FRO40251) => SNA80324, 353, 697, 50.65%\t\r\n",
      "(ELE17451, GRO30386) => GRO73461, 103, 468, 22.01%\t\r\n",
      "WARNING: mismatch between ELE17451_GRO85051_SNA80324 and ELE17451_GRO88324_*(117)\t\r\n",
      "WARNING: mismatch between ELE17451_SNA59903_SNA72163 and ELE17451_SNA72163_*(272)\t\r\n",
      "(ELE20847, FRO40251) => FRO92469, 122, 434, 28.11%\t\r\n",
      "(ELE20847, FRO40251) => GRO85051, 139, 434, 32.03%\t\r\n",
      "(ELE20847, FRO40251) => SNA80324, 232, 434, 53.46%\t\r\n",
      "(ELE26917, FRO40251) => GRO85051, 146, 346, 42.20%\t\r\n",
      "(ELE26917, FRO40251) => SNA80324, 133, 346, 38.44%\t\r\n",
      "(ELE32164, FRO53271) => GRO59710, 105, 254, 41.34%\t\r\n",
      "(ELE32164, GRO59710) => GRO73461, 137, 911, 15.04%\t\r\n",
      "WARNING: mismatch between ELE59028_FRO85978_SNA93860 and ELE59935_FRO40251_*(134)\t\r\n",
      "WARNING: mismatch between ELE78169_GRO94758_SNA45677 and ELE91337_FRO35904_*(104)\t\r\n",
      "WARNING: mismatch between FRO19221_SNA53220_SNA93860 and FRO24098_FRO40251_*(106)\t\r\n",
      "(FRO40251, FRO53271) => GRO85051, 105, 309, 33.98%\t\r\n",
      "WARNING: mismatch between FRO40251_FRO53271_SNA80324 and FRO40251_FRO61354_*(126)\t\r\n",
      "(FRO40251, FRO80039) => SNA80324, 104, 249, 41.77%\t\r\n",
      "(FRO40251, FRO92469) => GRO73461, 211, 835, 25.27%\t\r\n",
      "WARNING: mismatch between FRO40251_FRO92469_SNA80324 and FRO40251_FRO98729_*(107)\t\r\n",
      "(FRO40251, GRO21487) => GRO73461, 182, 375, 48.53%\t\r\n",
      "(FRO40251, GRO21487) => GRO85051, 120, 375, 32.00%\t\r\n",
      "(FRO40251, GRO21487) => SNA80324, 118, 375, 31.47%\t\r\n",
      "(FRO40251, GRO30386) => SNA80324, 103, 224, 45.98%\t\r\n",
      "(FRO40251, GRO38814) => GRO73461, 106, 295, 35.93%\t\r\n",
      "(FRO40251, GRO38814) => GRO85051, 115, 295, 38.98%\t\r\n",
      "WARNING: mismatch between FRO40251_GRO38814_SNA80324 and FRO40251_GRO38983_*(153)\t\r\n",
      "(FRO40251, GRO46854) => GRO73461, 106, 210, 50.48%\t\r\n",
      "(FRO40251, GRO56726) => GRO73461, 103, 247, 41.70%\t\r\n",
      "(FRO40251, GRO69543) => GRO73461, 111, 227, 48.90%\t\r\n",
      "(FRO40251, GRO71621) => SNA80324, 142, 288, 49.31%\t\r\n",
      "(FRO40251, GRO73461) => GRO85051, 147, 882, 16.67%\t\r\n",
      "(FRO40251, GRO73461) => SNA80324, 232, 882, 26.30%\t\r\n",
      "WARNING: mismatch between FRO40251_GRO81087_SNA80324 and FRO40251_GRO85051_*(1213)\t\r\n",
      "(FRO40251, GRO85051) => SNA45677, 107, 1213, 8.82%\t\r\n",
      "WARNING: mismatch between FRO40251_GRO85051_SNA80324 and FRO40251_GRO94758_*(230)\t\r\n",
      "WARNING: mismatch between FRO40251_GRO94758_SNA80324 and FRO40251_GRO99222_*(142)\t\r\n",
      "(FRO40251, SNA45677) => SNA80324, 126, 309, 40.78%\t\r\n",
      "WARNING: mismatch between FRO40251_SNA55762_SNA80324 and FRO40251_SNA72163_*(201)\t\r\n",
      "WARNING: mismatch between FRO40251_SNA80324_SNA96271 and FRO40251_SNA90094_*(201)\t\r\n",
      "WARNING: mismatch between FRO73056_GRO44993_GRO73461 and FRO78087_FRO80039_*(116)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results3/part-0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.8*\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt).\n",
    "You can download pyFIM from here:\n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM)\n",
    "in terms of results and execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.8* (Conceptual Exercise)\n",
    "\n",
    "Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores\n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form:\n",
    "\n",
    "(item1, item2) ⇒ item3\n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore\n",
    "ONLY the NECESSARY subset of a large combinatorial space.\n",
    "Describe how you might design a framework to perform this exercise.\n",
    "\n",
    "In particular, focus on the following:\n",
    "  — map-reduce steps required\n",
    "  - enumeration of item sets and filtering for frequent candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping historyserver\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ stop historyserver "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
