{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Hamlin and Tigi Thomas  \n",
    "nickhamlin@gmail.com, tgthomas@berkeley.edu   \n",
    "Time of Submission: 9:23 PM EST, Wednesday, Feb 3, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 3 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submission Notes:\n",
    "- For each problem, we've included a summary of the question as posed in the instructions.  In many cases, we have not included the full text to keep the final submission as uncluttered as possible.  For reference, we've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Problem statements are listed in *italics*, while our responses are shown in plain text. \n",
    "- We've included the full output of the hadoop jobs in our responses so that counter results are shown.  However, these don't always render nicely into PDF form.  In these situations, please reference [the complete rendered notebook on Github](https://github.com/nickhamlin/mids_261_homework/blob/master/HW3/MIDS-W261-2015-HWK-Week03-Hamlin-Thomas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/uev2esasn7bvtnd/AAAlxSC1J3ZHCm7EgkJbemAZa/HW3-Questions.txt?dl=0)**\n",
    "- [Counter Example in mrjob](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/5thl14n4pqvhzt5/Counter.ipynb)\n",
    "\n",
    "###Handy Hadoop Links:\n",
    "- [Jobtracker](http://localhost:8088/cluster)  \n",
    "- [Namenode](http://localhost:50070/dfshealth.html#tab-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW3.0.  \n",
    "\n",
    "\n",
    "*What is a merge sort? Where is it used in Hadoop?* \n",
    "\n",
    "Merge sort is an algorithm that takes two sorted lists and merges them into a single sorted list.  It does this by tracking two pointers at the top of each list. In each iteration, the values of the two pointers are compared, and the smaller of the two is appended to the final list.  Therefore, as the algorithm iterates, the end result is a single sorted list. If the input to merge sort is an unsorted list, it must first be broken up into single elements, which essentially are (very small) sorted lists that can then proceed through the algorithm. This schematic from [interactivepython.org](http://interactivepython.org/runestone/static/pythonds/SortSearch/TheMergeSort.html) helps make the process clear.\n",
    "\n",
    "<img src=\"http://interactivepython.org/runestone/static/pythonds/_images/mergesortB.png\">\n",
    "\n",
    "Hadoop uses merge sort during the shuffle process to take the multiple spill files that are generated for each partion and merge them together before sending them to the combiner.  Merge sort is also used on the reducer side to combine files received from multiple mappers into a single stream.\n",
    "\n",
    "*How is a combiner function in the context of Hadoop? Give an example where it can be used and justify why it should be used in the context of this problem.*\n",
    "\n",
    "A combiner function is used to combine records with the same key before they are transferred to the reducer.  For example, in a word count implementation, if a mapper generates two key-value pairs for the same word, a combiner would combine those two records together into a single pair with the same key and a value representing the sum of the two input values.  In Hadoop, combiners can be used to reduce the amount of information that must be sent via the network from the mappers to the reducers.  In the previous word count example, if combiners were omitted, the mappers would have to send two pairs across the network.  But, by adding a combiner, only one pair per word needs to be sent.  This reduction in traffic can make a dramatic difference in the processing speed associated with a particular job, especially at large scale.  In addition, if a combiner is available, Hadoop may or may not decide to use it in the execution of a particular job, so mission-critical logic should not be included in them and should be saved for the mapper or reducer.\n",
    "\n",
    "*What is the Hadoop shuffle?* \n",
    "\n",
    "The shuffle is the process by which Hadoop sorts the output of the mappers and transfers it to the reducers.  It consists of three main steps: the partition, the sort, and the combine. The partition step takes the output of the mapper and partitions the results, by key, into separate files (one file for each reducer). In the sort step, records within the same partition are sorted.  Finally, the combine step takes records with the same key and merges them together.  Once these steps are completed, the output can then be transferred to the reducer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW3.1. Using Counters to do EDA\n",
    "\n",
    "*Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc.* \n",
    "\n",
    "*While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.*\n",
    "\n",
    "*The [consumer complaints](https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0) dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. \n",
    "\n",
    "###User-defined Counters\n",
    "\n",
    "*Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).*\n",
    "\n",
    "*Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 3.1 - Mapper and Reducer\n",
    "Here, we simply use the type of product that the mapper sees to iterate the appropriate counter.  After that, the reducer just passes the results through, since we're primarily interested in the counter outputs, not the data itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.1 - Mapper Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    product=line.split(',')[1] #extract product field from second field\n",
    "    \n",
    "    #Iterate the counter depending on the product\n",
    "    if product=='Debt collection':\n",
    "        sys.stderr.write(\"reporter:counter:Debt,Total,1\\n\")\n",
    "    if product=='Mortgage':\n",
    "        sys.stderr.write(\"reporter:counter:Mortgage,Total,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:Other,Total,1\\n\")\n",
    "    print product+'\\t1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.1 - Reducer Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:24:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:24:20 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_1_final_output\n"
     ]
    }
   ],
   "source": [
    "#Load the input data into HDFS and make sure the output directory is clear\n",
    "#!bin/hdfs dfs -put Consumer_Complaints.csv\n",
    "!bin/hdfs dfs -rm -r hw_3_1_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar761796270609264517/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob5110212568772663617.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:24:23 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:24:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:24:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:24:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:24:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:24:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:24:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0113\n",
      "16/02/03 00:24:25 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0113\n",
      "16/02/03 00:24:25 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0113/\n",
      "16/02/03 00:24:25 INFO mapreduce.Job: Running job: job_1454033924139_0113\n",
      "16/02/03 00:24:31 INFO mapreduce.Job: Job job_1454033924139_0113 running in uber mode : false\n",
      "16/02/03 00:24:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:24:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:24:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:24:47 INFO mapreduce.Job: Job job_1454033924139_0113 completed successfully\n",
      "16/02/03 00:24:47 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5504154\n",
      "\t\tFILE: Number of bytes written=11337353\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910816\n",
      "\t\tHDFS: Number of bytes written=4878322\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12223\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3517\n",
      "\t\tTotal time spent by all map tasks (ms)=12223\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3517\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12223\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3517\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12516352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3601408\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4878322\n",
      "\t\tMap output materialized bytes=5504160\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=5504160\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=312913\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=249\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=547356672\n",
      "\tDebt\n",
      "\t\tTotal=44372\n",
      "\tMortgage\n",
      "\t\tTotal=125752\n",
      "\tOther\n",
      "\t\tTotal=187161\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4878322\n",
      "16/02/03 00:24:47 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_1_final_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the job\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/Consumer_Complaints.csv \\\n",
    "-output /user/nicholashamlin/hw_3_1_final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the job results above, we can tell based on the counter results that there are **44372 debt records, 125752 mortgage records, and 187161 other records.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "###Part A\n",
    "*For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux*\n",
    "\n",
    "*Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a test file that we can use to test our code\n",
    "! echo \"foo foo quux labs foo bar quux\" > testfile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 3.2 Part A - Mapper and Reducer\n",
    "For this section, as well as the subsequent ones, we've included two counters.  One that increments when the file itself is called (\"script calls\") and one that increments when each file prints a line (\"line calls\").  While it's useful to be able to see the contrasts between the two, we're mainly interested in the script calls counter for the purposes of this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2 - Mapper Function Code\n",
    "import sys\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Script Calls,1\\n\") \n",
    "for line in sys.stdin:\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Line Calls,1\\n\")    \n",
    "    line=line.strip()\n",
    "    words=line.split()\n",
    "    for word in words:\n",
    "        print word+'\\t1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2 - Reducer Function Code\n",
    "import sys\n",
    "current_word=''\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "\n",
    "#Increment script call counter once when the file runs\n",
    "sys.stderr.write(\"reporter:counter:Reducer,Script Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    line=line.strip().split('\\t') #Parse line into a list of fields\n",
    "    word,sub_count=line\n",
    "    if current_word==word:\n",
    "        count+=int(sub_count) #Extract chunk count from the second field of each incoming line\n",
    "    else:\n",
    "        if current_word:\n",
    "            \n",
    "            #Increment line call counter whenever we emit a record\n",
    "            sys.stderr.write(\"reporter:counter:Reducer,Line Calls,1\\n\")\n",
    "            print current_word+'\\t'+str(count)\n",
    "        current_word=word\n",
    "        count=int(sub_count)\n",
    "\n",
    "#Make sure to emit final record and increment counter accordingly.\n",
    "if current_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Line Calls,1\\n\")\n",
    "    print current_word+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:25:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:25:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_2_a_output\n"
     ]
    }
   ],
   "source": [
    "#Load the input data into HDFS and make sure the output directory is clear\n",
    "#!bin/hdfs dfs -put testfile.txt\n",
    "!bin/hdfs dfs -rm -r hw_3_2_a_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar7165969867792931402/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob8875353377033464228.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:25:15 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:25:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:25:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:25:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:25:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:25:17 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 00:25:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:25:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:25:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0114\n",
      "16/02/03 00:25:17 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0114\n",
      "16/02/03 00:25:17 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0114/\n",
      "16/02/03 00:25:17 INFO mapreduce.Job: Running job: job_1454033924139_0114\n",
      "16/02/03 00:25:25 INFO mapreduce.Job: Job job_1454033924139_0114 running in uber mode : false\n",
      "16/02/03 00:25:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:25:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:25:41 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/02/03 00:25:44 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/02/03 00:25:45 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/02/03 00:25:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:25:46 INFO mapreduce.Job: Job job_1454033924139_0114 completed successfully\n",
      "16/02/03 00:25:46 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=548428\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=137\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3865\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31515\n",
      "\t\tTotal time spent by all map tasks (ms)=3865\n",
      "\t\tTotal time spent by all reduce tasks (ms)=31515\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3865\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=31515\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3957760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=32271360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=440\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=804257792\n",
      "\tMapper\n",
      "\t\tLine Calls=1\n",
      "\t\tScript Calls=1\n",
      "\tReducer\n",
      "\t\tLine Calls=4\n",
      "\t\tScript Calls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/02/03 00:25:46 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_2_a_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the job in Hadoop using 4 reducers!\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/testfile.txt -output /user/nicholashamlin/hw_3_2_a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, with a single mapper and four reducers, we are able to see the corresponding values counted in the Script Calls counters in the Hadoop output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part B\n",
    "*Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.2.C - Mapper Function Code\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Script Count,1\\n\") \n",
    "for line in reader(sys.stdin):\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\")    \n",
    "    try:\n",
    "        int(line[0]) #check if the ID field is an integer, skip the record if not\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    words = re.findall(WORD_RE, line[3])\n",
    "    for word in words:\n",
    "        print word.lower()+'\\t1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2 - Reducer Function Code\n",
    "import sys\n",
    "current_word=''\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Reducer,Script Count,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    line=line.strip().split('\\t') #Parse line into a list of fields\n",
    "    word,sub_count=line\n",
    "    if current_word==word:\n",
    "        count+=int(sub_count) #Extract chunk count from the second field of each incoming line\n",
    "    else:\n",
    "        if current_word:\n",
    "            sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "            print current_word+'\\t'+str(count)\n",
    "        current_word=word\n",
    "        count=int(sub_count)\n",
    "if current_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "    print current_word+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:25:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:25:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_2_b_output\n"
     ]
    }
   ],
   "source": [
    "#Make sure the output directory is clear\n",
    "!bin/hdfs dfs -rm -r hw_3_2_b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar2970440139442435838/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob4700736460910043168.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:26:01 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:26:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:26:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:26:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:26:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:26:03 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:26:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:26:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:26:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0115\n",
      "16/02/03 00:26:04 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0115\n",
      "16/02/03 00:26:04 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0115/\n",
      "16/02/03 00:26:04 INFO mapreduce.Job: Running job: job_1454033924139_0115\n",
      "16/02/03 00:26:11 INFO mapreduce.Job: Job job_1454033924139_0115 running in uber mode : false\n",
      "16/02/03 00:26:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:26:24 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/03 00:26:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:26:34 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/02/03 00:26:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:26:35 INFO mapreduce.Job: Job job_1454033924139_0115 completed successfully\n",
      "16/02/03 00:26:35 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16121335\n",
      "\t\tFILE: Number of bytes written=32681392\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910816\n",
      "\t\tHDFS: Number of bytes written=2174\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23485\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13320\n",
      "\t\tTotal time spent by all map tasks (ms)=23485\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13320\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=23485\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13320\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=24048640\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=13639680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348308\n",
      "\t\tMap output bytes=13424707\n",
      "\t\tMap output materialized bytes=16121347\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=16121347\n",
      "\t\tReduce input records=1348308\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=2696616\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=293\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=686817280\n",
      "\tMapper\n",
      "\t\tLine Count=312913\n",
      "\t\tScript Count=2\n",
      "\tReducer\n",
      "\t\tLine Count=174\n",
      "\t\tScript Count=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2174\n",
      "16/02/03 00:26:35 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_2_b_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the job in Hadoop, this time making sure to specify multiple mappers/reducers\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/Consumer_Complaints.csv \\\n",
    "-output /user/nicholashamlin/hw_3_2_b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 3.2 PART B RESULTS:\n",
      "10 First Results:\n",
      "16/02/03 00:27:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t3503\n",
      "account\t57448\n",
      "acct\t163\n",
      "an\t2964\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Examine the output of the job in HDFS and print some of the results to make sure it worked\n",
    "! echo \"HW 3.2 PART B RESULTS:\"\n",
    "! echo \"10 First Results:\"\n",
    "!bin/hdfs dfs -cat hw_3_2_b_output/* | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two cells above, we can see that the job has run successfully, and that, as we'd expect, the script call counters for both the mapper and reducer correspond to the number of tasks we chose for the job (2 each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part C\n",
    "*Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.*\n",
    "\n",
    "Here, we use exactly the same mapper and reducer as the previous job, but now we add a combiner.  This combiner uses the same logic as the reducer, and is only separated into its own file so we can use it to iterate on its own counter.  If we didn't care about this distinction, we could simply point the job to use the reducer as the combiner and the collective work would be tracked under the single counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.2.C - Mapper Function Code\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Script Count,1\\n\") \n",
    "for line in reader(sys.stdin):\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\")    \n",
    "    try:\n",
    "        int(line[0]) #check if the ID field is an integer, skip the record if not\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    words = re.findall(WORD_RE, line[3])\n",
    "    for word in words:\n",
    "        print word.lower()+'\\t1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2 - Combiner Function Code (same as reducer)\n",
    "import sys\n",
    "current_word=''\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Combiner,Script Count,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    line=line.strip().split('\\t') #Parse line into a list of fields\n",
    "    word,sub_count=line\n",
    "    if current_word==word:\n",
    "        count+=int(sub_count) #Extract chunk count from the second field of each incoming line\n",
    "    else:\n",
    "        if current_word:\n",
    "            sys.stderr.write(\"reporter:counter:Combiner,Line Count,1\\n\")\n",
    "            print current_word+'\\t'+str(count)\n",
    "        current_word=word\n",
    "        count=int(sub_count)\n",
    "if current_word:\n",
    "    sys.stderr.write(\"reporter:counter:Combiner,Line Count,1\\n\")\n",
    "    print current_word+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2 - Reducer Function Code\n",
    "import sys\n",
    "current_word=''\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Reducer,Script Count,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    line=line.strip().split('\\t') #Parse line into a list of fields\n",
    "    word,sub_count=line\n",
    "    if current_word==word:\n",
    "        count+=int(sub_count) #Extract chunk count from the second field of each incoming line\n",
    "    else:\n",
    "        if current_word:\n",
    "            sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "            print current_word+'\\t'+str(count)\n",
    "        current_word=word\n",
    "        count=int(sub_count)\n",
    "if current_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "    print current_word+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:27:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:27:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_2_c_output\n"
     ]
    }
   ],
   "source": [
    "#Make sure combiner is executable and the HDFS output directory is clear\n",
    "#!chmod +x ./combiner.py\n",
    "!bin/hdfs dfs -rm -r hw_3_2_c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, ./combiner.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar671604768103212361/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob2931200418503877332.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:27:38 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:27:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:27:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:27:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:27:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:27:40 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:27:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:27:40 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:27:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0116\n",
      "16/02/03 00:27:41 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0116\n",
      "16/02/03 00:27:41 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0116/\n",
      "16/02/03 00:27:41 INFO mapreduce.Job: Running job: job_1454033924139_0116\n",
      "16/02/03 00:27:48 INFO mapreduce.Job: Job job_1454033924139_0116 running in uber mode : false\n",
      "16/02/03 00:27:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:28:00 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/03 00:28:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:28:07 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/02/03 00:28:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:28:08 INFO mapreduce.Job: Job job_1454033924139_0116 completed successfully\n",
      "16/02/03 00:28:08 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4657\n",
      "\t\tFILE: Number of bytes written=450712\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910816\n",
      "\t\tHDFS: Number of bytes written=2174\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21251\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8062\n",
      "\t\tTotal time spent by all map tasks (ms)=21251\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8062\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=21251\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8062\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21761024\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8255488\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348308\n",
      "\t\tMap output bytes=13424707\n",
      "\t\tMap output materialized bytes=4669\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=1348308\n",
      "\t\tCombine output records=324\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=4669\n",
      "\t\tReduce input records=324\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=648\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=300\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=688390144\n",
      "\tCombiner\n",
      "\t\tLine Count=324\n",
      "\t\tScript Count=4\n",
      "\tMapper\n",
      "\t\tLine Count=312913\n",
      "\t\tScript Count=2\n",
      "\tReducer\n",
      "\t\tLine Count=174\n",
      "\t\tScript Count=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2174\n",
      "16/02/03 00:28:08 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_2_c_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the word count job in Hadoop with a single reducer\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-file ./combiner.py   -combiner ./combiner.py \\\n",
    "-input /user/nicholashamlin/Consumer_Complaints.csv \\\n",
    "-output /user/nicholashamlin/hw_3_2_c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 3.2 PART C RESULTS:\n",
      "10 First Results:\n",
      "16/02/03 00:30:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t3503\n",
      "account\t57448\n",
      "acct\t163\n",
      "an\t2964\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Examine the output of the job in HDFS and print the results\n",
    "! echo \"HW 3.2 PART C RESULTS:\"\n",
    "! echo \"10 First Results:\"\n",
    "!bin/hdfs dfs -cat hw_3_2_c_output/* | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, when we add the combiner, we see that it runs four times, in addition to the two map and reduce tasks.  Had we used the reducer as a combiner, we'd have seen 2 map tasks and 6 reduce tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "*Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).*\n",
    "\n",
    "#### HW 3.2 Part D - Mapper and Reducer\n",
    "This code works very similarly to the word count jobs we ran in the previous homework, with the addition of the counters.  Since we need to produce both raw frequencies and relative frequencies for each word, we need to use order inversion to make sure that the total word count is available to the reducer before it begins iterating through the individual words.  We can accomplish this by taking advantage of the Hadoop shuffle and assigning the total a special key that will be automatically sorted to the top before the data is passed to the reducer.  Thankfully, we only have one reducer, so no fancy partioning is necessary to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.2.D - Mapper Function Code\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "total_words=0\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Script Count,1\\n\") \n",
    "for line in reader(sys.stdin):\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\")    \n",
    "    try:\n",
    "        int(line[0]) #check if the ID field is an integer, skip the record if not\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    words = re.findall(WORD_RE, line[3])\n",
    "    \n",
    "    for word in words:\n",
    "        print word.lower()+'\\t1' #emit one record for each word\n",
    "        total_words+=1\n",
    "        \n",
    "#emit overall total with special key for order inversion\n",
    "print '**Total\\t'+str(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.2.D - Reducer Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "current_word=''\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "number_of_words=0\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer,Script Count,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    line=line.strip().split('\\t') #Parse line into a list of fields\n",
    "    word,sub_count=line\n",
    "    sub_count=int(sub_count)\n",
    "    \n",
    "    #Extract total number of words from first record in the stream (thanks order inversion!)\n",
    "    if word=='**Total':\n",
    "        number_of_words=sub_count\n",
    "        continue\n",
    "    \n",
    "    if current_word==word:\n",
    "        count+=sub_count #Extract chunk count from the second field of each incoming line\n",
    "    else:\n",
    "        if current_word:\n",
    "            sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "            print current_word+'\\t'+str(count)+'\\t'+str(count/number_of_words)\n",
    "        current_word=word\n",
    "        count=int(sub_count)\n",
    "\n",
    "#Don't forget to emit final record\n",
    "if current_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "    print current_word+'\\t'+str(count)+'\\t'+str(count/number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:30:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_2_d_tmp_output\n"
     ]
    }
   ],
   "source": [
    "#Load the input data into HDFS and make sure the output directory is clear\n",
    "#!bin/hdfs dfs -put testfile.txt\n",
    "!bin/hdfs dfs -rm -r hw_3_2_d_tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar3877860112520264432/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob6006346864795557354.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:30:41 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:30:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:30:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:30:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:30:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:30:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:30:44 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:30:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:30:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0117\n",
      "16/02/03 00:30:44 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0117\n",
      "16/02/03 00:30:44 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0117/\n",
      "16/02/03 00:30:44 INFO mapreduce.Job: Running job: job_1454033924139_0117\n",
      "16/02/03 00:30:50 INFO mapreduce.Job: Job job_1454033924139_0117 running in uber mode : false\n",
      "16/02/03 00:30:50 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:31:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:31:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:31:09 INFO mapreduce.Job: Job job_1454033924139_0117 completed successfully\n",
      "16/02/03 00:31:10 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16121363\n",
      "\t\tFILE: Number of bytes written=32571771\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910816\n",
      "\t\tHDFS: Number of bytes written=5095\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18008\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5460\n",
      "\t\tTotal time spent by all map tasks (ms)=18008\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5460\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18008\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5460\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18440192\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5591040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348310\n",
      "\t\tMap output bytes=13424737\n",
      "\t\tMap output materialized bytes=16121369\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=175\n",
      "\t\tReduce shuffle bytes=16121369\n",
      "\t\tReduce input records=1348310\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=2696620\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=213\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=539492352\n",
      "\tMapper\n",
      "\t\tLine Count=312913\n",
      "\t\tScript Count=2\n",
      "\tReducer\n",
      "\t\tLine Count=174\n",
      "\t\tScript Count=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5095\n",
      "16/02/03 00:31:10 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_2_d_tmp_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the word count job in Hadoop with a single reducer\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/Consumer_Complaints.csv -output /user/nicholashamlin/hw_3_2_d_tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 3.2.D - Sorting via the Hadoop Shuffle using identity mapper/reducers\n",
    "We can use the secondary sort functionality available in Hadoop to handle the logic associated with sorting our results.  As such, we just need an identity mapper and reducer to pass the records through the shuffle.  Since we'll be using this function frequently through this assignment, we'll define it once here and recycle it for subsequent sorting tasks that use this same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting identity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile identity.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.X - Identity Mapper/Reducer Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make sure it's executable!\n",
    "!chmod +x identity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:32:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:32:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_2_d_final_output\n"
     ]
    }
   ],
   "source": [
    "#Load the input data into HDFS and make sure the output directory is clear\n",
    "#!bin/hdfs dfs -put testfile.txt\n",
    "!bin/hdfs dfs -rm -r hw_3_2_d_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./identity.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar4836867262158006418/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob4474839850733033311.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:32:09 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:32:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:32:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:32:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:32:11 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:32:11 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:32:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:32:11 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/03 00:32:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:32:11 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/03 00:32:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0118\n",
      "16/02/03 00:32:11 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0118\n",
      "16/02/03 00:32:11 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0118/\n",
      "16/02/03 00:32:11 INFO mapreduce.Job: Running job: job_1454033924139_0118\n",
      "16/02/03 00:32:17 INFO mapreduce.Job: Job job_1454033924139_0118 running in uber mode : false\n",
      "16/02/03 00:32:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:32:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:32:28 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:32:28 INFO mapreduce.Job: Job job_1454033924139_0118 completed successfully\n",
      "16/02/03 00:32:29 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5449\n",
      "\t\tFILE: Number of bytes written=340318\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7891\n",
      "\t\tHDFS: Number of bytes written=5095\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8117\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2790\n",
      "\t\tTotal time spent by all map tasks (ms)=8117\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2790\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8117\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2790\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8311808\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2856960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=174\n",
      "\t\tMap output records=174\n",
      "\t\tMap output bytes=5095\n",
      "\t\tMap output materialized bytes=5455\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=5455\n",
      "\t\tReduce input records=174\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=348\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=207\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=553648128\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7643\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5095\n",
      "16/02/03 00:32:29 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_2_d_final_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the sorting job using the output of the previous data in Hadoop with a single reducer\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options='-k2,2nr' \\\n",
    "-file ./identity.py    -mapper ./identity.py \\\n",
    " -reducer ./identity.py \\\n",
    "-input /user/nicholashamlin/hw_3_2_d_tmp_output -output /user/nicholashamlin/hw_3_2_d_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 3.2 PART D RESULTS:\n",
      "50 Most Common Words:\n",
      "Word | Frequency | Relative Frequency\n",
      "16/02/03 00:33:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "loan\t119630\t0.179490411074\n",
      "collection\t72394\t0.108618480476\n",
      "foreclosure\t70487\t0.105757256586\n",
      "modification\t70487\t0.105757256586\n",
      "account\t57448\t0.0861938070332\n",
      "credit\t55251\t0.0828974730607\n",
      "or\t40508\t0.0607773766763\n",
      "payments\t39993\t0.0600046811843\n",
      "servicing\t36767\t0.0551644566075\n",
      "escrow\t36767\t0.0551644566075\n",
      "report\t34903\t0.0523677490405\n",
      "incorrect\t29133\t0.0437105587714\n",
      "information\t29069\t0.0436145344772\n",
      "on\t29069\t0.0436145344772\n",
      "debt\t27874\t0.04182158086\n",
      "closing\t19000\t0.0285072123247\n",
      "not\t18477\t0.027722513796\n",
      "owed\t17972\t0.0269648221\n",
      "cont'd\t17972\t0.0269648221\n",
      "attempts\t17972\t0.0269648221\n",
      "collect\t17972\t0.0269648221\n",
      "and\t16448\t0.0246782435956\n",
      "opening\t16205\t0.0243136513538\n",
      "management\t16205\t0.0243136513538\n",
      "of\t13983\t0.0209798078914\n",
      "my\t10731\t0.0161005734451\n",
      "withdrawals\t10555\t0.0158365066362\n",
      "deposits\t10555\t0.0158365066362\n",
      "problems\t9484\t0.0142296000888\n",
      "application\t8868\t0.0133053662577\n",
      "communication\t8671\t0.0130097914772\n",
      "tactics\t8671\t0.0130097914772\n",
      "mortgage\t8625\t0.0129407740158\n",
      "originator\t8625\t0.0129407740158\n",
      "broker\t8625\t0.0129407740158\n",
      "to\t8401\t0.0126046889863\n",
      "unable\t8178\t0.0122701043364\n",
      "billing\t8158\t0.0122400967445\n",
      "other\t7886\t0.0118319934944\n",
      "verification\t7655\t0.0114854058077\n",
      "disclosure\t7655\t0.0114854058077\n",
      "disputes\t6938\t0.0104096336373\n",
      "reporting\t6559\t0.00984098977041\n",
      "lease\t6337\t0.00950790550009\n",
      "the\t6248\t0.00937437171604\n",
      "low\t5663\t0.00849664965236\n",
      "by\t5663\t0.00849664965236\n",
      "being\t5663\t0.00849664965236\n",
      "funds\t5663\t0.00849664965236\n",
      "caused\t5663\t0.00849664965236\n",
      "===================================\n",
      "10 Least Common Words:\n",
      "Word | Frequency | Relative Frequency\n",
      "16/02/03 00:33:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "apply\t118\t0.000177044792332\n",
      "amount\t98\t0.000147037200412\n",
      "credited\t92\t0.000138034922835\n",
      "payment\t92\t0.000138034922835\n",
      "convenience\t75\t0.000112528469703\n",
      "checks\t75\t0.000112528469703\n",
      "amt\t71\t0.000106526951319\n",
      "day\t71\t0.000106526951319\n",
      "disclosures\t64\t9.60242941464e-05\n",
      "missing\t64\t9.60242941464e-05\n"
     ]
    }
   ],
   "source": [
    "# Examine the output of the job in HDFS and print the results\n",
    "! echo \"HW 3.2 PART D RESULTS:\"\n",
    "! echo \"50 Most Common Words:\"\n",
    "! echo \"Word | Frequency | Relative Frequency\"\n",
    "!bin/hdfs dfs -cat hw_3_2_d_final_output/* | head -50\n",
    "! echo \"===================================\"\n",
    "! echo \"10 Least Common Words:\"\n",
    "! echo \"Word | Frequency | Relative Frequency\"\n",
    "!bin/hdfs dfs -cat hw_3_2_d_final_output/* | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the [online browsing behavior dataset](https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0)\n",
    "\n",
    "      \n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. The items are separated by spaces.\n",
    "\n",
    "Do some exploratory data analysis of this dataset. How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.3 Mapper and Reducer #1\n",
    "The first mapreduce job does the majority of the work.  The mapper emits one space-delimited row with the format {product_id cart_id 1 number_of_products_in_cart] where cart_id is a auto incremented identifier for the row to make the largest cart easy to find at the end.  By passing the number of products in the cart along with each product, we can track the largest cart in the reducer as we iterate through all the mapper output.  In addition, we use an order-inversion pattern to emit the total number of all products seen as the first record(s) that the reducer sees.  This enables lazy calculation of product relative frequencies without the need for storing many intermediate values in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.3 - Mapper Function Code\n",
    "import sys\n",
    "count = 0 #Running total of occurrances for the chosen product\n",
    "product_count=0\n",
    "cart_id=1 #The carts don't have IDs of their own, but we can make our own\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    products=line.split() #split on whitespace\n",
    "\n",
    "    for product in products:\n",
    "        product_count+=1\n",
    "        print product+' '+str(cart_id)+' 1 '+str(len(products)) #emit one row per product per cart\n",
    "    cart_id+=1\n",
    "\n",
    "#Emit total with special key for order inversion\n",
    "print '**Total '+'0'+' '+str(product_count)+' 0' #emit total number of products for order inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.3 - Reducer Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "current_product=None\n",
    "count = 0 #Running total of occurrances for the chosen product\n",
    "largest_basket_id=0\n",
    "largest_basket_size=0\n",
    "unique_products=0\n",
    "total_product_count=0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    #Parse line into fields\n",
    "    product,cart_id,sub_count,cart_total=line.strip().split(' ')\n",
    "    sub_count=int(sub_count)\n",
    "    cart_total=int(cart_total)\n",
    "    \n",
    "    if product=='**Total': #Extract total products for order inversion\n",
    "        total_product_count+=sub_count\n",
    "        continue\n",
    "        \n",
    "    #If we find a cart that's bigger than any we've seen so far, record it\n",
    "    if cart_total>largest_basket_size: \n",
    "        largest_basket_size=cart_total\n",
    "        largest_basket_id=cart_id\n",
    "  \n",
    "    if current_product==product:\n",
    "        count+=int(sub_count)\n",
    "    else:\n",
    "        if current_product and current_product!='**Total':\n",
    "            print current_product+'\\t'+str(count)+'\\t'+str(count/total_product_count)\n",
    "            unique_products+=1\n",
    "        current_product=product\n",
    "        count=int(sub_count)\n",
    "if current_product:\n",
    "    print current_product+'\\t'+str(count)+'\\t'+str(count/total_product_count)\n",
    "    unique_products+=1\n",
    "    \n",
    "#Print aggregated stats separately with special key to make them easy to find\n",
    "print '*Largest Cart\\t'+str(largest_basket_id)+'\\t'+str(largest_basket_size)\n",
    "print '*Unique Products'+'\\t'+str(unique_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.3 - Running the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:33:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:33:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_3_tmp_output\n"
     ]
    }
   ],
   "source": [
    "### Make sure data is available and 1st job output directory is clear in HDFS\n",
    "#!bin/hdfs dfs -put ProductPurchaseData.txt\n",
    "!bin/hdfs dfs -rm -r hw_3_3_tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar2177335998639084698/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob572022290791535742.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:33:21 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:33:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:33:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:33:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:33:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:33:23 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:33:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:33:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:33:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0119\n",
      "16/02/03 00:33:24 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0119\n",
      "16/02/03 00:33:24 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0119/\n",
      "16/02/03 00:33:24 INFO mapreduce.Job: Running job: job_1454033924139_0119\n",
      "16/02/03 00:33:32 INFO mapreduce.Job: Job job_1454033924139_0119 running in uber mode : false\n",
      "16/02/03 00:33:32 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:33:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:33:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:33:46 INFO mapreduce.Job: Job job_1454033924139_0119 completed successfully\n",
      "16/02/03 00:33:46 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8406183\n",
      "\t\tFILE: Number of bytes written=17141402\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462847\n",
      "\t\tHDFS: Number of bytes written=368680\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10614\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3972\n",
      "\t\tTotal time spent by all map tasks (ms)=10614\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3972\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10614\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3972\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10868736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4067328\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=7644525\n",
      "\t\tMap output materialized bytes=8406189\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=380577\n",
      "\t\tReduce shuffle bytes=8406189\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=12594\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=173\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=555220992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368680\n",
      "16/02/03 00:33:46 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_3_tmp_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the word count job in Hadoop with a single reducer, as per the instructions\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/ProductPurchaseData.txt -output /user/nicholashamlin/hw_3_3_tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.3 Mapper and Reducer #2\n",
    "As in previous problems, we run a second job focused solely on sorting the output of the first job. We can use the identity mapper/reducer from earlier and tackle the entire sort via the shuffle using secondary sort keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:34:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:35:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_3_final_output\n"
     ]
    }
   ],
   "source": [
    "#Make sure the destination directory for the second job is clear\n",
    "!bin/hdfs dfs -rm -r hw_3_3_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./identity.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar918449771401492720/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob249352555378736292.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:35:02 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:35:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:35:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:35:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:35:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:35:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:35:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:35:04 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/03 00:35:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:35:04 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/03 00:35:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0120\n",
      "16/02/03 00:35:05 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0120\n",
      "16/02/03 00:35:05 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0120/\n",
      "16/02/03 00:35:05 INFO mapreduce.Job: Running job: job_1454033924139_0120\n",
      "16/02/03 00:35:11 INFO mapreduce.Job: Job job_1454033924139_0120 running in uber mode : false\n",
      "16/02/03 00:35:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:35:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:35:25 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:35:25 INFO mapreduce.Job: Job job_1454033924139_0120 completed successfully\n",
      "16/02/03 00:35:25 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=393875\n",
      "\t\tFILE: Number of bytes written=1117173\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=373020\n",
      "\t\tHDFS: Number of bytes written=368680\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11308\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4105\n",
      "\t\tTotal time spent by all map tasks (ms)=11308\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4105\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11308\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4105\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11579392\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4203520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12594\n",
      "\t\tMap output records=12594\n",
      "\t\tMap output bytes=368681\n",
      "\t\tMap output materialized bytes=393881\n",
      "\t\tInput split bytes=244\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=393881\n",
      "\t\tReduce input records=12594\n",
      "\t\tReduce output records=12594\n",
      "\t\tSpilled Records=25188\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=261\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=550502400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=372776\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368680\n",
      "16/02/03 00:35:25 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_3_final_output\n",
      "\n",
      "real\t0m23.590s\n",
      "user\t0m5.507s\n",
      "sys\t0m0.396s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Sort the results in the shuffle, using identity mapper/reducers\n",
    "time bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options='-k2,2nr -k1,1' \\\n",
    "-file ./identity.py    -mapper ./identity.py \\\n",
    "-reducer ./identity.py \\\n",
    "-input /user/nicholashamlin/hw_3_3_tmp_output -output /user/nicholashamlin/hw_3_3_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 3.3 RESULTS:\n",
      "\n",
      "50 Most Frequent Products (Summary Stats in first two rows):\n",
      "Product ID | Raw Frequency | Relative Frequency\n",
      "16/02/03 00:36:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "*Unique Products\t12592\n",
      "*Largest Cart\t6914\t37\n",
      "DAI62779\t6667\t0.0175067747831\n",
      "FRO40251\t3881\t0.010191059387\n",
      "ELE17451\t3875\t0.0101753040775\n",
      "GRO73461\t3602\t0.00945843749344\n",
      "SNA80324\t3044\t0.00799319370628\n",
      "ELE32164\t2851\t0.0074863979161\n",
      "DAI75645\t2736\t0.00718442114993\n",
      "SNA45677\t2455\t0.0064465474865\n",
      "FRO31317\t2330\t0.0061183118711\n",
      "DAI85309\t2293\t0.00602115412894\n",
      "ELE26917\t2292\t0.00601852824402\n",
      "FRO80039\t2233\t0.00586360103355\n",
      "GRO21487\t2115\t0.00555374661261\n",
      "SNA99873\t2083\t0.00546971829507\n",
      "GRO59710\t2004\t0.00526227338613\n",
      "GRO71621\t1920\t0.00504169905258\n",
      "FRO85978\t1918\t0.00503644728273\n",
      "GRO30386\t1840\t0.00483162825872\n",
      "ELE74009\t1816\t0.00476860702057\n",
      "GRO56726\t1784\t0.00468457870302\n",
      "DAI63921\t1773\t0.00465569396887\n",
      "GRO46854\t1756\t0.00461105392517\n",
      "ELE66600\t1713\t0.00449814087347\n",
      "DAI83733\t1712\t0.00449551498855\n",
      "FRO32293\t1702\t0.00446925613932\n",
      "ELE66810\t1697\t0.0044561267147\n",
      "SNA55762\t1646\t0.00432220658362\n",
      "DAI22177\t1627\t0.00427231477008\n",
      "FRO78087\t1531\t0.00402022981745\n",
      "ELE99737\t1516\t0.0039808415436\n",
      "ELE34057\t1489\t0.00390994265067\n",
      "GRO94758\t1489\t0.00390994265067\n",
      "FRO35904\t1436\t0.00377077074974\n",
      "FRO53271\t1420\t0.00372875659097\n",
      "SNA93860\t1407\t0.00369462008697\n",
      "SNA90094\t1390\t0.00364998004327\n",
      "GRO38814\t1352\t0.00355019641619\n",
      "ELE56788\t1345\t0.00353181522173\n",
      "GRO61133\t1321\t0.00346879398357\n",
      "DAI88807\t1316\t0.00345566455896\n",
      "ELE74482\t1316\t0.00345566455896\n",
      "ELE59935\t1311\t0.00344253513434\n",
      "SNA96271\t1295\t0.00340052097557\n",
      "DAI43223\t1290\t0.00338739155095\n",
      "ELE91337\t1289\t0.00338476566603\n",
      "GRO15017\t1275\t0.0033480032771\n",
      "DAI31081\t1261\t0.00331124088818\n",
      "GRO81087\t1220\t0.00320357960633\n",
      "DAI22896\t1219\t0.0032009537214\n",
      "GRO85051\t1214\t0.00318782429679\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Examine the output of the job in HDFS and print the results\n",
    "! echo \"HW 3.3 RESULTS:\"\n",
    "! echo \"\"\n",
    "! echo \"50 Most Frequent Products (Summary Stats in first two rows):\"\n",
    "! echo \"Product ID | Raw Frequency | Relative Frequency\"\n",
    "!bin/hdfs dfs -cat hw_3_3_final_output/* | head -52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on these results, we have 12592 unique products browsed.  The largest session covered 37 products, and the most commonly browsed product was DAI62779, which was seen 6667 times for a relative frequency of 0.0175.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW3.4. Pairs\n",
    "\n",
    "*Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 (i.e. product pairs need to occur together at least 100 times to be considered frequent) and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.*\n",
    "\n",
    "*List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2.*\n",
    "\n",
    "*Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.*\n",
    "\n",
    "*Please output records of the following form for the top 50 pairs (itemsets of size 2):* \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "*Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order.* \n",
    "\n",
    "*Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 3.4 - Mapper and Reducer #1\n",
    "The first job does most of the work here.  Each cart is broken into a lists of products.  As we iterate through the list, a pair is emitted for each combination of the current product and every subsequent product that follows.  This ensures that we don't double count pairs.  Similarly, pairs are sorted lexicographically before being sent to the reducer.  As before, we use order inversion to ensure that the reducers can access the overal total and compute relative frequency efficiently. \n",
    "\n",
    "In the reducer, we take the same approach as we have with past word counts, with the key difference being that two pieces of information are compared (both products) to the current state of each iteration.  If the incoming pair matches the present pair, we increment our totals.  If not, we emit the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.4 - Mapper Function Code\n",
    "import sys\n",
    "number_of_carts=0\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Script Count,1\\n\") \n",
    "\n",
    "#Define data split for custom partitioner\n",
    "group1 = \"abcdefghijklm\"\n",
    "group2 = \"nopqrstuvwxyz\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    products=line.split() #split on whitespace\n",
    "\n",
    "    for i,product in enumerate(products):\n",
    "        product_a=product\n",
    "        \n",
    "        #Only iterate through products we haven't seen yet to avoid duplicates\n",
    "        for product_b in products[i+1:]:\n",
    "            \n",
    "            #Sort output in alphabetical order\n",
    "            output=sorted([product_a,product_b])\n",
    "            sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\")\n",
    "            \n",
    "            #Emit one pair for every result, but with extra key for custom partitioner\n",
    "            if product_a[0].lower() in group1:\n",
    "                print output[0]+'\\t'+output[1]+'\\t1\\t1'\n",
    "            else:\n",
    "                print output[0]+'\\t'+output[1]+'\\t1\\t2'\n",
    "    number_of_carts+=1\n",
    "\n",
    "#Output total number of carts with a special key for order-inversion purposes\n",
    "#Two versions will enable one record for each reducer. \n",
    "print '**Total'+'\\t'+'**Total'+'\\t'+str(number_of_carts)+'\\t1'\n",
    "print '**Total'+'\\t'+'**Total'+'\\t'+str(number_of_carts)+'\\t2'\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.4 - Reducer Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer,Script Count,1\\n\") \n",
    "\n",
    "s=100 #cutoff for \"frequent\"\n",
    "\n",
    "#We want to track two products at a time as they come in from the mapper\n",
    "current_product_a=None \n",
    "current_product_b=None\n",
    "count = 0 #Running total of occurrances for the chosen product\n",
    "\n",
    "number_of_carts=0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    #Parse line into fields\n",
    "    product_a,product_b,support=line.strip().split('\\t')[0:3]\n",
    "    support=int(support)\n",
    "    \n",
    "    #Extract total products for order inversion\n",
    "    if product_a=='**Total' and product_b=='**Total': \n",
    "        number_of_carts+=support\n",
    "        continue\n",
    "    \n",
    "    #Only increment counter if both products match\n",
    "    if current_product_a==product_a and current_product_b==product_b:\n",
    "        count+=support\n",
    "    else:\n",
    "        if current_product_a and current_product_b and current_product_a!='**Total':\n",
    "            if count>=s: #only emit records with at least 100 pairs\n",
    "                print current_product_a+'\\t'+current_product_b+'\\t'+str(count)+'\\t'+str(count/number_of_carts)\n",
    "                sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\") \n",
    "        current_product_a=product_a\n",
    "        current_product_b=product_b\n",
    "        count=support\n",
    "        \n",
    "#Make sure to emit final result as well\n",
    "if current_product_a and current_product_b and current_product_a!='**Total':\n",
    "    if count>=s:\n",
    "        print current_product_a+'\\t'+current_product_b+'\\t'+str(count)+'\\t'+str(count/number_of_carts)\n",
    "        sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 3.4 - Running the jobs\n",
    "The first job is the one we're interested in tracking, since it's where we've implemented the pairs approach.  To monitor how long it takes, we use the simple `time` command available in bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:36:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:36:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_4_tmp_output\n"
     ]
    }
   ],
   "source": [
    "### Make sure 1st job output directory is clear in HDFS\n",
    "#!bin/hdfs dfs -put purchase_test.txt\n",
    "!bin/hdfs dfs -rm -r hw_3_4_tmp_output\n",
    "#!bin/hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar122817028257504486/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob3472187373271767699.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:36:30 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:36:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:36:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:36:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:36:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:36:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:36:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:36:32 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/03 00:36:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:36:32 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/03 00:36:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0121\n",
      "16/02/03 00:36:32 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0121\n",
      "16/02/03 00:36:32 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0121/\n",
      "16/02/03 00:36:32 INFO mapreduce.Job: Running job: job_1454033924139_0121\n",
      "16/02/03 00:36:38 INFO mapreduce.Job: Job job_1454033924139_0121 running in uber mode : false\n",
      "16/02/03 00:36:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:36:50 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/02/03 00:36:53 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/03 00:36:57 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/02/03 00:36:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:37:08 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/02/03 00:37:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:37:09 INFO mapreduce.Job: Job job_1454033924139_0121 completed successfully\n",
      "16/02/03 00:37:09 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=63351545\n",
      "\t\tFILE: Number of bytes written=127144360\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462847\n",
      "\t\tHDFS: Number of bytes written=45435\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=34697\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16911\n",
      "\t\tTotal time spent by all map tasks (ms)=34697\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16911\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=34697\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16911\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35529728\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17316864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534061\n",
      "\t\tMap output bytes=58283411\n",
      "\t\tMap output materialized bytes=63351557\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=935090\n",
      "\t\tReduce shuffle bytes=63351557\n",
      "\t\tReduce input records=2534061\n",
      "\t\tReduce output records=1171\n",
      "\t\tSpilled Records=5068122\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=733\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=709361664\n",
      "\tMapper\n",
      "\t\tLine Count=2534059\n",
      "\t\tScript Count=2\n",
      "\tReducer\n",
      "\t\tLine Count=1171\n",
      "\t\tScript Count=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=45435\n",
      "16/02/03 00:37:09 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_4_tmp_output\n",
      "\n",
      "real\t0m40.373s\n",
      "user\t0m5.450s\n",
      "sys\t0m0.407s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the first job in Hadoop, making sure to time the results\n",
    "\n",
    "time bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapreduce.partition.keypartitioner.options='-k1,2 -k4,4' \\\n",
    "-D stream.num.map.output.key.fields=4 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options='-k1,1 -k2,2' \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/ProductPurchaseData.txt -output /user/nicholashamlin/hw_3_4_tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 3.4 - Mapper and Reducer #2\n",
    "This second job uses the same identity mapper and reducer from earlier to feed records throught the shuffle, which takes care of the sorting we need.  We need to wait until the first job is completed to do this sort, otherwise we won't have calculated the overall totals for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:42:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:42:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_4_final_output\n"
     ]
    }
   ],
   "source": [
    "#Make sure final output directory is clear\n",
    "!bin/hdfs dfs -rm -r hw_3_4_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./identity.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar7188213226497342987/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob7565968427572811265.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:42:35 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:42:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:42:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:42:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:42:37 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/02/03 00:42:37 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:42:37 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/03 00:42:37 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/03 00:42:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0123\n",
      "16/02/03 00:42:38 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0123\n",
      "16/02/03 00:42:38 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0123/\n",
      "16/02/03 00:42:38 INFO mapreduce.Job: Running job: job_1454033924139_0123\n",
      "16/02/03 00:42:44 INFO mapreduce.Job: Job job_1454033924139_0123 running in uber mode : false\n",
      "16/02/03 00:42:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:42:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:42:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:42:55 INFO mapreduce.Job: Job job_1454033924139_0123 completed successfully\n",
      "16/02/03 00:42:55 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=47783\n",
      "\t\tFILE: Number of bytes written=425022\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=45679\n",
      "\t\tHDFS: Number of bytes written=45435\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8074\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2362\n",
      "\t\tTotal time spent by all map tasks (ms)=8074\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2362\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8074\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2362\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8267776\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2418688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1171\n",
      "\t\tMap output records=1171\n",
      "\t\tMap output bytes=45435\n",
      "\t\tMap output materialized bytes=47789\n",
      "\t\tInput split bytes=244\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1170\n",
      "\t\tReduce shuffle bytes=47789\n",
      "\t\tReduce input records=1171\n",
      "\t\tReduce output records=1171\n",
      "\t\tSpilled Records=2342\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=185\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45435\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=45435\n",
      "16/02/03 00:42:55 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_4_final_output\n",
      "\n",
      "real\t0m20.775s\n",
      "user\t0m5.137s\n",
      "sys\t0m0.365s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the sorting job using the output of the previous data in Hadoop with a single reducer\n",
    "time bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options='-k 3,3nr -k 1,1 -k 2,2' \\\n",
    "-file ./identity.py    -mapper ./identity.py \\\n",
    "-reducer ./identity.py \\\n",
    "-input /user/nicholashamlin/hw_3_4_tmp_output -output /user/nicholashamlin/hw_3_4_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 3.4 RESULTS:\n",
      "50 Most Frequent Pairs:\n",
      "Product 1   |   Product 2 | Raw Freq. | Support \n",
      "16/02/03 00:43:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779\tELE17451\t1592\t0.0511880646925\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\n",
      "FRO40251\tSNA80324\t963\t0.0309636346098\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\n",
      "DAI43223\tELE32164\t711\t0.022861001254\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\n",
      "DAI62779\tSNA80324\t662\t0.0212854892126\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\n",
      "DAI62779\tELE26917\t650\t0.020899649529\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\n",
      "DAI75645\tSNA80324\t589\t0.0189382978039\n",
      "DAI62779\tDAI83733\t586\t0.018841837883\n",
      "ELE17451\tGRO73461\t580\t0.0186489180412\n",
      "DAI62779\tGRO59710\t561\t0.0180380052088\n",
      "DAI62779\tFRO80039\t550\t0.0176843188322\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\n",
      "DAI75645\tSNA80324\t541\t0.0173949390695\n",
      "GRO73461\tSNA80324\t539\t0.0173306324555\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\n",
      "ELE17451\tELE32164\t511\t0.0164303398605\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\n",
      "DAI62779\tFRO78087\t482\t0.0154978939584\n",
      "DAI85309\tELE17451\t482\t0.0154978939584\n",
      "DAI62779\tGRO94758\t479\t0.0154014340375\n",
      "DAI62779\tGRO21487\t471\t0.0151442075817\n",
      "ELE17451\tGRO30386\t468\t0.0150477476608\n",
      "DAI62779\tFRO19221\t462\t0.014854827819\n",
      "DAI62779\tSNA18336\t462\t0.014854827819\n",
      "DAI62779\tGRO46854\t461\t0.0148226745121\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\n",
      "FRO85978\tSNA95666\t459\t0.0147583678981\n",
      "ELE92920\tSNA18336\t451\t0.0145011414424\n",
      "FRO40251\tSNA80324\t449\t0.0144368348285\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\n",
      "FRO73056\tGRO44993\t438\t0.0140831484518\n",
      "DAI62779\tFRO85978\t434\t0.0139545352239\n",
      "ELE20847\tFRO40251\t434\t0.0139545352239\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Examine the output of the job in HDFS and print the results\n",
    "! echo \"HW 3.4 RESULTS:\"\n",
    "! echo \"50 Most Frequent Pairs:\"\n",
    "! echo \"Product 1   |   Product 2 | Raw Freq. | Support \"\n",
    "!bin/hdfs dfs -cat hw_3_4_final_output/* | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 3.4 - Discussion\n",
    "We ran these jobs on a dual-core Macbook Pro running a local install of Hadoop 2.6.3 with 2 mappers and 2 reducers.  **The main pairs job ran in 40.3 seconds** (as shown above in the output stream) and called the mapper and reducer function twice each, as we'd expect.  The second simple sort job using an identity mapper/reducer ran on the default 2 mappers/1 reducer configuration in about 20 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW3.5. Stripes\n",
    "*Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.*\n",
    "\n",
    "*Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)*\n",
    "\n",
    "*Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.5 Mapper and Reducer #1\n",
    "For the stripes implementation, the mapper emits an associative array for each product that contains the list of products paired with it.  In the reducer, these arrays are unpacked and summed to product the final total for each pair of products.  We implement this simply in python using the Counter object, which enables us to increment the marginal product counts using similar syntax to the pairs approach and without needing to worry about locating and incrementing each individual value for each individual product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.5 - Mapper #1 Function Code\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Script Count,1\\n\") \n",
    "number_of_carts=0\n",
    "\n",
    "#Define data split for custom partitioner\n",
    "group1 = \"abcdefghijklm\"\n",
    "group2 = \"nopqrstuvwxyz\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    products=line.split() #split on whitespace\n",
    "    try:\n",
    "        products.sort() #This products against double-counting\n",
    "    except:\n",
    "        pass #don't bother sorting if there aren't any pairs\n",
    "    for i,product_a in enumerate(products):\n",
    "        line_output={}\n",
    "        for product_b in products[i+1:]:\n",
    "            try:\n",
    "                line_output[product_b]+=1\n",
    "            except KeyError: #If we haven't seen a product before, add it to the dictionary\n",
    "                line_output[product_b]=1\n",
    "        \n",
    "        if line_output.keys(): #only emit a record if we find more than one product in the cart\n",
    "            if product_a[0].lower() in group1:\n",
    "                print product_a+'\\t1\\t'+str(line_output)\n",
    "            else:\n",
    "                print product_a+'\\t2\\t'+str(line_output)\n",
    "            sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\") \n",
    "    number_of_carts+=1\n",
    "    \n",
    "    \n",
    "print '**Total'+'\\t1\\t{\"number_of_carts\":'+str(number_of_carts)+'}'\n",
    "print '**Total'+'\\t2\\t{\"number_of_carts\":'+str(number_of_carts)+'}'\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.5 - Reducer #1 Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "from collections import Counter, OrderedDict\n",
    "sys.stderr.write(\"reporter:counter:Reducer,Script Count,1\\n\") \n",
    "s=100 #cutoff for \"frequent\"\n",
    "current_product_dict=Counter({}) #Counters make tracking individual product counts easier\n",
    "current_product=None\n",
    "count = 0 #Running total of occurrances for the chosen product\n",
    "\n",
    "number_of_carts=0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    #Parse line into fields\n",
    "    product,group,product_dict=line.strip().split('\\t')\n",
    "    #The dict is passed from the mapper as a string, so we need to convert it back to a dict\n",
    "    product_dict=Counter(eval(product_dict))  \n",
    "    \n",
    "    if product=='**Total': #Extract total products for order inversion\n",
    "        number_of_carts+=product_dict['number_of_carts']\n",
    "        continue\n",
    "  \n",
    "    if current_product==product:\n",
    "        #The counter is smart enough to increment keys when they exist, and create them when they don't\n",
    "        current_product_dict+=Counter(product_dict)\n",
    "    else:\n",
    "        if current_product and current_product!='**Total':\n",
    "            #Ordering the results ensures we maintain lexicographic sorting through the reducer\n",
    "            for i in OrderedDict(sorted(current_product_dict.items())):\n",
    "                if current_product_dict[i]>=s:\n",
    "                    sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\") \n",
    "                    print current_product+'\\t'+i+'\\t'+str(current_product_dict[i])+'\\t'+str(current_product_dict[i]/number_of_carts)\n",
    "        current_product=product\n",
    "        current_product_dict=product_dict\n",
    "\n",
    "#Make sure to emit final row\n",
    "if current_product and current_product!='**Total':\n",
    "    for i in OrderedDict(sorted(current_product_dict.items())):\n",
    "        if current_product_dict[i]>=s:\n",
    "            sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\") \n",
    "            print current_product+'\\t'+i+'\\t'+str(current_product_dict[i])+'\\t'+str(current_product_dict[i]/number_of_carts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.5 - Second mapper/reducer and running the job\n",
    "As in our pairs implementation, we can use the same identity mapper/reducer to pass records through a Hadoop shuffle with secondary sort to enable the proper final ordering of the records from our first job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:43:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:43:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_5_tmp_output\n"
     ]
    }
   ],
   "source": [
    "### Make sure 1st job output directory is clear in HDFS\n",
    "!bin/hdfs dfs -rm -r hw_3_5_tmp_output\n",
    "#!bin/hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar451974825488861623/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob7016206982505071666.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:43:27 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:43:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:43:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:43:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:43:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 00:43:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/03 00:43:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 00:43:29 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/03 00:43:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/03 00:43:29 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/03 00:43:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0124\n",
      "16/02/03 00:43:29 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0124\n",
      "16/02/03 00:43:29 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0124/\n",
      "16/02/03 00:43:29 INFO mapreduce.Job: Running job: job_1454033924139_0124\n",
      "16/02/03 00:43:34 INFO mapreduce.Job: Job job_1454033924139_0124 running in uber mode : false\n",
      "16/02/03 00:43:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:43:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:43:54 INFO mapreduce.Job:  map 100% reduce 34%\n",
      "16/02/03 00:43:56 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "16/02/03 00:43:57 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "16/02/03 00:43:59 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/02/03 00:44:00 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/02/03 00:44:02 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "16/02/03 00:44:03 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/02/03 00:44:08 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/02/03 00:44:15 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/02/03 00:44:18 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "16/02/03 00:44:19 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/02/03 00:44:22 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/02/03 00:44:25 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/02/03 00:44:28 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/02/03 00:44:30 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "16/02/03 00:44:33 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/02/03 00:44:36 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/02/03 00:44:37 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/02/03 00:44:40 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/02/03 00:44:42 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/02/03 00:44:45 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/02/03 00:44:46 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/02/03 00:44:49 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/02/03 00:44:51 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/02/03 00:44:54 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/02/03 00:44:55 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/02/03 00:44:57 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/02/03 00:44:58 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/02/03 00:45:04 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/02/03 00:45:07 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/02/03 00:45:13 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/02/03 00:45:16 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/02/03 00:45:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:45:22 INFO mapreduce.Job: Job job_1454033924139_0124 completed successfully\n",
      "16/02/03 00:45:22 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=43180068\n",
      "\t\tFILE: Number of bytes written=86801358\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462847\n",
      "\t\tHDFS: Number of bytes written=51765\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15730\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=165371\n",
      "\t\tTotal time spent by all map tasks (ms)=15730\n",
      "\t\tTotal time spent by all reduce tasks (ms)=165371\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15730\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=165371\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16107520\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=169339904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=349727\n",
      "\t\tMap output bytes=42341631\n",
      "\t\tMap output materialized bytes=43180080\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12013\n",
      "\t\tReduce shuffle bytes=43180080\n",
      "\t\tReduce input records=349727\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=699454\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=360\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=680525824\n",
      "\tMapper\n",
      "\t\tLine Count=349725\n",
      "\t\tScript Count=2\n",
      "\tReducer\n",
      "\t\tLine Count=1334\n",
      "\t\tScript Count=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=51765\n",
      "16/02/03 00:45:22 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_5_tmp_output\n",
      "\n",
      "real\t1m55.395s\n",
      "user\t0m5.323s\n",
      "sys\t0m0.398s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the primary stripes job in Hadoop\n",
    "time bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keypartitioner.options='-k1,2' \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options='-k1,1' \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/ProductPurchaseData.txt -output /user/nicholashamlin/hw_3_5_tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:47:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:47:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw_3_5_final_output\n"
     ]
    }
   ],
   "source": [
    "#Make sure the final output directory is clear\n",
    "!bin/hdfs dfs -rm -r hw_3_5_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./identity.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar3891535724588519065/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob6117757244022260949.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/03 00:47:51 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 00:47:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 00:47:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:47:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/03 00:47:53 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/02/03 00:47:53 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/02/03 00:47:53 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/03 00:47:53 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/03 00:47:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454033924139_0125\n",
      "16/02/03 00:47:53 INFO impl.YarnClientImpl: Submitted application application_1454033924139_0125\n",
      "16/02/03 00:47:53 INFO mapreduce.Job: The url to track the job: http://Nicholass-MacBook-Pro.local:8088/proxy/application_1454033924139_0125/\n",
      "16/02/03 00:47:53 INFO mapreduce.Job: Running job: job_1454033924139_0125\n",
      "16/02/03 00:47:58 INFO mapreduce.Job: Job job_1454033924139_0125 running in uber mode : false\n",
      "16/02/03 00:47:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 00:48:06 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/03 00:48:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 00:48:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 00:48:13 INFO mapreduce.Job: Job job_1454033924139_0125 completed successfully\n",
      "16/02/03 00:48:14 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=54439\n",
      "\t\tFILE: Number of bytes written=548175\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=56227\n",
      "\t\tHDFS: Number of bytes written=51765\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18018\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3071\n",
      "\t\tTotal time spent by all map tasks (ms)=18018\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3071\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18018\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3071\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18450432\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3144704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=1334\n",
      "\t\tMap output bytes=51765\n",
      "\t\tMap output materialized bytes=54451\n",
      "\t\tInput split bytes=366\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1334\n",
      "\t\tReduce shuffle bytes=54451\n",
      "\t\tReduce input records=1334\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=2668\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=370\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=756023296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=55861\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=51765\n",
      "16/02/03 00:48:14 INFO streaming.StreamJob: Output directory: /user/nicholashamlin/hw_3_5_final_output\n",
      "\n",
      "real\t0m23.687s\n",
      "user\t0m5.093s\n",
      "sys\t0m0.357s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the sorting job with identity mapper/reducer\n",
    "time bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options='-k 3,3nr -k 1,1 -k 2,2' \\\n",
    "-file ./identity.py    -mapper ./identity.py \\\n",
    "-reducer ./identity.py \\\n",
    "-input /user/nicholashamlin/hw_3_5_tmp_output -output /user/nicholashamlin/hw_3_5_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 3.5 RESULTS:\n",
      "\n",
      "50 Most Frequent Pairs:\n",
      "Product 1   |   Product 2 | Raw Freq. | Support \n",
      "16/02/03 00:49:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779\tELE17451\t1592\t0.0511880646925\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\n",
      "DAI62779\tSNA80324\t923\t0.0296775023311\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\n",
      "DAI43223\tELE32164\t711\t0.022861001254\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\n",
      "DAI62779\tELE26917\t650\t0.020899649529\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\n",
      "DAI62779\tSNA45677\t604\t0.0194205974084\n",
      "ELE17451\tSNA80324\t597\t0.0191955242597\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\n",
      "DAI62779\tSNA55762\t593\t0.0190669110318\n",
      "DAI62779\tDAI83733\t586\t0.018841837883\n",
      "ELE17451\tGRO73461\t580\t0.0186489180412\n",
      "GRO73461\tSNA80324\t562\t0.0180701585158\n",
      "DAI62779\tGRO59710\t561\t0.0180380052088\n",
      "DAI62779\tFRO80039\t550\t0.0176843188322\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\n",
      "DAI62779\tSNA93860\t537\t0.0172663258416\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\n",
      "ELE17451\tELE32164\t511\t0.0164303398605\n",
      "DAI62779\tSNA18336\t506\t0.0162695733256\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\n",
      "DAI62779\tFRO78087\t482\t0.0154978939584\n",
      "DAI85309\tELE17451\t482\t0.0154978939584\n",
      "DAI62779\tGRO94758\t479\t0.0154014340375\n",
      "DAI62779\tGRO21487\t471\t0.0151442075817\n",
      "GRO85051\tSNA80324\t471\t0.0151442075817\n",
      "ELE17451\tGRO30386\t468\t0.0150477476608\n",
      "FRO85978\tSNA95666\t463\t0.014886981126\n",
      "DAI62779\tFRO19221\t462\t0.014854827819\n",
      "DAI62779\tGRO46854\t461\t0.0148226745121\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\n",
      "ELE92920\tSNA18336\t455\t0.0146297546703\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Examine the output of the job in HDFS and print the results\n",
    "! echo \"HW 3.5 RESULTS:\"\n",
    "! echo \"\"\n",
    "! echo \"50 Most Frequent Pairs:\"\n",
    "! echo \"Product 1   |   Product 2 | Raw Freq. | Support \"\n",
    "!bin/hdfs dfs -cat hw_3_5_final_output/* | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 3.5 - Discussion of results\n",
    "Again, we are running this job on a single Macbook Pro with 2 cores using two mappers and two reducers.  This time though, **we find the job takes 1 minutes and 55 seconds to run**, with the mapper and reducer both getting called twice.  This extra runtime is likely caused by the fact that reducing is a more computationally complex process when we're using stripes.  Pairs, on the other hand, generates many more intermediate data points for the reducer to process, though their structure is simpler.  The main benefit of the stripes paradigm is that it reduces the amount of network throughput required during a job, and it's in this exchange of data points that efficiencies can be created.  However, since we're running this job on a single machine rather than a cluster, the network traffic savings don't outweight the additional computational complexity of the stripes reducer step.\n",
    "\n",
    "We also explored whether or not we could improve performance by using a manual dictionary merge instead of the python Counter object, which carries some extra computational complexity.  While this change helped slightly, it didn't make a major difference (and did increase the complexity of the code itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
