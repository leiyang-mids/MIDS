{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#2\n",
    "####Lei Yang (leiyang@berkeley.edu)\n",
    "####Due: 2016-01-26, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.0.*\n",
    "- What is a race condition in the context of parallel computation? Give an example.\n",
    "- What is MapReduce?\n",
    "- How does it differ from Hadoop?\n",
    "- Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW2.0 Answer:</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-resourcemanager-Leis-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-nodemanager-Leis-MacBook-Pro.local.out\n",
      "16/01/23 19:06:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n",
      "16/01/23 19:06:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###create a folder and upload enronemail_1h.txt to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 10:56:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 10:56:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/lei\n",
    "!hdfs dfs -put enronemail_1h.txt /user/lei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.1.* Sort in Hadoop MapReduce\n",
    "- Given as input: Records of the form < integer, “NA”>, where integer is any integer, and “NA” is just the empty string.\n",
    "- Output: sorted key value pairs of the form < integer, “NA”> in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "- Write code to generate N  random records of the form < integer, “NA”>. Let N = 10,000.\n",
    "- Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.1 Answer:</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Simulate 10,000 records of < integer, \"NA\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 19:07:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 19:07:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted HW2_1.txt\n",
      "16/01/23 19:07:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm HW2_1.txt\n",
    "import random\n",
    "text_file = open(\"HW2_1.txt\", \"w\")\n",
    "N = 10000\n",
    "for i in range(N):\n",
    "    text_file.writelines('<%d, \"NA\">\\n' %(random.randint(1, N)))\n",
    "text_file.close()\n",
    "!hdfs dfs -put HW2_1.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "for line in sys.stdin:   \n",
    "    parts = line.strip('<').split(',')\n",
    "    if len(parts) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        temp = int(parts[0])\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    print \"%s\\t%s\" %(parts[0], line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys, Queue\n",
    "\n",
    "n_max, n_min = 10, 10\n",
    "q_max = Queue.Queue(n_max)\n",
    "a_min = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    rec = line.split('\\t')[1].strip('\\n')\n",
    "    # put the smallest\n",
    "    if len(a_min) < n_min:\n",
    "        a_min.append(rec)\n",
    "    \n",
    "    # whatever left is the biggest\n",
    "    if q_max.full():\n",
    "        q_max.get()\n",
    "    q_max.put(rec)\n",
    "\n",
    "print '\\n%d smallest records:' %n_min\n",
    "for record in a_min:\n",
    "    print record\n",
    "\n",
    "print '\\n%d biggest records:' %n_max\n",
    "for i in range(n_max):\n",
    "    print q_max.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop Streaming\n",
    "- add parameter *-D mapred.text.key.comparator.options=-n* to sort the key by numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 19:23:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 19:23:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results\n",
      "16/01/23 19:23:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 19:23:46 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 19:23:46 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 19:23:46 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 19:23:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 19:23:46 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 19:23:46 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/23 19:23:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local44658220_0001\n",
      "16/01/23 19:23:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 19:23:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 19:23:46 INFO mapreduce.Job: Running job: job_local44658220_0001\n",
      "16/01/23 19:23:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 19:23:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 19:23:46 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 19:23:46 INFO mapred.LocalJobRunner: Starting task: attempt_local44658220_0001_m_000000_0\n",
      "16/01/23 19:23:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 19:23:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 19:23:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 19:23:46 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/leiyang/HW2_1.txt:0+128915\n",
      "16/01/23 19:23:46 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper.py]\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: \n",
      "16/01/23 19:23:47 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: bufstart = 0; bufend = 177830; bufvoid = 104857600\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "16/01/23 19:23:47 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 19:23:47 INFO mapred.Task: Task:attempt_local44658220_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "16/01/23 19:23:47 INFO mapred.Task: Task 'attempt_local44658220_0001_m_000000_0' done.\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local44658220_0001_m_000000_0\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: Starting task: attempt_local44658220_0001_r_000000_0\n",
      "16/01/23 19:23:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 19:23:47 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 19:23:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 19:23:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4843ca49\n",
      "16/01/23 19:23:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 19:23:47 INFO reduce.EventFetcher: attempt_local44658220_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 19:23:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local44658220_0001_m_000000_0 decomp: 197832 len: 197836 to MEMORY\n",
      "16/01/23 19:23:47 INFO reduce.InMemoryMapOutput: Read 197832 bytes from map-output for attempt_local44658220_0001_m_000000_0\n",
      "16/01/23 19:23:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 197832, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->197832\n",
      "16/01/23 19:23:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 19:23:47 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 19:23:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 19:23:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 197828 bytes\n",
      "16/01/23 19:23:47 INFO reduce.MergeManagerImpl: Merged 1 segments, 197832 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 19:23:47 INFO reduce.MergeManagerImpl: Merging 1 files, 197836 bytes from disk\n",
      "16/01/23 19:23:47 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 19:23:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 19:23:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 197828 bytes\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer.py]\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 19:23:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/23 19:23:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 19:23:47 INFO mapred.Task: Task:attempt_local44658220_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 19:23:47 INFO mapred.Task: Task attempt_local44658220_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 19:23:47 INFO output.FileOutputCommitter: Saved output of task 'attempt_local44658220_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/results/_temporary/0/task_local44658220_0001_r_000000\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "16/01/23 19:23:47 INFO mapred.Task: Task 'attempt_local44658220_0001_r_000000_0' done.\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local44658220_0001_r_000000_0\n",
      "16/01/23 19:23:47 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 19:23:47 INFO mapreduce.Job: Job job_local44658220_0001 running in uber mode : false\n",
      "16/01/23 19:23:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 19:23:47 INFO mapreduce.Job: Job job_local44658220_0001 completed successfully\n",
      "16/01/23 19:23:47 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=607764\n",
      "\t\tFILE: Number of bytes written=1388624\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=257830\n",
      "\t\tHDFS: Number of bytes written=323\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=177830\n",
      "\t\tMap output materialized bytes=197836\n",
      "\t\tInput split bytes=96\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6352\n",
      "\t\tReduce shuffle bytes=197836\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=24\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=128915\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=323\n",
      "16/01/23 19:23:47 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -D mapred.text.key.comparator.options=-n -mapper mapper.py -reducer reducer.py -input HW2_1.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Check 10 smallest and biggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 19:23:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "10 smallest records:\t\n",
      "<1, \"NA\">\t\n",
      "<100, \"NA\">\t\n",
      "<1000, \"NA\">\t\n",
      "<1000, \"NA\">\t\n",
      "<1002, \"NA\">\t\n",
      "<1004, \"NA\">\t\n",
      "<1005, \"NA\">\t\n",
      "<1005, \"NA\">\t\n",
      "<1006, \"NA\">\t\n",
      "<1006, \"NA\">\t\n",
      "\t\n",
      "10 biggest records:\t\n",
      "<9991, \"NA\">\t\n",
      "<9993, \"NA\">\t\n",
      "<9994, \"NA\">\t\n",
      "<9994, \"NA\">\t\n",
      "<9994, \"NA\">\t\n",
      "<9996, \"NA\">\t\n",
      "<9996, \"NA\">\t\n",
      "<9999, \"NA\">\t\n",
      "<9999, \"NA\">\t\n",
      "<9999, \"NA\">\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.2.*  WORDCOUNT\n",
    "- Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). \n",
    "- Examine the word “assistance” and report its word count results.\n",
    "- CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "  - 8    \n",
    "  - \\#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    line = line.split('\\t', 2)[-1]\n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    line = regex.sub(' ', line.lower())\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # save count            \n",
    "            wordcount[current_word] = current_count\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# found count for word assistance\n",
    "findword = 'assistance'\n",
    "print '%s\\t%d' %(findword, wordcount[findword] if findword in wordcount else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:21:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results\n",
      "16/01/23 13:21:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:21:39 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 13:21:39 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 13:21:39 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 13:21:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 13:21:40 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 13:21:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1327097026_0001\n",
      "16/01/23 13:21:40 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 13:21:40 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 13:21:40 INFO mapreduce.Job: Running job: job_local1327097026_0001\n",
      "16/01/23 13:21:40 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 13:21:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:21:40 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 13:21:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1327097026_0001_m_000000_0\n",
      "16/01/23 13:21:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:21:40 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 13:21:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 13:21:40 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper.py]\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 13:21:40 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 13:21:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:40 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/23 13:21:40 INFO streaming.PipeMapRed: R/W/S=100/571/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:40 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 13:21:40 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 13:21:40 INFO mapred.LocalJobRunner: \n",
      "16/01/23 13:21:40 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: bufstart = 0; bufend = 252202; bufvoid = 104857600\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26081588(104326352); length = 132809/6553600\n",
      "16/01/23 13:21:40 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 13:21:40 INFO mapred.Task: Task:attempt_local1327097026_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/23 13:21:41 INFO mapred.Task: Task 'attempt_local1327097026_0001_m_000000_0' done.\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: Finishing task: attempt_local1327097026_0001_m_000000_0\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: Starting task: attempt_local1327097026_0001_r_000000_0\n",
      "16/01/23 13:21:41 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:21:41 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 13:21:41 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 13:21:41 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@234aaba8\n",
      "16/01/23 13:21:41 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 13:21:41 INFO reduce.EventFetcher: attempt_local1327097026_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 13:21:41 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1327097026_0001_m_000000_0 decomp: 318610 len: 318614 to MEMORY\n",
      "16/01/23 13:21:41 INFO reduce.InMemoryMapOutput: Read 318610 bytes from map-output for attempt_local1327097026_0001_m_000000_0\n",
      "16/01/23 13:21:41 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318610, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318610\n",
      "16/01/23 13:21:41 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:21:41 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 13:21:41 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 13:21:41 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318606 bytes\n",
      "16/01/23 13:21:41 INFO reduce.MergeManagerImpl: Merged 1 segments, 318610 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 13:21:41 INFO reduce.MergeManagerImpl: Merging 1 files, 318614 bytes from disk\n",
      "16/01/23 13:21:41 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 13:21:41 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 13:21:41 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318606 bytes\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer.py]\n",
      "16/01/23 13:21:41 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 13:21:41 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: Records R/W=33203/1\n",
      "16/01/23 13:21:41 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 13:21:41 INFO mapred.Task: Task:attempt_local1327097026_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:21:41 INFO mapred.Task: Task attempt_local1327097026_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 13:21:41 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1327097026_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/results/_temporary/0/task_local1327097026_0001_r_000000\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: Records R/W=33203/1 > reduce\n",
      "16/01/23 13:21:41 INFO mapred.Task: Task 'attempt_local1327097026_0001_r_000000_0' done.\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: Finishing task: attempt_local1327097026_0001_r_000000_0\n",
      "16/01/23 13:21:41 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 13:21:41 INFO mapreduce.Job: Job job_local1327097026_0001 running in uber mode : false\n",
      "16/01/23 13:21:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 13:21:41 INFO mapreduce.Job: Job job_local1327097026_0001 completed successfully\n",
      "16/01/23 13:21:41 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=849328\n",
      "\t\tFILE: Number of bytes written=1756126\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=14\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=252202\n",
      "\t\tMap output materialized bytes=318614\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=318614\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14\n",
      "16/01/23 13:21:41 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper mapper.py -reducer reducer.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.2 Results:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.2.1*  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer \n",
    "- same as above, only sort total count at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # save count            \n",
    "            wordcount[current_word] = current_count\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# sort count top get top n counts:\n",
    "n = 10\n",
    "sort_count = sorted(wordcount.items(), key=operator.itemgetter(1))\n",
    "print 'Top %d counts out of %d words:' %(n, len(sort_count))\n",
    "for i in range(n):\n",
    "    print '%s\\t%d' %(sort_count[-i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:22:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:22:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results\n",
      "16/01/23 13:22:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:22:09 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 13:22:09 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 13:22:09 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 13:22:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 13:22:09 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 13:22:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1546098219_0001\n",
      "16/01/23 13:22:10 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 13:22:10 INFO mapreduce.Job: Running job: job_local1546098219_0001\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 13:22:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Starting task: attempt_local1546098219_0001_m_000000_0\n",
      "16/01/23 13:22:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:22:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 13:22:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper.py]\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=100/602/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: \n",
      "16/01/23 13:22:10 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: bufstart = 0; bufend = 252202; bufvoid = 104857600\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26081588(104326352); length = 132809/6553600\n",
      "16/01/23 13:22:10 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 13:22:10 INFO mapred.Task: Task:attempt_local1546098219_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/23 13:22:10 INFO mapred.Task: Task 'attempt_local1546098219_0001_m_000000_0' done.\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local1546098219_0001_m_000000_0\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Starting task: attempt_local1546098219_0001_r_000000_0\n",
      "16/01/23 13:22:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:22:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 13:22:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 13:22:10 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4843ca49\n",
      "16/01/23 13:22:10 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 13:22:10 INFO reduce.EventFetcher: attempt_local1546098219_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 13:22:10 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1546098219_0001_m_000000_0 decomp: 318610 len: 318614 to MEMORY\n",
      "16/01/23 13:22:10 INFO reduce.InMemoryMapOutput: Read 318610 bytes from map-output for attempt_local1546098219_0001_m_000000_0\n",
      "16/01/23 13:22:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318610, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318610\n",
      "16/01/23 13:22:10 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:22:10 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 13:22:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 13:22:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318606 bytes\n",
      "16/01/23 13:22:10 INFO reduce.MergeManagerImpl: Merged 1 segments, 318610 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 13:22:10 INFO reduce.MergeManagerImpl: Merging 1 files, 318614 bytes from disk\n",
      "16/01/23 13:22:10 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 13:22:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 13:22:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318606 bytes\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer.py]\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 13:22:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: Records R/W=33203/1\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 13:22:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 13:22:10 INFO mapred.Task: Task:attempt_local1546098219_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:22:10 INFO mapred.Task: Task attempt_local1546098219_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 13:22:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1546098219_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/results/_temporary/0/task_local1546098219_0001_r_000000\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Records R/W=33203/1 > reduce\n",
      "16/01/23 13:22:10 INFO mapred.Task: Task 'attempt_local1546098219_0001_r_000000_0' done.\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local1546098219_0001_r_000000_0\n",
      "16/01/23 13:22:10 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 13:22:11 INFO mapreduce.Job: Job job_local1546098219_0001 running in uber mode : false\n",
      "16/01/23 13:22:11 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 13:22:11 INFO mapreduce.Job: Job job_local1546098219_0001 completed successfully\n",
      "16/01/23 13:22:11 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=849328\n",
      "\t\tFILE: Number of bytes written=1756126\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=252202\n",
      "\t\tMap output materialized bytes=318614\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=318614\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=11\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "16/01/23 13:22:11 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper mapper.py -reducer reducer.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.2.1 Results:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:22:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 10 counts out of 5408 words:\t\n",
      "the\t1247\n",
      "to\t964\n",
      "and\t670\n",
      "of\t566\n",
      "a\t543\n",
      "you\t445\n",
      "in\t418\n",
      "your\t395\n",
      "ect\t382\n",
      "for\t374\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.3.* Multinomial NAIVE BAYES with NO Smoothing\n",
    "- Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. \n",
    "- Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). \n",
    "- Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    " - the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    " - E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. \n",
    " - Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. \n",
    " - Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. \n",
    " - Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "- Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. \n",
    "- Plot a histogram of the posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see. \n",
    "- Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    " - Let DF represent the evalution set in the following:\n",
    " - Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    " - Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (training)\n",
    "- Differ from previous one, in that we need to separate counts for spam and ham\n",
    "- key~value pair: word~[1, flag for ham/spam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_t.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%d\\t%s\\t%s' % (word, 1, isSpam, msgID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (training)\n",
    "Assuming *positional independence*, and without smoothing, the multinomial NB conditional probability $P(t | c)$ can be estimated as:\n",
    "$$\n",
    "\\hat{P}(t\\mid c)=\\frac{T_{ct}+0}{(\\sum_{t^\\prime \\in V}{T_{ct^\\prime}})+B},\n",
    "$$\n",
    "\n",
    "where $B=|V|$ is the number of terms in the vocabulary $V$ (including all text classes), and $T_{ct}$ is the count of word *t* in class *c*. \n",
    "\n",
    "To classify a message, the posterior probability of class $c$ can be calculated as:\n",
    "$$\n",
    "c_{map}=\\arg\\max_{c\\in\\mathbb C}[\\log{\\hat{P}(c)}+\\sum_{1\\leqslant k \\leqslant n_d}{\\log{\\hat{P}(t_k\\mid c)}}],\n",
    "$$\n",
    "where $\\hat{P}(t_k\\mid c)$ is estimated above with *positional independence* assumption as $\\hat{P}(t\\mid c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (classification)\n",
    "- key~value pair: msgID~[probability, flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probability from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"prob/part-00000\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip bad message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1                \n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isSpam, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_c.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "current_msg = None\n",
    "current_prob = [0, 0] \n",
    "current_truth = 0\n",
    "msgID = None\n",
    "n_error = 0\n",
    "n_msg = 0\n",
    "n_zero = [0, 0]\n",
    "\n",
    "print '%s\\t%s\\t%s' %('TRUTH', 'PREDICTION', 'EMAIL ID')\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    #print line\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper_c.py\n",
    "    msgID, p0, p1, isSpam, pr0, pr1 = line.split('\\t')\n",
    "    prob = [float(p0), float(p1)]\n",
    "    \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:        \n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_msg == msgID:        \n",
    "        current_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], current_prob], 0)\n",
    "    else:\n",
    "        if current_msg:\n",
    "            # count finish for current word, predict and print result\n",
    "            pred = np.argmax(current_prob)\n",
    "            n_error += pred != current_truth\n",
    "            n_msg += 1\n",
    "            n_zero[current_truth] += float('-inf') in current_prob\n",
    "            print '%s\\t%s\\t%s' %(current_truth, pred, current_msg)\n",
    "                    \n",
    "        # initialize new count for new word\n",
    "        prior = [math.log(float(pr0)), math.log(float(pr1))]\n",
    "        current_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], prior], 0)\n",
    "        current_msg = msgID\n",
    "        current_truth = isSpam\n",
    "\n",
    "# do not forget to print the last msg result if needed!\n",
    "if current_msg == msgID:\n",
    "    pred = np.argmax(current_prob)\n",
    "    n_error += pred != isSpam\n",
    "    n_msg += 1\n",
    "    n_zero[current_truth] += float('-inf') in current_prob\n",
    "    print '%s\\t%s\\t%s' %(current_truth, pred, msgID)\n",
    "    \n",
    "# calculate the overall error rate\n",
    "print 'Error rate: %.4f' %(1.0*n_error/n_msg)\n",
    "print 'Number of zero probability: spam(%d), ham(%d)' %(n_zero[1], n_zero[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the jobs with Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 20:55:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 20:55:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results\n",
      "16/01/23 20:55:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 20:55:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted prob\n",
      "16/01/23 20:55:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 20:55:44 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 20:55:44 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 20:55:45 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 20:55:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 20:55:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 20:55:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local417393215_0001\n",
      "16/01/23 20:55:45 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 20:55:45 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 20:55:45 INFO mapreduce.Job: Running job: job_local417393215_0001\n",
      "16/01/23 20:55:45 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 20:55:45 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 20:55:45 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 20:55:45 INFO mapred.LocalJobRunner: Starting task: attempt_local417393215_0001_m_000000_0\n",
      "16/01/23 20:55:45 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 20:55:45 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 20:55:45 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 20:55:45 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper_t.py]\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 20:55:45 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 20:55:45 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:45 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:45 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/23 20:55:45 INFO streaming.PipeMapRed: R/W/S=100/5684/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 20:55:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 20:55:45 INFO mapred.LocalJobRunner: \n",
      "16/01/23 20:55:45 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: bufstart = 0; bufend = 1105594; bufvoid = 104857600\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26081588(104326352); length = 132809/6553600\n",
      "16/01/23 20:55:45 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 20:55:45 INFO mapred.Task: Task:attempt_local417393215_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/23 20:55:46 INFO mapred.Task: Task 'attempt_local417393215_0001_m_000000_0' done.\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local417393215_0001_m_000000_0\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: Starting task: attempt_local417393215_0001_r_000000_0\n",
      "16/01/23 20:55:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 20:55:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 20:55:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 20:55:46 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@14dc9b4e\n",
      "16/01/23 20:55:46 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 20:55:46 INFO reduce.EventFetcher: attempt_local417393215_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 20:55:46 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local417393215_0001_m_000000_0 decomp: 1172002 len: 1172006 to MEMORY\n",
      "16/01/23 20:55:46 INFO reduce.InMemoryMapOutput: Read 1172002 bytes from map-output for attempt_local417393215_0001_m_000000_0\n",
      "16/01/23 20:55:46 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1172002, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1172002\n",
      "16/01/23 20:55:46 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 20:55:46 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 20:55:46 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 20:55:46 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1171998 bytes\n",
      "16/01/23 20:55:46 INFO reduce.MergeManagerImpl: Merged 1 segments, 1172002 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 20:55:46 INFO reduce.MergeManagerImpl: Merging 1 files, 1172006 bytes from disk\n",
      "16/01/23 20:55:46 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 20:55:46 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 20:55:46 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1171998 bytes\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer_t.py]\n",
      "16/01/23 20:55:46 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 20:55:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: Records R/W=33203/1\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 20:55:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 20:55:46 INFO mapred.Task: Task:attempt_local417393215_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 20:55:46 INFO mapred.Task: Task attempt_local417393215_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 20:55:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_local417393215_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/prob/_temporary/0/task_local417393215_0001_r_000000\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: Records R/W=33203/1 > reduce\n",
      "16/01/23 20:55:46 INFO mapred.Task: Task 'attempt_local417393215_0001_r_000000_0' done.\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local417393215_0001_r_000000_0\n",
      "16/01/23 20:55:46 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 20:55:46 INFO mapreduce.Job: Job job_local417393215_0001 running in uber mode : false\n",
      "16/01/23 20:55:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 20:55:46 INFO mapreduce.Job: Job job_local417393215_0001 completed successfully\n",
      "16/01/23 20:55:46 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2556112\n",
      "\t\tFILE: Number of bytes written=4313298\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=173923\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172006\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172006\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=173923\n",
      "16/01/23 20:55:46 INFO streaming.StreamJob: Output directory: prob\n",
      "16/01/23 20:55:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 20:55:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 20:55:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 20:55:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 20:55:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 20:55:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local310726038_0001\n",
      "16/01/23 20:55:48 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 20:55:48 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 20:55:48 INFO mapreduce.Job: Running job: job_local310726038_0001\n",
      "16/01/23 20:55:48 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 20:55:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 20:55:48 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 20:55:48 INFO mapred.LocalJobRunner: Starting task: attempt_local310726038_0001_m_000000_0\n",
      "16/01/23 20:55:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 20:55:48 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 20:55:48 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 20:55:48 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 20:55:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper_c.py]\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 20:55:48 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 20:55:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:49 INFO mapreduce.Job: Job job_local310726038_0001 running in uber mode : false\n",
      "16/01/23 20:55:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 20:55:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 20:55:50 INFO streaming.PipeMapRed: Records R/W=96/1\n",
      "16/01/23 20:55:50 INFO streaming.PipeMapRed: R/W/S=100/96/0 in:100=100/1 [rec/s] out:97=97/1 [rec/s]\n",
      "16/01/23 20:55:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 20:55:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: \n",
      "16/01/23 20:55:50 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 20:55:50 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 20:55:50 INFO mapred.MapTask: bufstart = 0; bufend = 2186426; bufvoid = 104857600\n",
      "16/01/23 20:55:50 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26081588(104326352); length = 132809/6553600\n",
      "16/01/23 20:55:50 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 20:55:50 INFO mapred.Task: Task:attempt_local310726038_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: Records R/W=96/1\n",
      "16/01/23 20:55:50 INFO mapred.Task: Task 'attempt_local310726038_0001_m_000000_0' done.\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local310726038_0001_m_000000_0\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: Starting task: attempt_local310726038_0001_r_000000_0\n",
      "16/01/23 20:55:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 20:55:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 20:55:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 20:55:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3ca5953\n",
      "16/01/23 20:55:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 20:55:50 INFO reduce.EventFetcher: attempt_local310726038_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 20:55:50 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local310726038_0001_m_000000_0 decomp: 2252834 len: 2252838 to MEMORY\n",
      "16/01/23 20:55:50 INFO reduce.InMemoryMapOutput: Read 2252834 bytes from map-output for attempt_local310726038_0001_m_000000_0\n",
      "16/01/23 20:55:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2252834, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2252834\n",
      "16/01/23 20:55:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 20:55:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 20:55:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 20:55:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2252809 bytes\n",
      "16/01/23 20:55:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 2252834 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 20:55:50 INFO reduce.MergeManagerImpl: Merging 1 files, 2252838 bytes from disk\n",
      "16/01/23 20:55:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 20:55:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 20:55:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2252809 bytes\n",
      "16/01/23 20:55:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 20:55:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer_c.py]\n",
      "16/01/23 20:55:50 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 20:55:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: Records R/W=33203/1\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 20:55:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 20:55:51 INFO mapred.Task: Task:attempt_local310726038_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 20:55:51 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 20:55:51 INFO mapred.Task: Task attempt_local310726038_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 20:55:51 INFO output.FileOutputCommitter: Saved output of task 'attempt_local310726038_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/results/_temporary/0/task_local310726038_0001_r_000000\n",
      "16/01/23 20:55:51 INFO mapred.LocalJobRunner: Records R/W=33203/1 > reduce\n",
      "16/01/23 20:55:51 INFO mapred.Task: Task 'attempt_local310726038_0001_r_000000_0' done.\n",
      "16/01/23 20:55:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local310726038_0001_r_000000_0\n",
      "16/01/23 20:55:51 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 20:55:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 20:55:51 INFO mapreduce.Job: Job job_local310726038_0001 completed successfully\n",
      "16/01/23 20:55:51 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4717776\n",
      "\t\tFILE: Number of bytes written=7555806\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2765\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=2186426\n",
      "\t\tMap output materialized bytes=2252838\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=2252838\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=103\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2765\n",
      "16/01/23 20:55:51 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r results\n",
    "!hdfs dfs -rm -r prob\n",
    "# run training job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper mapper_t.py -reducer reducer_t.py -input /user/lei/enronemail_1h.txt -output prob\n",
    "# run classification job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.*/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper mapper_c.py -reducer reducer_c.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.3 Results: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 20:56:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "TRUTH\tPREDICTION\tEMAIL ID\n",
      "0\t0\t0001.1999-12-10.farmer\n",
      "0\t0\t0001.1999-12-10.kaminski\n",
      "0\t0\t0001.2000-01-17.beck\n",
      "0\t0\t0001.2000-06-06.lokay\n",
      "0\t0\t0001.2001-02-07.kitchen\n",
      "0\t0\t0001.2001-04-02.williams\n",
      "0\t0\t0002.1999-12-13.farmer\n",
      "0\t0\t0002.2001-02-07.kitchen\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\n",
      "1\t1\t0002.2003-12-18.GP\n",
      "1\t1\t0002.2004-08-01.BG\n",
      "0\t0\t0003.1999-12-10.kaminski\n",
      "0\t0\t0003.1999-12-14.farmer\n",
      "0\t0\t0003.2000-01-17.beck\n",
      "0\t0\t0003.2001-02-08.kitchen\n",
      "1\t1\t0003.2003-12-18.GP\n",
      "1\t1\t0003.2004-08-01.BG\n",
      "0\t0\t0004.1999-12-10.kaminski\n",
      "0\t0\t0004.1999-12-14.farmer\n",
      "0\t0\t0004.2001-04-02.williams\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\n",
      "1\t1\t0004.2004-08-01.BG\n",
      "0\t0\t0005.1999-12-12.kaminski\n",
      "0\t0\t0005.1999-12-14.farmer\n",
      "0\t0\t0005.2000-06-06.lokay\n",
      "0\t0\t0005.2001-02-08.kitchen\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\n",
      "1\t1\t0005.2003-12-18.GP\n",
      "0\t0\t0006.1999-12-13.kaminski\n",
      "0\t0\t0006.2001-02-08.kitchen\n",
      "0\t0\t0006.2001-04-03.williams\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\n",
      "1\t1\t0006.2003-12-18.GP\n",
      "1\t1\t0006.2004-08-01.BG\n",
      "0\t0\t0007.1999-12-13.kaminski\n",
      "0\t0\t0007.1999-12-14.farmer\n",
      "0\t0\t0007.2000-01-17.beck\n",
      "0\t0\t0007.2001-02-09.kitchen\n",
      "1\t1\t0007.2003-12-18.GP\n",
      "1\t1\t0007.2004-08-01.BG\n",
      "0\t0\t0008.2001-02-09.kitchen\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\n",
      "1\t1\t0008.2003-12-18.GP\n",
      "1\t1\t0008.2004-08-01.BG\n",
      "0\t0\t0009.1999-12-13.kaminski\n",
      "0\t0\t0009.1999-12-14.farmer\n",
      "0\t0\t0009.2000-06-07.lokay\n",
      "0\t0\t0009.2001-02-09.kitchen\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\n",
      "1\t1\t0009.2003-12-18.GP\n",
      "0\t0\t0010.1999-12-14.farmer\n",
      "0\t0\t0010.1999-12-14.kaminski\n",
      "0\t0\t0010.2001-02-09.kitchen\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\n",
      "1\t1\t0010.2003-12-18.GP\n",
      "1\t1\t0010.2004-08-01.BG\n",
      "0\t0\t0011.1999-12-14.farmer\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\n",
      "1\t1\t0011.2003-12-18.GP\n",
      "1\t1\t0011.2004-08-01.BG\n",
      "0\t0\t0012.1999-12-14.farmer\n",
      "0\t0\t0012.1999-12-14.kaminski\n",
      "0\t0\t0012.2000-01-17.beck\n",
      "0\t0\t0012.2000-06-08.lokay\n",
      "0\t0\t0012.2001-02-09.kitchen\n",
      "1\t1\t0012.2003-12-19.GP\n",
      "0\t0\t0013.1999-12-14.farmer\n",
      "0\t0\t0013.1999-12-14.kaminski\n",
      "0\t0\t0013.2001-04-03.williams\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\n",
      "1\t1\t0013.2004-08-01.BG\n",
      "0\t0\t0014.1999-12-14.kaminski\n",
      "0\t0\t0014.1999-12-15.farmer\n",
      "0\t0\t0014.2001-02-12.kitchen\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\n",
      "1\t1\t0014.2003-12-19.GP\n",
      "1\t1\t0014.2004-08-01.BG\n",
      "0\t0\t0015.1999-12-14.kaminski\n",
      "0\t0\t0015.1999-12-15.farmer\n",
      "0\t0\t0015.2000-06-09.lokay\n",
      "0\t0\t0015.2001-02-12.kitchen\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\n",
      "1\t1\t0015.2003-12-19.GP\n",
      "0\t0\t0016.1999-12-15.farmer\n",
      "0\t0\t0016.2001-02-12.kitchen\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\n",
      "1\t1\t0016.2003-12-19.GP\n",
      "1\t1\t0016.2004-08-01.BG\n",
      "0\t0\t0017.1999-12-14.kaminski\n",
      "0\t0\t0017.2000-01-17.beck\n",
      "0\t0\t0017.2001-04-03.williams\n",
      "1\t1\t0017.2003-12-18.GP\n",
      "1\t1\t0017.2004-08-01.BG\n",
      "1\t1\t0017.2004-08-02.BG\n",
      "0\t0\t0018.1999-12-14.kaminski\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\n",
      "1\t1\t0018.2003-12-18.GP\n",
      "Error rate: 0.0000\t\n",
      "Number of zero probability: spam(44), ham(56)\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Discussion on posterior probability:\n",
    "- the NB posterior probability is: \n",
    "$$\n",
    "P(Y=y_k\\mid X_1,X_2, ...,X_n)=\\frac{P(Y=y_k)\\prod_i{P(X_i\\mid Y=y_k)}}{\\sum_j{P(Y=y_j)\\prod_i{P(X_i\\mid Y=y_j)}}}\n",
    "$$\n",
    "- from the results we can see for each message, there are words that only belong to one category\n",
    " - in this case the conditional probability of those words for the opposite category, $P(X_i\\mid Y=y_j)$, are zero   \n",
    " - and this will make one term (with respect to $j$) on the denominator become zero, \n",
    " - and thus the posterior probability is 1 for the category that contains all the words from the email, and 0 for the other category\n",
    "- this fact illustrates that NB classifier is *over-confident*, the training error in this case is small, but the validation error can be big\n",
    "- and this brings the need for smoothing, such that the posterior probability of one class won't be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.4* Repeat HW2.3 with the following modification: \n",
    "- use Laplace plus-one smoothing,\n",
    "- compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (training)\n",
    "- minor change: assign 1 to the smooth_factor (instead of 0 in HW2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 1 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the jobs with Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 21:01:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:01:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results\n",
      "16/01/23 21:01:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:01:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted prob\n",
      "16/01/23 21:01:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:01:31 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 21:01:31 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 21:01:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 21:01:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 21:01:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 21:01:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local193142017_0001\n",
      "16/01/23 21:01:32 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 21:01:32 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 21:01:32 INFO mapreduce.Job: Running job: job_local193142017_0001\n",
      "16/01/23 21:01:32 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 21:01:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:01:32 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 21:01:32 INFO mapred.LocalJobRunner: Starting task: attempt_local193142017_0001_m_000000_0\n",
      "16/01/23 21:01:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:01:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:01:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 21:01:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper_t.py]\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 21:01:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 21:01:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:32 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/23 21:01:32 INFO streaming.PipeMapRed: R/W/S=100/5627/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:01:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:01:32 INFO mapred.LocalJobRunner: \n",
      "16/01/23 21:01:32 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: bufstart = 0; bufend = 1105594; bufvoid = 104857600\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26081588(104326352); length = 132809/6553600\n",
      "16/01/23 21:01:32 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 21:01:33 INFO mapred.Task: Task:attempt_local193142017_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/23 21:01:33 INFO mapred.Task: Task 'attempt_local193142017_0001_m_000000_0' done.\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local193142017_0001_m_000000_0\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: Starting task: attempt_local193142017_0001_r_000000_0\n",
      "16/01/23 21:01:33 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:01:33 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:01:33 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:01:33 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1f582895\n",
      "16/01/23 21:01:33 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 21:01:33 INFO reduce.EventFetcher: attempt_local193142017_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 21:01:33 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local193142017_0001_m_000000_0 decomp: 1172002 len: 1172006 to MEMORY\n",
      "16/01/23 21:01:33 INFO reduce.InMemoryMapOutput: Read 1172002 bytes from map-output for attempt_local193142017_0001_m_000000_0\n",
      "16/01/23 21:01:33 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1172002, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1172002\n",
      "16/01/23 21:01:33 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:01:33 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 21:01:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:01:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1171998 bytes\n",
      "16/01/23 21:01:33 INFO reduce.MergeManagerImpl: Merged 1 segments, 1172002 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 21:01:33 INFO reduce.MergeManagerImpl: Merging 1 files, 1172006 bytes from disk\n",
      "16/01/23 21:01:33 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 21:01:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:01:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1171998 bytes\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer_t.py]\n",
      "16/01/23 21:01:33 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 21:01:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: Records R/W=33203/1\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:01:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:01:33 INFO mapred.Task: Task:attempt_local193142017_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:01:33 INFO mapred.Task: Task attempt_local193142017_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 21:01:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_local193142017_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/prob/_temporary/0/task_local193142017_0001_r_000000\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: Records R/W=33203/1 > reduce\n",
      "16/01/23 21:01:33 INFO mapred.Task: Task 'attempt_local193142017_0001_r_000000_0' done.\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local193142017_0001_r_000000_0\n",
      "16/01/23 21:01:33 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 21:01:33 INFO mapreduce.Job: Job job_local193142017_0001 running in uber mode : false\n",
      "16/01/23 21:01:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 21:01:33 INFO mapreduce.Job: Job job_local193142017_0001 completed successfully\n",
      "16/01/23 21:01:33 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2556112\n",
      "\t\tFILE: Number of bytes written=4313298\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=231359\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172006\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172006\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=231359\n",
      "16/01/23 21:01:33 INFO streaming.StreamJob: Output directory: prob\n",
      "16/01/23 21:01:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 21:01:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 21:01:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 21:01:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 21:01:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 21:01:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local317361227_0001\n",
      "16/01/23 21:01:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 21:01:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 21:01:35 INFO mapreduce.Job: Running job: job_local317361227_0001\n",
      "16/01/23 21:01:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 21:01:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:01:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 21:01:35 INFO mapred.LocalJobRunner: Starting task: attempt_local317361227_0001_m_000000_0\n",
      "16/01/23 21:01:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:01:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:01:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 21:01:35 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 21:01:35 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper_c.py]\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 21:01:35 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 21:01:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:36 INFO mapreduce.Job: Job job_local317361227_0001 running in uber mode : false\n",
      "16/01/23 21:01:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 21:01:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:01:37 INFO streaming.PipeMapRed: Records R/W=96/1\n",
      "16/01/23 21:01:37 INFO streaming.PipeMapRed: R/W/S=100/105/0 in:100=100/1 [rec/s] out:106=106/1 [rec/s]\n",
      "16/01/23 21:01:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:01:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: \n",
      "16/01/23 21:01:37 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 21:01:37 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 21:01:37 INFO mapred.MapTask: bufstart = 0; bufend = 2328331; bufvoid = 104857600\n",
      "16/01/23 21:01:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26081588(104326352); length = 132809/6553600\n",
      "16/01/23 21:01:37 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 21:01:37 INFO mapred.Task: Task:attempt_local317361227_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: Records R/W=96/1\n",
      "16/01/23 21:01:37 INFO mapred.Task: Task 'attempt_local317361227_0001_m_000000_0' done.\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local317361227_0001_m_000000_0\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: Starting task: attempt_local317361227_0001_r_000000_0\n",
      "16/01/23 21:01:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:01:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:01:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:01:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3983a97f\n",
      "16/01/23 21:01:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 21:01:37 INFO reduce.EventFetcher: attempt_local317361227_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 21:01:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local317361227_0001_m_000000_0 decomp: 2394739 len: 2394743 to MEMORY\n",
      "16/01/23 21:01:37 INFO reduce.InMemoryMapOutput: Read 2394739 bytes from map-output for attempt_local317361227_0001_m_000000_0\n",
      "16/01/23 21:01:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2394739, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2394739\n",
      "16/01/23 21:01:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:01:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 21:01:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:01:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2394714 bytes\n",
      "16/01/23 21:01:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 2394739 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 21:01:37 INFO reduce.MergeManagerImpl: Merging 1 files, 2394743 bytes from disk\n",
      "16/01/23 21:01:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 21:01:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:01:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2394714 bytes\n",
      "16/01/23 21:01:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:01:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer_c.py]\n",
      "16/01/23 21:01:38 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 21:01:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: Records R/W=33203/1\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:01:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:01:38 INFO mapred.Task: Task:attempt_local317361227_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:01:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:01:38 INFO mapred.Task: Task attempt_local317361227_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 21:01:38 INFO output.FileOutputCommitter: Saved output of task 'attempt_local317361227_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/results/_temporary/0/task_local317361227_0001_r_000000\n",
      "16/01/23 21:01:38 INFO mapred.LocalJobRunner: Records R/W=33203/1 > reduce\n",
      "16/01/23 21:01:38 INFO mapred.Task: Task 'attempt_local317361227_0001_r_000000_0' done.\n",
      "16/01/23 21:01:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local317361227_0001_r_000000_0\n",
      "16/01/23 21:01:38 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 21:01:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 21:01:38 INFO mapreduce.Job: Job job_local317361227_0001 completed successfully\n",
      "16/01/23 21:01:38 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5001586\n",
      "\t\tFILE: Number of bytes written=7981521\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2763\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=2328331\n",
      "\t\tMap output materialized bytes=2394743\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=2394743\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=103\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2763\n",
      "16/01/23 21:01:38 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r results\n",
    "!hdfs dfs -rm -r prob\n",
    "# run training job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_t.py -reducer reducer_t.py -input /user/lei/enronemail_1h.txt -output prob\n",
    "# run classification job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_c.py -reducer reducer_c.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.4 Results: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 21:01:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "TRUTH\tPREDICTION\tEMAIL ID\n",
      "0\t0\t0001.1999-12-10.farmer\n",
      "0\t0\t0001.1999-12-10.kaminski\n",
      "0\t0\t0001.2000-01-17.beck\n",
      "0\t0\t0001.2000-06-06.lokay\n",
      "0\t0\t0001.2001-02-07.kitchen\n",
      "0\t0\t0001.2001-04-02.williams\n",
      "0\t0\t0002.1999-12-13.farmer\n",
      "0\t0\t0002.2001-02-07.kitchen\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\n",
      "1\t1\t0002.2003-12-18.GP\n",
      "1\t1\t0002.2004-08-01.BG\n",
      "0\t0\t0003.1999-12-10.kaminski\n",
      "0\t0\t0003.1999-12-14.farmer\n",
      "0\t0\t0003.2000-01-17.beck\n",
      "0\t0\t0003.2001-02-08.kitchen\n",
      "1\t1\t0003.2003-12-18.GP\n",
      "1\t1\t0003.2004-08-01.BG\n",
      "0\t0\t0004.1999-12-10.kaminski\n",
      "0\t0\t0004.1999-12-14.farmer\n",
      "0\t0\t0004.2001-04-02.williams\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\n",
      "1\t1\t0004.2004-08-01.BG\n",
      "0\t0\t0005.1999-12-12.kaminski\n",
      "0\t0\t0005.1999-12-14.farmer\n",
      "0\t0\t0005.2000-06-06.lokay\n",
      "0\t0\t0005.2001-02-08.kitchen\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\n",
      "1\t1\t0005.2003-12-18.GP\n",
      "0\t0\t0006.1999-12-13.kaminski\n",
      "0\t0\t0006.2001-02-08.kitchen\n",
      "0\t0\t0006.2001-04-03.williams\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\n",
      "1\t1\t0006.2003-12-18.GP\n",
      "1\t1\t0006.2004-08-01.BG\n",
      "0\t0\t0007.1999-12-13.kaminski\n",
      "0\t0\t0007.1999-12-14.farmer\n",
      "0\t0\t0007.2000-01-17.beck\n",
      "0\t0\t0007.2001-02-09.kitchen\n",
      "1\t1\t0007.2003-12-18.GP\n",
      "1\t1\t0007.2004-08-01.BG\n",
      "0\t0\t0008.2001-02-09.kitchen\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\n",
      "1\t1\t0008.2003-12-18.GP\n",
      "1\t1\t0008.2004-08-01.BG\n",
      "0\t0\t0009.1999-12-13.kaminski\n",
      "0\t0\t0009.1999-12-14.farmer\n",
      "0\t0\t0009.2000-06-07.lokay\n",
      "0\t0\t0009.2001-02-09.kitchen\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\n",
      "1\t1\t0009.2003-12-18.GP\n",
      "0\t0\t0010.1999-12-14.farmer\n",
      "0\t0\t0010.1999-12-14.kaminski\n",
      "0\t0\t0010.2001-02-09.kitchen\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\n",
      "1\t1\t0010.2003-12-18.GP\n",
      "1\t1\t0010.2004-08-01.BG\n",
      "0\t0\t0011.1999-12-14.farmer\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\n",
      "1\t1\t0011.2003-12-18.GP\n",
      "1\t1\t0011.2004-08-01.BG\n",
      "0\t0\t0012.1999-12-14.farmer\n",
      "0\t0\t0012.1999-12-14.kaminski\n",
      "0\t0\t0012.2000-01-17.beck\n",
      "0\t0\t0012.2000-06-08.lokay\n",
      "0\t0\t0012.2001-02-09.kitchen\n",
      "1\t1\t0012.2003-12-19.GP\n",
      "0\t0\t0013.1999-12-14.farmer\n",
      "0\t0\t0013.1999-12-14.kaminski\n",
      "0\t0\t0013.2001-04-03.williams\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\n",
      "1\t1\t0013.2004-08-01.BG\n",
      "0\t0\t0014.1999-12-14.kaminski\n",
      "0\t0\t0014.1999-12-15.farmer\n",
      "0\t0\t0014.2001-02-12.kitchen\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\n",
      "1\t1\t0014.2003-12-19.GP\n",
      "1\t1\t0014.2004-08-01.BG\n",
      "0\t0\t0015.1999-12-14.kaminski\n",
      "0\t0\t0015.1999-12-15.farmer\n",
      "0\t0\t0015.2000-06-09.lokay\n",
      "0\t0\t0015.2001-02-12.kitchen\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\n",
      "1\t1\t0015.2003-12-19.GP\n",
      "0\t0\t0016.1999-12-15.farmer\n",
      "0\t0\t0016.2001-02-12.kitchen\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\n",
      "1\t1\t0016.2003-12-19.GP\n",
      "1\t1\t0016.2004-08-01.BG\n",
      "0\t0\t0017.1999-12-14.kaminski\n",
      "0\t0\t0017.2000-01-17.beck\n",
      "0\t0\t0017.2001-04-03.williams\n",
      "1\t1\t0017.2003-12-18.GP\n",
      "1\t0\t0017.2004-08-01.BG\n",
      "1\t1\t0017.2004-08-02.BG\n",
      "0\t1\t0018.1999-12-14.kaminski\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\n",
      "1\t1\t0018.2003-12-18.GP\n",
      "Error rate: 0.0200\t\n",
      "Number of zero probability: spam(0), ham(0)\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW2.4 Discussion:\n",
    "- smoothing essentially add noise to the data\n",
    "- and thus introduce some misclassification on the training set\n",
    "- but it can have better results on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.5.* Repeat HW2.4. \n",
    "- This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. \n",
    "- How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer (training)\n",
    "- minor change with previous: at the end not emitting word~probability pair with less than 3 word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    # only emit probability when the count (spam and ham together) is no less than 3\n",
    "    if sum(wordcount[key]) >= 3:\n",
    "        print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Mapper (classification)\n",
    "- change: at the end only emit word which has probability from training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probability from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"prob/part-00000\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip bad message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1    \n",
    "        if word in prob:\n",
    "            print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isSpam, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the jobs with Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 21:05:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:05:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results\n",
      "16/01/23 21:05:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:05:23 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted prob\n",
      "16/01/23 21:05:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:05:25 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 21:05:25 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 21:05:25 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 21:05:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 21:05:25 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 21:05:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1259524004_0001\n",
      "16/01/23 21:05:26 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 21:05:26 INFO mapreduce.Job: Running job: job_local1259524004_0001\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 21:05:26 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Starting task: attempt_local1259524004_0001_m_000000_0\n",
      "16/01/23 21:05:26 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:05:26 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:05:26 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper_t.py]\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=100/5572/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: \n",
      "16/01/23 21:05:26 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: bufstart = 0; bufend = 1105594; bufvoid = 104857600\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26081588(104326352); length = 132809/6553600\n",
      "16/01/23 21:05:26 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 21:05:26 INFO mapred.Task: Task:attempt_local1259524004_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/23 21:05:26 INFO mapred.Task: Task 'attempt_local1259524004_0001_m_000000_0' done.\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local1259524004_0001_m_000000_0\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Starting task: attempt_local1259524004_0001_r_000000_0\n",
      "16/01/23 21:05:26 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:05:26 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:05:26 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:05:26 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4816ec79\n",
      "16/01/23 21:05:26 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 21:05:26 INFO reduce.EventFetcher: attempt_local1259524004_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 21:05:26 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1259524004_0001_m_000000_0 decomp: 1172002 len: 1172006 to MEMORY\n",
      "16/01/23 21:05:26 INFO reduce.InMemoryMapOutput: Read 1172002 bytes from map-output for attempt_local1259524004_0001_m_000000_0\n",
      "16/01/23 21:05:26 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1172002, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1172002\n",
      "16/01/23 21:05:26 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:05:26 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 21:05:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:05:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1171998 bytes\n",
      "16/01/23 21:05:26 INFO reduce.MergeManagerImpl: Merged 1 segments, 1172002 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 21:05:26 INFO reduce.MergeManagerImpl: Merging 1 files, 1172006 bytes from disk\n",
      "16/01/23 21:05:26 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 21:05:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:05:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1171998 bytes\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer_t.py]\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 21:05:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: Records R/W=33203/1\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:05:26 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:05:26 INFO mapred.Task: Task:attempt_local1259524004_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:05:26 INFO mapred.Task: Task attempt_local1259524004_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 21:05:26 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1259524004_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/prob/_temporary/0/task_local1259524004_0001_r_000000\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Records R/W=33203/1 > reduce\n",
      "16/01/23 21:05:26 INFO mapred.Task: Task 'attempt_local1259524004_0001_r_000000_0' done.\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local1259524004_0001_r_000000_0\n",
      "16/01/23 21:05:26 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 21:05:27 INFO mapreduce.Job: Job job_local1259524004_0001 running in uber mode : false\n",
      "16/01/23 21:05:27 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 21:05:27 INFO mapreduce.Job: Job job_local1259524004_0001 completed successfully\n",
      "16/01/23 21:05:27 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2556112\n",
      "\t\tFILE: Number of bytes written=4316306\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=66986\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172006\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172006\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=1877\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=66986\n",
      "16/01/23 21:05:27 INFO streaming.StreamJob: Output directory: prob\n",
      "16/01/23 21:05:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:05:28 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 21:05:28 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 21:05:28 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 21:05:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 21:05:29 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 21:05:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1108376799_0001\n",
      "16/01/23 21:05:29 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 21:05:29 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 21:05:29 INFO mapreduce.Job: Running job: job_local1108376799_0001\n",
      "16/01/23 21:05:29 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 21:05:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:05:29 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 21:05:29 INFO mapred.LocalJobRunner: Starting task: attempt_local1108376799_0001_m_000000_0\n",
      "16/01/23 21:05:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:05:29 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:05:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/lei/enronemail_1h.txt:0+203979\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 21:05:29 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 21:05:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./mapper_c.py]\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 21:05:29 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 21:05:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:30 INFO mapreduce.Job: Job job_local1108376799_0001 running in uber mode : false\n",
      "16/01/23 21:05:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 21:05:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: Records R/W=96/1\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: R/W/S=100/98/0 in:100=100/1 [rec/s] out:98=98/1 [rec/s]\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: \n",
      "16/01/23 21:05:31 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 21:05:31 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 21:05:31 INFO mapred.MapTask: bufstart = 0; bufend = 1930332; bufvoid = 104857600\n",
      "16/01/23 21:05:31 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26099432(104397728); length = 114965/6553600\n",
      "16/01/23 21:05:31 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 21:05:31 INFO mapred.Task: Task:attempt_local1108376799_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: Records R/W=96/1\n",
      "16/01/23 21:05:31 INFO mapred.Task: Task 'attempt_local1108376799_0001_m_000000_0' done.\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108376799_0001_m_000000_0\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: Starting task: attempt_local1108376799_0001_r_000000_0\n",
      "16/01/23 21:05:31 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 21:05:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 21:05:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 21:05:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@459c8859\n",
      "16/01/23 21:05:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 21:05:31 INFO reduce.EventFetcher: attempt_local1108376799_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 21:05:31 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1108376799_0001_m_000000_0 decomp: 1987818 len: 1987822 to MEMORY\n",
      "16/01/23 21:05:31 INFO reduce.InMemoryMapOutput: Read 1987818 bytes from map-output for attempt_local1108376799_0001_m_000000_0\n",
      "16/01/23 21:05:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1987818, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1987818\n",
      "16/01/23 21:05:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:05:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 21:05:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:05:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1987793 bytes\n",
      "16/01/23 21:05:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 1987818 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 21:05:31 INFO reduce.MergeManagerImpl: Merging 1 files, 1987822 bytes from disk\n",
      "16/01/23 21:05:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 21:05:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 21:05:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1987793 bytes\n",
      "16/01/23 21:05:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/leiyang/GitHub/MIDS/W261/HW2-Questions/./reducer_c.py]\n",
      "16/01/23 21:05:31 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 21:05:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:31 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 21:05:32 INFO streaming.PipeMapRed: Records R/W=28742/1\n",
      "16/01/23 21:05:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 21:05:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 21:05:32 INFO mapred.Task: Task:attempt_local1108376799_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 21:05:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 21:05:32 INFO mapred.Task: Task attempt_local1108376799_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 21:05:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108376799_0001_r_000000_0' to hdfs://localhost:9000/user/leiyang/results/_temporary/0/task_local1108376799_0001_r_000000\n",
      "16/01/23 21:05:32 INFO mapred.LocalJobRunner: Records R/W=28742/1 > reduce\n",
      "16/01/23 21:05:32 INFO mapred.Task: Task 'attempt_local1108376799_0001_r_000000_0' done.\n",
      "16/01/23 21:05:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108376799_0001_r_000000_0\n",
      "16/01/23 21:05:32 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 21:05:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 21:05:32 INFO mapreduce.Job: Job job_local1108376799_0001 completed successfully\n",
      "16/01/23 21:05:32 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4187744\n",
      "\t\tFILE: Number of bytes written=6763766\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2765\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=28742\n",
      "\t\tMap output bytes=1930332\n",
      "\t\tMap output materialized bytes=1987822\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1987822\n",
      "\t\tReduce input records=28742\n",
      "\t\tReduce output records=103\n",
      "\t\tSpilled Records=57484\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2765\n",
      "16/01/23 21:05:32 INFO streaming.StreamJob: Output directory: results\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r results\n",
    "!hdfs dfs -rm -r prob\n",
    "# run training job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_t.py -reducer reducer_t.py -input /user/lei/enronemail_1h.txt -output prob\n",
    "# run classification job\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2*/libexec/share/hadoop/tools/lib/hadoop-streaming-2*.jar -mapper mapper_c.py -reducer reducer_c.py -input /user/lei/enronemail_1h.txt -output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.5 Results: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 21:05:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "TRUTH\tPREDICTION\tEMAIL ID\n",
      "0\t0\t0001.1999-12-10.farmer\n",
      "0\t0\t0001.1999-12-10.kaminski\n",
      "0\t0\t0001.2000-01-17.beck\n",
      "0\t0\t0001.2000-06-06.lokay\n",
      "0\t0\t0001.2001-02-07.kitchen\n",
      "0\t0\t0001.2001-04-02.williams\n",
      "0\t0\t0002.1999-12-13.farmer\n",
      "0\t0\t0002.2001-02-07.kitchen\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\n",
      "1\t1\t0002.2003-12-18.GP\n",
      "1\t1\t0002.2004-08-01.BG\n",
      "0\t0\t0003.1999-12-10.kaminski\n",
      "0\t0\t0003.1999-12-14.farmer\n",
      "0\t0\t0003.2000-01-17.beck\n",
      "0\t0\t0003.2001-02-08.kitchen\n",
      "1\t1\t0003.2003-12-18.GP\n",
      "1\t1\t0003.2004-08-01.BG\n",
      "0\t0\t0004.1999-12-10.kaminski\n",
      "0\t0\t0004.1999-12-14.farmer\n",
      "0\t0\t0004.2001-04-02.williams\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\n",
      "1\t1\t0004.2004-08-01.BG\n",
      "0\t0\t0005.1999-12-12.kaminski\n",
      "0\t0\t0005.1999-12-14.farmer\n",
      "0\t0\t0005.2000-06-06.lokay\n",
      "0\t0\t0005.2001-02-08.kitchen\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\n",
      "1\t1\t0005.2003-12-18.GP\n",
      "0\t0\t0006.1999-12-13.kaminski\n",
      "0\t0\t0006.2001-02-08.kitchen\n",
      "0\t0\t0006.2001-04-03.williams\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\n",
      "1\t1\t0006.2003-12-18.GP\n",
      "1\t1\t0006.2004-08-01.BG\n",
      "0\t0\t0007.1999-12-13.kaminski\n",
      "0\t0\t0007.1999-12-14.farmer\n",
      "0\t0\t0007.2000-01-17.beck\n",
      "0\t0\t0007.2001-02-09.kitchen\n",
      "1\t1\t0007.2003-12-18.GP\n",
      "1\t1\t0007.2004-08-01.BG\n",
      "0\t0\t0008.2001-02-09.kitchen\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\n",
      "1\t1\t0008.2003-12-18.GP\n",
      "1\t1\t0008.2004-08-01.BG\n",
      "0\t0\t0009.1999-12-13.kaminski\n",
      "0\t0\t0009.1999-12-14.farmer\n",
      "0\t0\t0009.2000-06-07.lokay\n",
      "0\t0\t0009.2001-02-09.kitchen\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\n",
      "1\t1\t0009.2003-12-18.GP\n",
      "0\t0\t0010.1999-12-14.farmer\n",
      "0\t0\t0010.1999-12-14.kaminski\n",
      "0\t0\t0010.2001-02-09.kitchen\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\n",
      "1\t0\t0010.2003-12-18.GP\n",
      "1\t1\t0010.2004-08-01.BG\n",
      "0\t0\t0011.1999-12-14.farmer\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\n",
      "1\t1\t0011.2003-12-18.GP\n",
      "1\t1\t0011.2004-08-01.BG\n",
      "0\t0\t0012.1999-12-14.farmer\n",
      "0\t0\t0012.1999-12-14.kaminski\n",
      "0\t0\t0012.2000-01-17.beck\n",
      "0\t0\t0012.2000-06-08.lokay\n",
      "0\t0\t0012.2001-02-09.kitchen\n",
      "1\t1\t0012.2003-12-19.GP\n",
      "0\t0\t0013.1999-12-14.farmer\n",
      "0\t0\t0013.1999-12-14.kaminski\n",
      "0\t0\t0013.2001-04-03.williams\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\n",
      "1\t1\t0013.2004-08-01.BG\n",
      "0\t0\t0014.1999-12-14.kaminski\n",
      "0\t0\t0014.1999-12-15.farmer\n",
      "0\t0\t0014.2001-02-12.kitchen\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\n",
      "1\t1\t0014.2003-12-19.GP\n",
      "1\t1\t0014.2004-08-01.BG\n",
      "0\t0\t0015.1999-12-14.kaminski\n",
      "0\t0\t0015.1999-12-15.farmer\n",
      "0\t0\t0015.2000-06-09.lokay\n",
      "0\t0\t0015.2001-02-12.kitchen\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\n",
      "1\t1\t0015.2003-12-19.GP\n",
      "0\t0\t0016.1999-12-15.farmer\n",
      "0\t0\t0016.2001-02-12.kitchen\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\n",
      "1\t1\t0016.2003-12-19.GP\n",
      "1\t1\t0016.2004-08-01.BG\n",
      "0\t0\t0017.1999-12-14.kaminski\n",
      "0\t0\t0017.2000-01-17.beck\n",
      "0\t0\t0017.2001-04-03.williams\n",
      "1\t1\t0017.2003-12-18.GP\n",
      "1\t1\t0017.2004-08-01.BG\n",
      "1\t1\t0017.2004-08-02.BG\n",
      "0\t0\t0018.1999-12-14.kaminski\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\n",
      "1\t1\t0018.2003-12-18.GP\n",
      "Error rate: 0.0100\t\n",
      "Number of zero probability: spam(43), ham(54)\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW2.5 Discussion:\n",
    "- similar to smoothing, ignoring small frequency words may eliminate some that only belong to one category\n",
    "- and to reduce the chance of binary (1-0) posterior probability for prediction\n",
    "- training error increases a bit, not surprisingly, but may benefit prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.6.* Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "- Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK- multinomial NB training error: 0.0000\n",
      "SK- Bernoulli   NB training error: 0.1600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "with open('enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "train_label = [msg[1] for msg in emails]\n",
    "train_data = [msg[2]+msg[3] if len(msg)==4 else msg[2] for msg in emails]\n",
    "msg_id = [msg[0].lower() for msg in emails]\n",
    "\n",
    "# feature vectorization\n",
    "uniVectorizer = CountVectorizer()\n",
    "dtmTrain = uniVectorizer.fit_transform(train_data) \n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(dtmTrain, train_label)\n",
    "pred_mnb = mnb.predict(dtmTrain)\n",
    "training_error_mnb = 1.0*sum(pred_mnb != train_label) / len(train_label)\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier from sklearn\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(dtmTrain, train_label)\n",
    "pred_bnb = bnb.predict(dtmTrain)\n",
    "training_error_bnb = 1.0*sum(pred_bnb != train_label) / len(train_label)\n",
    "\n",
    "print 'SK- multinomial NB training error: %.4f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB training error: %.4f' %training_error_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###*HW 2.6.1. OPTIONAL* (note this exercise is a stretch HW and optional)\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "- Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "- Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<span style=\"color:red\">HW2.6.1 Results: </span>See above\n",
    "###HW2.6 Discussion:\n",
    "- Bernoulli NB classifier focuses on the appearance of word, while Multinomial NB classifier emphasizes on the frequency of the word\n",
    "- thus the performance difference between the two can be caused by the bias in training data: there are words that appear way more frequently in one category than the other. In this case multinomial will perform better.\n",
    "- I'd choose Bernoulli classifier, as for SPAM detection, the appearance of keywords is more important than the frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.7. OPTIONAL* (note this exercise is a stretch HW and optional)\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "- Line 1 contains the subject\n",
    "- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. \n",
    "Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.8.*\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    "- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    "- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?\n",
    "\n",
    "###*HW2.8.1.*\n",
    "- Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "- Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 19:05:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 19:06:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
