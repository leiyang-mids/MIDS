{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#2\n",
    "####Lei Yang (leiyang@berkeley.edu)\n",
    "####Due: 2016-01-26, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.0.*\n",
    "- What is a race condition in the context of parallel computation? Give an example.\n",
    "- What is MapReduce?\n",
    "- How does it differ from Hadoop?\n",
    "- Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.0.0 Answer:</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.1.* Sort in Hadoop MapReduce\n",
    "- Given as input: Records of the form <integer, “NA”>, where integer is any integer, and “NA” is just the empty string.\n",
    "- Output: sorted key value pairs of the form <integer, “NA”> in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "- Write code to generate N  random records of the form <integer, “NA”>. Let N = 10,000.\n",
    "- Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.0.1 Answer:</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.2.*  WORDCOUNT\n",
    "- Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). \n",
    "- Examine the word “assistance” and report its word count results.\n",
    "- CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "  - 8    \n",
    "  - \\#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.2.1*  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's define *pNaiveBayes.sh* script first, we only need to do this once since it is the same throughout HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py & reducer.py, and make all scripts executable\n",
    "- *mapper.py* counts the single specified word for the chunk, and output an integer\n",
    "- *reducer.py* collates counts from all chunks, and output the total count of the single specified word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "countword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        for word in line.lower().split()[2:]:\n",
    "            if countword in word:\n",
    "                count += 1\n",
    "print countword + ' ' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for filename in sys.argv[1:]:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            temp = line.split()\n",
    "            word = temp[0]\n",
    "            sum += int(temp[1])\n",
    "print word + ': ' + str(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.2 Results: </span>by checking the ouput file, we know there are 10 counts of word 'assistance'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance: 10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.3.* Multinomial NAIVE BAYES with NO Smoothing\n",
    "- Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. \n",
    "- Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). \n",
    "- Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    " - the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    " - E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "- Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the log posterior probabilities (i.e., log(Pr(Class|Doc))) for each class over the training set. Summarize what you see. \n",
    "- Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    " - Let DF represent the evalution set in the following:\n",
    " - Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    " - Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define mapper.py:\n",
    "- obtains count for each word from the chunk, for spam and non-spam email separately, \n",
    "- records all counts in a dictionary, \n",
    "- outputs the dictionaries (non-spam count, and spam count), (non)spam counts, and keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keyword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keyword + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define reducer.py:\n",
    "- collapse wrod counts from all chunks\n",
    "- estimate NB model parameters: prior and conditional probabilities\n",
    "- classify messages that contains the keyword\n",
    "- **Note:** for messages that don't contain the keyword, the decision is solely based on prior probability, which will always give non-spam prediction, thus we skip those messages and only focus on those with the specified keyword\n",
    "- output results\n",
    "\n",
    "####Parameter estimation background:\n",
    "Assuming *positional independence*, and with *add-one Laplace smoothing*, the multinomial NB conditional probability $P(t | c)$ can be estimated as:\n",
    "$$\n",
    "\\hat{P}(t\\mid c)=\\frac{T_{ct}+1}{(\\sum_{t^\\prime \\in V}{T_{ct^\\prime}})+B},\n",
    "$$\n",
    "\n",
    "where $B=|V|$ is the number of terms in the vocabulary $V$ (including all text classes), and $T_{ct}$ is the count of word *t* in class *c*. \n",
    "\n",
    "To classify a message, the posterior probability of class $c$ can be calculated as:\n",
    "$$\n",
    "c_{map}=\\arg\\max_{c\\in\\mathbb C}[\\log{\\hat{P}(c)}+\\sum_{1\\leqslant k \\leqslant n_d}{\\log{\\hat{P}(t_k\\mid c)}}],\n",
    "$$\n",
    "where $\\hat{P}(t_k\\mid c)$ is estimated above with *positional independence* assumption as $\\hat{P}(t\\mid c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keyword = counts[4]\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with key word: ' + keyword\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "p_word_s = 1.0*((s_count[keyword] if keyword in s_count else 0) + 0) / (tot_s + B) # no smoothing\n",
    "p_word_n = 1.0*((n_count[keyword] if keyword in n_count else 0) + 0) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### prior probability: same for every message, since it's determined by training data ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "# print model parameters\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "print 'P(%s|spam) = %f' %(keyword, p_word_s)\n",
    "print 'P(%s|non-spam) = %f' %(keyword, p_word_n)\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'TRUTH \\t CLASS \\t ID'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        n_word = sum([1 if keyword in word else 0 for word in words])\n",
    "        # if the message doesn't contain our keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        #### posterior probability ####\n",
    "        p_s_word = math.log(p_s) + n_word * math.log(p_word_s)\n",
    "        p_n_word = math.log(p_n) + n_word * math.log(p_word_n)\n",
    "        isSpam = True if p_s_word > p_n_word else False        \n",
    "        # print results\n",
    "        print ('spam' if int(msg[1]) else 'ham') + '\\t' + ('spam' if isSpam else 'ham') + '\\t' + msg[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.3 Results: </span>run the NB classifier with keyword 'assistance', the output file are displayed below:\n",
    "- **Model parameters**: \n",
    " - prior \n",
    " - likelihood\n",
    "- **Classification results**: \n",
    " - TRUTH: original label\n",
    " - CLASS: filter result\n",
    " - ID: message ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with key word: assistance\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000189\r\n",
      "P(assistance|non-spam) = 0.000047\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "TRUTH \t CLASS \t ID\r\n",
      "spam\tspam\t0002.2004-08-01.bg\r\n",
      "ham\tspam\t0004.1999-12-10.kaminski\r\n",
      "ham\tspam\t0005.1999-12-12.kaminski\r\n",
      "spam\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0013.2004-08-01.bg\r\n",
      "spam\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "spam\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.4* Repeat HW2.3 with the following modification: \n",
    "- use Laplace plus-one smoothing,\n",
    "- compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of mapper.py remains the same as it still just counts words for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "keywords = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam\n",
    "print \"'\" + keywords + \"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of reducer.py is modified to consider multiple keywords, which we use dictionaries to represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # pass along the keyword for classification\n",
    "    keywords = counts[4].split()\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with keywords: ' + str(keywords)\n",
    "   \n",
    "# we now estimate NB parameters for the specified word, according to the formular above\n",
    "B = len(Set(s_count.keys() + n_count.keys()))\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "\n",
    "#### prior probability ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "#### conditional probabilities for words ####\n",
    "p_word_s, p_word_n = {}, {}\n",
    "for word in keywords:\n",
    "    p_word_s[word] = 1.0*((s_count[word] if word in s_count else 0) + 1) / (tot_s + B)\n",
    "    p_word_n[word] = 1.0*((n_count[word] if word in n_count else 0) + 1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### print model parameters ####\n",
    "print '\\n============= Model Parameters ============='\n",
    "print 'P(spam) = %f' %(p_s)\n",
    "print 'P(non-spam) = %f' %(p_n)\n",
    "for word in keywords:\n",
    "    print 'P(%s|spam) = %f' %(word, p_word_s[word])\n",
    "    print 'P(%s|non-spam) = %f' %(word, p_word_n[word])\n",
    "\n",
    "#### likelihood: dependend on the frequency of specified word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'TRUTH \\t CLASS \\t ID'\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        #### initialize posterior probability ####\n",
    "        p_s_word = math.log(p_s)\n",
    "        p_n_word = math.log(p_n)\n",
    "        \n",
    "        #### add likelihood for each keyword ####\n",
    "        n_word = 0\n",
    "        for key in keywords:\n",
    "            n_key = sum([1 if key in word else 0 for word in words])\n",
    "            n_word += n_key\n",
    "            p_s_word += n_key * math.log(p_word_s[key])\n",
    "            p_n_word += n_key * math.log(p_word_n[key])\n",
    "            \n",
    "        # if the message doesn't contain any keyword, skip it;\n",
    "        if n_word == 0:\n",
    "            continue\n",
    "        isSpam = True if p_s_word > p_n_word else False        \n",
    "        # print results\n",
    "        print ('spam' if int(msg[1]) else 'ham') + '\\t' + ('spam' if isSpam else 'ham') + '\\t' + msg[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.4 Results: </span>run the NB classifier with keywords 'assistance', 'valium' and 'enlargementWithATypo', the output file are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with keywords: ['assistance', 'valium', 'enlargementwithatypo']\r\n",
      "\r\n",
      "============= Model Parameters =============\r\n",
      "P(spam) = 0.440000\r\n",
      "P(non-spam) = 0.560000\r\n",
      "P(assistance|spam) = 0.000227\r\n",
      "P(assistance|non-spam) = 0.000093\r\n",
      "P(valium|spam) = 0.000038\r\n",
      "P(valium|non-spam) = 0.000047\r\n",
      "P(enlargementwithatypo|spam) = 0.000038\r\n",
      "P(enlargementwithatypo|non-spam) = 0.000047\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "TRUTH \t CLASS \t ID\r\n",
      "spam\tspam\t0002.2004-08-01.bg\r\n",
      "ham\tspam\t0004.1999-12-10.kaminski\r\n",
      "ham\tspam\t0005.1999-12-12.kaminski\r\n",
      "spam\tham\t0009.2003-12-18.gp\r\n",
      "spam\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0013.2004-08-01.bg\r\n",
      "spam\tham\t0016.2003-12-19.gp\r\n",
      "spam\tham\t0017.2004-08-01.bg\r\n",
      "spam\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "spam\tspam\t0018.2003-12-18.gp\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance valium enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.5.* Repeat HW2.4. \n",
    "- This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. \n",
    "- How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of mapper.py remains the same as it still just counts words for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# let's use two dictionaries to hold the word counts for spam and non-spam\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "#keywords = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for email in myfile.readlines():\n",
    "        isSpam = email.split('\\t')[1] == '1'\n",
    "        if isSpam:\n",
    "            nSpam += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in s_count:\n",
    "                    s_count[word] = 1\n",
    "                else:\n",
    "                    s_count[word] += 1\n",
    "        else:\n",
    "            nNormal += 1\n",
    "            for word in email.lower().split()[2:]: # only use subject & content for modeling\n",
    "                if word not in n_count:\n",
    "                    n_count[word] = 1\n",
    "                else:\n",
    "                    n_count[word] += 1\n",
    "print n_count\n",
    "print s_count\n",
    "print nNormal\n",
    "print nSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Definition of reducer.py is modified to consider all present words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "from sets import Set\n",
    "\n",
    "n_count, s_count = {}, {}\n",
    "nSpam, nNormal = 0, 0\n",
    "counts = []\n",
    "\n",
    "# scan through each output file from the chunks\n",
    "for filename in sys.argv[1:]:\n",
    "    # we first read out the 2 count dictionaries\n",
    "    with open (filename, \"r\") as myfile:         \n",
    "        for line in myfile.readlines():\n",
    "            cmd = 'counts.append(' + line + ')'\n",
    "            exec cmd\n",
    "            \n",
    "    # we then combine word counts, for non-spam and spam messages, respectively\n",
    "    for word in counts[0]:\n",
    "        if word not in n_count:\n",
    "            n_count[word] = counts[0][word]\n",
    "        else:\n",
    "            n_count[word] += counts[0][word]\n",
    "    \n",
    "    for word in counts[1]:\n",
    "        if word not in s_count:\n",
    "            s_count[word] = counts[1][word]\n",
    "        else:\n",
    "            s_count[word] += counts[1][word]\n",
    "            \n",
    "    # combine spam and non-spam count\n",
    "    nNormal += int(counts[2])\n",
    "    nSpam += int(counts[3])\n",
    "    \n",
    "    # clear counts for next chunk\n",
    "    counts = []\n",
    "\n",
    "testfile = 'enronemail_1h.txt'\n",
    "print 'Classify messages with all words'\n",
    "   \n",
    "# we now estimate NB parameters for all present words\n",
    "allwords = Set(s_count.keys() + n_count.keys())\n",
    "B = len(allwords)\n",
    "tot_n = sum(n_count.values())\n",
    "tot_s = sum(s_count.values())\n",
    "\n",
    "#### prior probability ####\n",
    "p_s = 1.0*nSpam/(nSpam+nNormal)\n",
    "p_n = 1.0*nNormal/(nSpam+nNormal)\n",
    "\n",
    "#### conditional probabilities for words ####\n",
    "p_word_s, p_word_n = {}, {}\n",
    "for word in allwords:\n",
    "    p_word_s[word] = 1.0*((s_count[word] if word in s_count else 0) + .1) / (tot_s + B) #Laplace add 1 smoothing\n",
    "    p_word_n[word] = 1.0*((n_count[word] if word in n_count else 0) + .1) / (tot_n + B)\n",
    "\n",
    "# finally we classify the messages which contains the specified word\n",
    "#### we won't print model parameters, to save some space ####\n",
    "#### likelihood: dependend on the frequency of current word ####\n",
    "print '\\n============= Classification Results ============='\n",
    "print 'TRUTH \\t CLASS \\t ID'\n",
    "n_correct = 0\n",
    "with open (testfile, \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        msg = line.lower().split()\n",
    "        words = msg[2:] # only include words in subject and content\n",
    "        #### initialize posterior probability ####\n",
    "        p_s_word = math.log(p_s)\n",
    "        p_n_word = math.log(p_n)\n",
    "        \n",
    "        #### add likelihood for each keyword ####        \n",
    "        for key in Set(words):\n",
    "            n_key = sum([1 if key in word else 0 for word in words])\n",
    "            p_s_word += n_key * math.log(p_word_s[key])\n",
    "            p_n_word += n_key * math.log(p_word_n[key])\n",
    "            \n",
    "        isSpam = True if p_s_word > p_n_word else False\n",
    "        n_correct += isSpam == int(msg[1])\n",
    "        # print results\n",
    "        print ('spam' if int(msg[1]) else 'ham') + '\\t' + ('spam' if isSpam else 'ham') + '\\t' + msg[0]\n",
    "\n",
    "print '\\nOur multinomial NB training error: %f' %(1-1.0*n_correct/(nSpam+nNormal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">HW1.5 Results: </span>run the NB classifier all present words, the output file are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify messages with all words\r\n",
      "\r\n",
      "============= Classification Results =============\r\n",
      "TRUTH \t CLASS \t ID\r\n",
      "ham\tham\t0001.1999-12-10.farmer\r\n",
      "ham\tham\t0001.1999-12-10.kaminski\r\n",
      "ham\tham\t0001.2000-01-17.beck\r\n",
      "ham\tham\t0001.2000-06-06.lokay\r\n",
      "ham\tham\t0001.2001-02-07.kitchen\r\n",
      "ham\tham\t0001.2001-04-02.williams\r\n",
      "ham\tham\t0002.1999-12-13.farmer\r\n",
      "ham\tham\t0002.2001-02-07.kitchen\r\n",
      "spam\tspam\t0002.2001-05-25.sa_and_hp\r\n",
      "spam\tspam\t0002.2003-12-18.gp\r\n",
      "spam\tspam\t0002.2004-08-01.bg\r\n",
      "ham\tham\t0003.1999-12-10.kaminski\r\n",
      "ham\tham\t0003.1999-12-14.farmer\r\n",
      "ham\tham\t0003.2000-01-17.beck\r\n",
      "ham\tham\t0003.2001-02-08.kitchen\r\n",
      "spam\tspam\t0003.2003-12-18.gp\r\n",
      "spam\tspam\t0003.2004-08-01.bg\r\n",
      "ham\tham\t0004.1999-12-10.kaminski\r\n",
      "ham\tham\t0004.1999-12-14.farmer\r\n",
      "ham\tham\t0004.2001-04-02.williams\r\n",
      "spam\tspam\t0004.2001-06-12.sa_and_hp\r\n",
      "spam\tspam\t0004.2004-08-01.bg\r\n",
      "ham\tham\t0005.1999-12-12.kaminski\r\n",
      "ham\tham\t0005.1999-12-14.farmer\r\n",
      "ham\tham\t0005.2000-06-06.lokay\r\n",
      "ham\tham\t0005.2001-02-08.kitchen\r\n",
      "spam\tspam\t0005.2001-06-23.sa_and_hp\r\n",
      "spam\tspam\t0005.2003-12-18.gp\r\n",
      "ham\tham\t0006.1999-12-13.kaminski\r\n",
      "ham\tham\t0006.2001-02-08.kitchen\r\n",
      "ham\tham\t0006.2001-04-03.williams\r\n",
      "spam\tspam\t0006.2001-06-25.sa_and_hp\r\n",
      "spam\tspam\t0006.2003-12-18.gp\r\n",
      "spam\tspam\t0006.2004-08-01.bg\r\n",
      "ham\tham\t0007.1999-12-13.kaminski\r\n",
      "ham\tham\t0007.1999-12-14.farmer\r\n",
      "ham\tham\t0007.2000-01-17.beck\r\n",
      "ham\tham\t0007.2001-02-09.kitchen\r\n",
      "spam\tspam\t0007.2003-12-18.gp\r\n",
      "spam\tspam\t0007.2004-08-01.bg\r\n",
      "ham\tham\t0008.2001-02-09.kitchen\r\n",
      "spam\tspam\t0008.2001-06-12.sa_and_hp\r\n",
      "spam\tspam\t0008.2001-06-25.sa_and_hp\r\n",
      "spam\tspam\t0008.2003-12-18.gp\r\n",
      "spam\tspam\t0008.2004-08-01.bg\r\n",
      "ham\tham\t0009.1999-12-13.kaminski\r\n",
      "ham\tham\t0009.1999-12-14.farmer\r\n",
      "ham\tham\t0009.2000-06-07.lokay\r\n",
      "ham\tham\t0009.2001-02-09.kitchen\r\n",
      "spam\tspam\t0009.2001-06-26.sa_and_hp\r\n",
      "spam\tspam\t0009.2003-12-18.gp\r\n",
      "ham\tham\t0010.1999-12-14.farmer\r\n",
      "ham\tham\t0010.1999-12-14.kaminski\r\n",
      "ham\tham\t0010.2001-02-09.kitchen\r\n",
      "spam\tspam\t0010.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0010.2003-12-18.gp\r\n",
      "spam\tspam\t0010.2004-08-01.bg\r\n",
      "ham\tham\t0011.1999-12-14.farmer\r\n",
      "spam\tspam\t0011.2001-06-28.sa_and_hp\r\n",
      "spam\tspam\t0011.2001-06-29.sa_and_hp\r\n",
      "spam\tspam\t0011.2003-12-18.gp\r\n",
      "spam\tspam\t0011.2004-08-01.bg\r\n",
      "ham\tham\t0012.1999-12-14.farmer\r\n",
      "ham\tham\t0012.1999-12-14.kaminski\r\n",
      "ham\tham\t0012.2000-01-17.beck\r\n",
      "ham\tham\t0012.2000-06-08.lokay\r\n",
      "ham\tham\t0012.2001-02-09.kitchen\r\n",
      "spam\tspam\t0012.2003-12-19.gp\r\n",
      "ham\tham\t0013.1999-12-14.farmer\r\n",
      "ham\tham\t0013.1999-12-14.kaminski\r\n",
      "ham\tham\t0013.2001-04-03.williams\r\n",
      "spam\tspam\t0013.2001-06-30.sa_and_hp\r\n",
      "spam\tspam\t0013.2004-08-01.bg\r\n",
      "ham\tham\t0014.1999-12-14.kaminski\r\n",
      "ham\tham\t0014.1999-12-15.farmer\r\n",
      "ham\tham\t0014.2001-02-12.kitchen\r\n",
      "spam\tspam\t0014.2001-07-04.sa_and_hp\r\n",
      "spam\tspam\t0014.2003-12-19.gp\r\n",
      "spam\tspam\t0014.2004-08-01.bg\r\n",
      "ham\tham\t0015.1999-12-14.kaminski\r\n",
      "ham\tham\t0015.1999-12-15.farmer\r\n",
      "ham\tham\t0015.2000-06-09.lokay\r\n",
      "ham\tham\t0015.2001-02-12.kitchen\r\n",
      "spam\tspam\t0015.2001-07-05.sa_and_hp\r\n",
      "spam\tspam\t0015.2003-12-19.gp\r\n",
      "ham\tham\t0016.1999-12-15.farmer\r\n",
      "ham\tham\t0016.2001-02-12.kitchen\r\n",
      "spam\tspam\t0016.2001-07-05.sa_and_hp\r\n",
      "spam\tspam\t0016.2001-07-06.sa_and_hp\r\n",
      "spam\tspam\t0016.2003-12-19.gp\r\n",
      "spam\tspam\t0016.2004-08-01.bg\r\n",
      "ham\tham\t0017.1999-12-14.kaminski\r\n",
      "ham\tham\t0017.2000-01-17.beck\r\n",
      "ham\tham\t0017.2001-04-03.williams\r\n",
      "spam\tspam\t0017.2003-12-18.gp\r\n",
      "spam\tspam\t0017.2004-08-01.bg\r\n",
      "spam\tspam\t0017.2004-08-02.bg\r\n",
      "ham\tham\t0018.1999-12-14.kaminski\r\n",
      "spam\tspam\t0018.2001-07-13.sa_and_hp\r\n",
      "spam\tspam\t0018.2003-12-18.gp\r\n",
      "\r\n",
      "Our multinomial NB training error: 0.000000\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"dummy\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.6.* Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "- Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUTH \t MNB_HW1.5 \t MNB_SK \t BNB_SK \t ID\n",
      "ham\tham\tham\tham\t0001.1999-12-10.farmer\n",
      "ham\tham\tham\tham\t0001.1999-12-10.kaminski\n",
      "ham\tham\tham\tham\t0001.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0001.2000-06-06.lokay\n",
      "ham\tham\tham\tham\t0001.2001-02-07.kitchen\n",
      "ham\tham\tham\tham\t0001.2001-04-02.williams\n",
      "ham\tham\tham\tham\t0002.1999-12-13.farmer\n",
      "ham\tham\tham\tham\t0002.2001-02-07.kitchen\n",
      "spam\tspam\tspam\tham\t0002.2001-05-25.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0002.2003-12-18.gp\n",
      "spam\tspam\tspam\tham\t0002.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0003.1999-12-10.kaminski\n",
      "ham\tham\tham\tham\t0003.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0003.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0003.2001-02-08.kitchen\n",
      "spam\tspam\tspam\tham\t0003.2003-12-18.gp\n",
      "spam\tspam\tspam\tham\t0003.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0004.1999-12-10.kaminski\n",
      "ham\tham\tham\tham\t0004.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0004.2001-04-02.williams\n",
      "spam\tspam\tspam\tspam\t0004.2001-06-12.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0004.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0005.1999-12-12.kaminski\n",
      "ham\tham\tham\tham\t0005.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0005.2000-06-06.lokay\n",
      "ham\tham\tham\tham\t0005.2001-02-08.kitchen\n",
      "spam\tspam\tspam\tham\t0005.2001-06-23.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0005.2003-12-18.gp\n",
      "ham\tham\tham\tham\t0006.1999-12-13.kaminski\n",
      "ham\tham\tham\tham\t0006.2001-02-08.kitchen\n",
      "ham\tham\tham\tham\t0006.2001-04-03.williams\n",
      "spam\tspam\tspam\tham\t0006.2001-06-25.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0006.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0006.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0007.1999-12-13.kaminski\n",
      "ham\tham\tham\tham\t0007.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0007.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0007.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0007.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0007.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0008.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0008.2001-06-12.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0008.2001-06-25.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0008.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0008.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0009.1999-12-13.kaminski\n",
      "ham\tham\tham\tham\t0009.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0009.2000-06-07.lokay\n",
      "ham\tham\tham\tham\t0009.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0009.2001-06-26.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0009.2003-12-18.gp\n",
      "ham\tham\tham\tham\t0010.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0010.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0010.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tspam\t0010.2001-06-28.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0010.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0010.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0011.1999-12-14.farmer\n",
      "spam\tspam\tspam\tspam\t0011.2001-06-28.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0011.2001-06-29.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0011.2003-12-18.gp\n",
      "spam\tspam\tspam\tham\t0011.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0012.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0012.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0012.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0012.2000-06-08.lokay\n",
      "ham\tham\tham\tham\t0012.2001-02-09.kitchen\n",
      "spam\tspam\tspam\tham\t0012.2003-12-19.gp\n",
      "ham\tham\tham\tham\t0013.1999-12-14.farmer\n",
      "ham\tham\tham\tham\t0013.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0013.2001-04-03.williams\n",
      "spam\tspam\tspam\tspam\t0013.2001-06-30.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0013.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0014.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0014.1999-12-15.farmer\n",
      "ham\tham\tham\tham\t0014.2001-02-12.kitchen\n",
      "spam\tspam\tspam\tspam\t0014.2001-07-04.sa_and_hp\n",
      "spam\tspam\tspam\tham\t0014.2003-12-19.gp\n",
      "spam\tspam\tspam\tham\t0014.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0015.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0015.1999-12-15.farmer\n",
      "ham\tham\tham\tham\t0015.2000-06-09.lokay\n",
      "ham\tham\tham\tham\t0015.2001-02-12.kitchen\n",
      "spam\tspam\tspam\tspam\t0015.2001-07-05.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0015.2003-12-19.gp\n",
      "ham\tham\tham\tham\t0016.1999-12-15.farmer\n",
      "ham\tham\tham\tham\t0016.2001-02-12.kitchen\n",
      "spam\tspam\tspam\tspam\t0016.2001-07-05.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0016.2001-07-06.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0016.2003-12-19.gp\n",
      "spam\tspam\tspam\tham\t0016.2004-08-01.bg\n",
      "ham\tham\tham\tham\t0017.1999-12-14.kaminski\n",
      "ham\tham\tham\tham\t0017.2000-01-17.beck\n",
      "ham\tham\tham\tham\t0017.2001-04-03.williams\n",
      "spam\tspam\tspam\tham\t0017.2003-12-18.gp\n",
      "spam\tspam\tspam\tspam\t0017.2004-08-01.bg\n",
      "spam\tspam\tspam\tspam\t0017.2004-08-02.bg\n",
      "ham\tham\tham\tham\t0018.1999-12-14.kaminski\n",
      "spam\tspam\tspam\tspam\t0018.2001-07-13.sa_and_hp\n",
      "spam\tspam\tspam\tspam\t0018.2003-12-18.gp\n",
      "\n",
      "Our multinomial NB training error: 0.000000\n",
      "SK- multinomial NB training error: 0.000000\n",
      "SK- Bernoulli   NB training error: 0.160000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "with open('enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "train_label = [msg[1] for msg in emails]\n",
    "train_data = [msg[2]+msg[3] if len(msg)==4 else msg[2] for msg in emails]\n",
    "msg_id = [msg[0].lower() for msg in emails]\n",
    "\n",
    "# feature vectorization\n",
    "uniVectorizer = CountVectorizer()\n",
    "dtmTrain = uniVectorizer.fit_transform(train_data) \n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(dtmTrain, train_label)\n",
    "pred_mnb = mnb.predict(dtmTrain)\n",
    "training_error_mnb = 1.0*sum(pred_mnb != train_label) / len(train_label)\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier from sklearn\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(dtmTrain, train_label)\n",
    "pred_bnb = bnb.predict(dtmTrain)\n",
    "training_error_bnb = 1.0*sum(pred_bnb != train_label) / len(train_label)\n",
    "\n",
    "# multinomial Naive Bayes Classifier from HW1.5\n",
    "!./pNaiveBayes.sh 4 \"dummy\"\n",
    "\n",
    "# load results from HW1.5 and generate comparison matrix\n",
    "print 'TRUTH \\t MNB_HW1.5 \\t MNB_SK \\t BNB_SK \\t ID'\n",
    "with open ('enronemail_1h.txt.output', \"r\") as myfile:  \n",
    "    for line in myfile.readlines():\n",
    "        if line.startswith('ham') or line.startswith('spam'):\n",
    "            result = line.split()            \n",
    "            idx = msg_id.index(result[2])\n",
    "            result.insert(2, 'spam' if pred_mnb[idx]=='1' else 'ham')\n",
    "            result.insert(3, 'spam' if pred_bnb[idx]=='1' else 'ham')\n",
    "            print str.join('\\t', result)\n",
    "            \n",
    "        if line.startswith('Our multinomial NB'):\n",
    "            print '\\n' + line.strip('\\n')     \n",
    "\n",
    "print 'SK- multinomial NB training error: %f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB training error: %f' %training_error_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###*HW 2.6.1. OPTIONAL* (note this exercise is a stretch HW and optional)\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "- Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "- Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW2.7. OPTIONAL* (note this exercise is a stretch HW and optional)\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "- Line 1 contains the subject\n",
    "- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values.\n",
    "\n",
    "###*HW2.8.*\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    "- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    "- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?\n",
    "\n",
    "###*HW2.8.1.*\n",
    "- Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "- Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
