1. install mrjob: sudo pip install mrjob
2. install emacs: sudo yum install emacs
3. NO NEED - copy later: set mrjob.conf: emacs ~/.mrjob.conf
4. TODO LATER: change replication number to 1 for HDFS in hdfs-site.xml
5. NO NEED - check yarn-site.xml
6. NO NEED - open channel
7. NO NEED - open port 19888 for job historyserver: add to 205 group
8. make dir:
 - hdfs dfs -mkdir /user/leiyang
 - mkdir lei
 - cd lei
9. set aws server name in RunPageRank.py, and copy local files to EMR
10. move wiki data to hdfs:
 - aws s3 cp s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt ./
 - aws s3 cp s3://ucb-mids-mls-networks/wikipedia/indices.txt ./
 - hdfs dfs -put all-pages-indexed-out.txt /user/leiyang/
 - hdfs dfs -put indices.txt /user/leiyang/
 - hdfs dfs -put PageRankIndex /user/leiyang/
 - hdfs dfs -put PageRank-test.txt /user/leiyang/
 - rm all-pages-indexed-out.txt
 - rm indices.txt
11. run unit test:
 - python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0.15 -i 2 -d 'hdfs:///user/leiyang/PageRankIndex' -s '0'
12. run 10 iteration job under ~/hadoop/lei/
 - python RunPageRank.py -g 'hdfs:///user/leiyang/all-pages-indexed-out.txt' -j 0.15 -i 10 -d 'hdfs:///user/leiyang/indices.txt' -s '0'
 - python RunPageRank.py -g 'hdfs:///user/leiyang/all-pages-indexed-out.txt' -j 0.15 -i 50 -d 'hdfs:///user/leiyang/indices.txt' -s '0'
 - hdfs dfs -cat /user/leiyang/join/p* > results_wiki_10
 - aws s3 cp results_wiki_10 s3://w261.data/HW9/results_wiki_10 --region 'us-west-2'
13. run 50 iteration job under ~/hadoop/lei/
 - python RunPageRank.py -g 'hdfs:///user/leiyang/all-pages-indexed-out.txt' -j 0.15 -i 40 -d 'hdfs:///user/leiyang/indices.txt' -s '15192277'
 - hdfs dfs -cat /user/leiyang/join/p* > results_wiki_50
 - aws s3 cp results_wiki_50 s3://w261.data/HW9/results_wiki_50 --region 'us-west-2'
14. TODO: add to driver - terminate cluster
 - aws emr terminate-clusters --cluster-ids j-CT2NH23KIIBL

================== screen notes ==================
ctrl+a --> send command to screen
ctrl+a+n --> switch between screens
ctrl+a+c --> add new screen
ctrl+a+d --> detach to screen
screen -r --> reattach to screen
ctrl+a+H --> terminate screen session
================== screen notes ==================

================== mrjob.conf content ==================
runners:
  hadoop:
    hadoop_home: /usr/lib/hadoop/
    strict_protocols: True
    no_output: True
================== mrjob.conf content ==================


================== copy local file ==================
scp -i ~/w205.pem PageRankInit.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem PageRankIter_W.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem PageRankDist_W.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem PageRankSort_W.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem PageRankJoin.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem RunPageRank_W.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem RunPageRank.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem helper.py hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem ./data/PageRank-test.txt hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem ./data/PageRankIndex hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/lei/
scp -i ~/w205.pem .mrjob.conf hadoop@ec2-54-174-116-49.compute-1.amazonaws.com:~/
================== copy local file ==================


- to kill a mapreduce job from yarn: yarn application -kill application_1458336673382_0011

=================== location of hdfs-site.xml ========================

./etc/hadoop/conf.empty/hdfs-site.xml
NOTE: python PageRankIter.py PageRank-test.txt -r 'hadoop' --i 1 --output-dir 's3://w261.data/HW9/test'

=================== location of hdfs-site.xml ========================

=================== open channel ========================
ssh -i w205.pem -N -L 50070:ec2-54-152-233-112.compute-1.amazonaws.com:50070 hadoop@ec2-54-152-233-112.compute-1.amazonaws.com
ssh -i w205.pem -N -L 8890:ec2-54-152-233-112.compute-1.amazonaws.com:8890 hadoop@ec2-54-152-233-112.compute-1.amazonaws.com
=================== open channel ========================

=================== start job history server - not necessary ========================
NOTE: job history server is already started with EMR, just log on is ok, no need to manually start

sudo find / -name "mr-jobhistory-daemon.sh" --> /usr/lib/hadoop-mapreduce/sbin/mr-jobhistory-daemon.sh
sudo find / -name "mapred-site.xml" -->
/etc/hadoop/conf.empty/mapred-site.xml
/var/aws/emr/bigtop-deploy/puppet/modules/hadoop/templates/mapred-site.xml
/var/aws/emr/bigtop-deploy/puppet/modules/ignite_hadoop/templates/mapred-site.xml

/usr/lib/hadoop-mapreduce/sbin/mr-jobhistory-daemon.sh --config /etc/hadoop/conf.empty/ start historyserver


NOTE: local real one - /usr/local/Cellar/hadoop/2.7.1/libexec/sbin/mr-jobhistory-daemon.sh

/etc/hadoop/conf.empty/mapred-site.xml
     /usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver
sudo /usr/lib/hadoop-mapreduce/sbin/mr-jobhistory-daemon.sh --config /etc/hadoop/conf.empty/ start historyserver
=================== start job history server - not necessary ========================

================== misc notes ==================
Note:
Bid cheaper price!
Wait for startup to finish
Use 205 security group to open ports
Set port tunnel
Install mrjob with sudo pip install mrjob
Install emacs with sudo yum install emacs
Set .mrjob.conf for Hadoop home --> may copy from s3
Copy s3 data to local to avoid traffic, using AWS s3 cp command.
possible to mount my own EBS data drive
Move over py files to local --> may also copy from s3
================== misc notes ==================

================== S3 notes ==================
hdfs dfs -cat /user/leiyang/join/p* > results_wiki_10
aws s3 cp results_wiki_10 s3://w261.data/HW9/results_wiki_10 --region 'us-west-2'
