{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply syntactic filters based on POS tags\n",
    "def filter_for_tags(tagged, tags=['NN', 'JJ', 'NNP']):\n",
    "    return [item for item in tagged if item[1] in tags]\n",
    "\n",
    "def normalize(tagged):\n",
    "    return [(item[0].replace('.', ''), item[1]) for item in tagged]\n",
    "\n",
    "def unique_everseen(iterable, key=None):\n",
    "    \"List unique elements, preserving order. Remember all elements ever seen.\"\n",
    "    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
    "    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in itertools.ifilterfalse(seen.__contains__, iterable):\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element\n",
    "\n",
    "def lDistance(firstString, secondString):\n",
    "    \"Function to find the Levenshtein distance between two words/sentences - gotten from http://rosettacode.org/wiki/Levenshtein_distance#Python\"\n",
    "    if len(firstString) > len(secondString):\n",
    "        firstString, secondString = secondString, firstString\n",
    "    distances = range(len(firstString) + 1)\n",
    "    for index2, char2 in enumerate(secondString):\n",
    "        newDistances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(firstString):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1])\n",
    "            else:\n",
    "                newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1])))\n",
    "        distances = newDistances\n",
    "    return distances[-1]\n",
    "\n",
    "def buildGraph(nodes):\n",
    "    \"nodes - list of hashables that represents the nodes of the graph\"\n",
    "    gr = nx.Graph() #initialize an undirected graph\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "    #add edges to the graph (weighted by Levenshtein distance)\n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        secondString = pair[1]\n",
    "        levDistance = lDistance(firstString, secondString)\n",
    "        gr.add_edge(firstString, secondString, weight=levDistance)\n",
    "\n",
    "    return gr\n",
    "\n",
    "def extractKeyphrases(text):\n",
    "    #tokenize the text using nltk\n",
    "    wordTokens = nltk.word_tokenize(text)\n",
    "\n",
    "    #assign POS tags to the words in the text\n",
    "    tagged = nltk.pos_tag(wordTokens)\n",
    "    textlist = [x[0] for x in tagged]\n",
    "    \n",
    "    tagged = filter_for_tags(tagged)\n",
    "    tagged = normalize(tagged)\n",
    "\n",
    "    unique_word_set = unique_everseen([x[0] for x in tagged])\n",
    "    word_set_list = list(unique_word_set)\n",
    "\n",
    "   #this will be used to determine adjacent words in order to construct keyphrases with two words\n",
    "\n",
    "    graph = buildGraph(word_set_list)\n",
    "\n",
    "    #pageRank - initial value of 1.0, error tolerance of 0,0001, \n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    #most important words in ascending order of importance\n",
    "    keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "\n",
    "    #the number of keyphrases returned will be relative to the size of the text (a third of the number of vertices)\n",
    "    aThird = len(word_set_list) / 3\n",
    "    keyphrases = keyphrases[0:aThird+1]\n",
    "\n",
    "    #take keyphrases with multiple words into consideration as done in the paper - if two words are adjacent in the text and are selected as keywords, join them\n",
    "    #together\n",
    "    modifiedKeyphrases = set([])\n",
    "    dealtWith = set([]) #keeps track of individual keywords that have been joined to form a keyphrase\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while j < len(textlist):\n",
    "        firstWord = textlist[i]\n",
    "        secondWord = textlist[j]\n",
    "        if firstWord in keyphrases and secondWord in keyphrases:\n",
    "            keyphrase = firstWord + ' ' + secondWord\n",
    "            modifiedKeyphrases.add(keyphrase)\n",
    "            dealtWith.add(firstWord)\n",
    "            dealtWith.add(secondWord)\n",
    "        else:\n",
    "            if firstWord in keyphrases and firstWord not in dealtWith: \n",
    "                modifiedKeyphrases.add(firstWord)\n",
    "\n",
    "            #if this is the last word in the text, and it is a keyword,\n",
    "            #it definitely has no chance of being a keyphrase at this point    \n",
    "            if j == len(textlist)-1 and secondWord in keyphrases and secondWord not in dealtWith:\n",
    "                modifiedKeyphrases.add(secondWord)\n",
    "        \n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "        \n",
    "    return modifiedKeyphrases\n",
    "\n",
    "def extractSentences(text):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
    "    graph = buildGraph(sentenceTokens)\n",
    "\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    #most important sentences in ascending order of importance\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "\n",
    "    #return a 100 word summary\n",
    "    summary = ' '.join(sentences)\n",
    "    summaryWords = summary.split()\n",
    "    summaryWords = summaryWords[0:101]\n",
    "    summary = ' '.join(summaryWords)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def writeFiles(summary, keyphrases, fileName):\n",
    "    \"outputs the keyphrases and summaries to appropriate files\"\n",
    "    print \"Generating output to \" + 'keywords/' + fileName\n",
    "    keyphraseFile = io.open('keywords/' + fileName, 'w')\n",
    "    for keyphrase in keyphrases:\n",
    "        keyphraseFile.write(keyphrase + '\\n')\n",
    "    keyphraseFile.close()\n",
    "\n",
    "    print \"Generating output to \" + 'summaries/' + fileName\n",
    "    summaryFile = io.open('summaries/' + fileName, 'w')\n",
    "    summaryFile.write(summary)\n",
    "    summaryFile.close()\n",
    "\n",
    "    print \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have not downloaded nltk packages please run the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading articles/1.txt\n",
      "Generating output to keywords/1.txt\n",
      "Generating output to summaries/1.txt\n",
      "-\n",
      "Reading articles/10.txt\n",
      "Generating output to keywords/10.txt\n",
      "Generating output to summaries/10.txt\n",
      "-\n",
      "Reading articles/2.txt\n",
      "Generating output to keywords/2.txt\n",
      "Generating output to summaries/2.txt\n",
      "-\n",
      "Reading articles/3.txt\n",
      "Generating output to keywords/3.txt\n",
      "Generating output to summaries/3.txt\n",
      "-\n",
      "Reading articles/4.txt\n",
      "Generating output to keywords/4.txt\n",
      "Generating output to summaries/4.txt\n",
      "-\n",
      "Reading articles/5.txt\n",
      "Generating output to keywords/5.txt\n",
      "Generating output to summaries/5.txt\n",
      "-\n",
      "Reading articles/6.txt\n",
      "Generating output to keywords/6.txt\n",
      "Generating output to summaries/6.txt\n",
      "-\n",
      "Reading articles/7.txt\n",
      "Generating output to keywords/7.txt\n",
      "Generating output to summaries/7.txt\n",
      "-\n",
      "Reading articles/8.txt\n",
      "Generating output to keywords/8.txt\n",
      "Generating output to summaries/8.txt\n",
      "-\n",
      "Reading articles/9.txt\n",
      "Generating output to keywords/9.txt\n",
      "Generating output to summaries/9.txt\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "articles = os.listdir(\"articles\")\n",
    "for article in articles:\n",
    "    print 'Reading articles/' + article\n",
    "    articleFile = io.open('articles/' + article, 'r')\n",
    "    text = articleFile.read()\n",
    "    keyphrases = extractKeyphrases(text)\n",
    "    summary = extractSentences(text)\n",
    "    writeFiles(summary, keyphrases, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
