{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#4\n",
    "####[Lei Yang](mailto:leiyang@berkeley.edu) | [Michael Kennedy](mailto:mkennedy@ischool.berkeley.edu) | [Natarajan Krishnaswami](mailto:natarajan@krishnaswami.org)\n",
    "####Due: 2016-02-011, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW4.0.* Q&A\n",
    "\n",
    "####What is MrJob? How is it different to Hadoop MapReduce?\n",
    "\n",
    "####What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?\n",
    "\n",
    "- **mapper_final()**: When Hadoop Streaming stops sending data to the map task, mrjob calls mapper_final(). That function emits the local aggregation result for this task, which is a much smaller set of output lines than the mapper would have output.\n",
    "- **combiner_final()**: when Hadoop Streaming stops sending data to the combine task\n",
    "- **reducer_final()**: when Hadoop Streaming stops sending data to the reduce task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 4.1.* Q&A\n",
    "####What is serialization in the context of MrJob or Hadoop?\n",
    "\n",
    "####When it used in these frameworks?\n",
    "\n",
    "####What is the default serialization mode for input and outputs for MrJob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-resourcemanager-Leis-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-nodemanager-Leis-MacBook-Pro.local.out\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n",
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/mapred-leiyang-historyserver-Leis-MacBook-Pro.local.out\n",
      "16/02/08 09:34:17 INFO hs.JobHistoryServer: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting JobHistoryServer\n",
      "STARTUP_MSG:   host = leis-macbook-pro.local/192.168.0.12\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.7.1\n",
      "STARTUP_MSG:   classpath = /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/modules/*.jar\n",
      "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z\n",
      "STARTUP_MSG:   java = 1.7.0_79\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 4.2:* \n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "- https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "- http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "**Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:**\n",
    "\n",
    "- C,\"10001\",10001   #Visitor id 10001\n",
    "- V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "- V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "- V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "- C,\"10002\",10002   #Visitor id 10001\n",
    "- V\n",
    "\n",
    "*Note: #denotes comments*\n",
    "\n",
    "**to the format:**\n",
    "\n",
    "- V,1000,1,C, 10001\n",
    "- V,1001,1,C, 10001\n",
    "- V,1002,1,C, 10001\n",
    "\n",
    "**Write the python code to accomplish this.**\n",
    "\n",
    "###MrJob:\n",
    "- no shuffling/sorting is needed for this task\n",
    "- thus no reducer, only single mapper to preserve the order of C & V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_2.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class ConvertLog(MRJob):\n",
    "    # visitor name\n",
    "    visitor = None\n",
    "    # mapper\n",
    "    def convert_mapper(self, _, line):        \n",
    "        # time of mapper being called\n",
    "        self.increment_counter('HW4_2', 'lines', 1)\n",
    "        # only emit lines start with C and V\n",
    "        line = line.strip()\n",
    "        if line[0] not in ['C', 'V']:\n",
    "            return\n",
    "        # process C and V lines\n",
    "        if line[0] == 'C':            \n",
    "            # get the latest visitor suffix\n",
    "            self.visitor = 'C,%s' %line.split(',')[2]            \n",
    "        else:\n",
    "            # emit the desire output, no need for key\n",
    "            yield None, '%s,%s' %(line, self.visitor)\n",
    "    \n",
    "    # MapReduce steps\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.convert_mapper)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ConvertLog.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execute MrJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_2.leiyang.20160207.164624.031610\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_2.leiyang.20160207.164624.031610/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  HW4_2:\n",
      "    lines: 131666\n",
      "Moving /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_2.leiyang.20160207.164624.031610/step-0-mapper_part-00000 -> /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_2.leiyang.20160207.164624.031610/output/part-00000\n",
      "Streaming final output from /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_2.leiyang.20160207.164624.031610/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_2.leiyang.20160207.164624.031610\n",
      "null\t\"V,1000,1,C,10001\"\n",
      "null\t\"V,1001,1,C,10001\"\n",
      "null\t\"V,1002,1,C,10001\"\n",
      "null\t\"V,1001,1,C,10002\"\n",
      "null\t\"V,1003,1,C,10002\"\n",
      "null\t\"V,1001,1,C,10003\"\n",
      "null\t\"V,1003,1,C,10003\"\n",
      "null\t\"V,1004,1,C,10003\"\n",
      "null\t\"V,1005,1,C,10004\"\n",
      "null\t\"V,1006,1,C,10005\"\n"
     ]
    }
   ],
   "source": [
    "# run the job locally\n",
    "!python HW4_2.py anonymous-msweb.data > HW4_2_results\n",
    "# results sample\n",
    "!head -10 HW4_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW4.3:* Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).\n",
    "\n",
    "###MrJob Steps:\n",
    "- mapper 1: emit V~count pairs\n",
    "- reducer 1: count visit times n for each page V\n",
    "- reducer 2: get 5 most frequently visited pages, with n sorted reversely on numeric order by the partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_3.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class FreqVisitPage(MRJob):\n",
    "    n_freq, i = 5, 0\n",
    "        \n",
    "    def mapper_count(self, dummy, line): \n",
    "        self.increment_counter('HW4_3','page_map',1)\n",
    "        # get page id\n",
    "        pID = line.strip().split(',')[1]\n",
    "        yield pID.strip(), 1\n",
    "        \n",
    "    def reducer_count(self, page, count):\n",
    "        self.increment_counter('HW4_3','page_count',1)\n",
    "        yield page, sum(count)\n",
    "        \n",
    "    def mapper_sort(self, page, count):        \n",
    "        yield (page, count), None\n",
    "        \n",
    "    def reducer_sort(self, key, _):\n",
    "        self.increment_counter('HW4_3','page_sort',1)\n",
    "        if self.i < self.n_freq:\n",
    "            self.i += 1\n",
    "            yield key        \n",
    "        \n",
    "    def steps(self):             \n",
    "        sort_jobconf = {  #key value pairs            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',\n",
    "        }\n",
    "        \n",
    "        count_jobconf = {\n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '2',\n",
    "        }\n",
    "        \n",
    "        return [MRStep(mapper=self.mapper_count, reducer=self.reducer_count, jobconf=count_jobconf)\n",
    "                ,MRStep(mapper=self.mapper_sort, reducer=self.reducer_sort, jobconf=sort_jobconf)\n",
    "               ]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FreqVisitPage.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_3.leiyang.20160207.205944.473167\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_3.leiyang.20160207.205944.473167/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW4_3.leiyang.20160207.205944.473167/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar8087091006866532002/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob2221887976295196557.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar4072438874045359374/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6375479652895239471.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW4_3.leiyang.20160207.205944.473167/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_3.leiyang.20160207.205944.473167\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW4_3.leiyang.20160207.205944.473167 from HDFS\n",
      "\"1008\"\t10836\n",
      "\"1034\"\t9383\n",
      "\"1004\"\t8463\n",
      "\"1018\"\t5330\n",
      "\"1017\"\t5108\n"
     ]
    }
   ],
   "source": [
    "### running the job locally\n",
    "#!python HW4_3.py HW4_2_results > HW4_3_results\n",
    "\n",
    "### running the job on hadoop\n",
    "!python HW4_3.py HW4_2_results --hadoop-home '/usr/local/Cellar/hadoop/2.7.1/' -r hadoop > HW4_3_results\n",
    "\n",
    "### results\n",
    "!cat HW4_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW4.4:* Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.\n",
    "\n",
    "**Dev Notes:**\n",
    "- mapper 1: repeat HW4.2 mapper to conver log format first\n",
    "- mapper 2: emit dummy key for webpage item, such that it alway comes immediately before website/visitor lines\n",
    " - for rows start with A, key is \n",
    " - partitioner: sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_4.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class FreqVisitor(MRJob):\n",
    "    \n",
    "    # member variables: visitor ID and url\n",
    "    url = visitorID = None\n",
    "    current_page = None\n",
    "    current_max = 0\n",
    "    \n",
    "    # 1. mapper\n",
    "    def convert_mapper(self, _, line):        \n",
    "        # time of mapper being called\n",
    "        self.increment_counter('HW4_2', 'lines', 1)\n",
    "        # only emit lines start with C and V\n",
    "        line = line.strip()\n",
    "        if line[0] not in ['C', 'V', 'A']:\n",
    "            return\n",
    "        temp = line.split(',')\n",
    "        # process A, C, and V lines\n",
    "        if line[0] == 'C':            \n",
    "            # get the latest visitor ID\n",
    "            self.visitorID = temp[2]  \n",
    "        elif line[0] == 'A':\n",
    "            # emit V_pageID_*_url as key, dummy 1 as value\n",
    "            yield 'V_%s_*_%s' %(temp[1], temp[4].strip('\"')), 1\n",
    "        else:\n",
    "            # emit V_pageID_C_visitorID as key, 1 as value\n",
    "            yield 'V_%s_C_%s' %(temp[1], self.visitorID), 1\n",
    "     \n",
    "    # 2. reducer to get count for each visitor on each page\n",
    "    def count_reducer(self, key, value):     \n",
    "        temp = key.strip().split('_')\n",
    "        # save webpage url for the following visisting records\n",
    "        if temp[2] == '*':\n",
    "            self.url = temp[3]\n",
    "        else:\n",
    "            yield key+'_'+self.url, sum(value)\n",
    "            \n",
    "    # 3. mapper for sorting: partition by page id, secondary sorting/ranking by count\n",
    "    def rank_mapper(self, key, count):   \n",
    "        v, pID, c, cID, url = key.strip().split('_')\n",
    "        yield 'V_%s' %pID, (count, 'C_%s - URL: %s' %(cID, url))\n",
    "    \n",
    "    # 4. reducer get max vistor of each page\n",
    "    # NOTE: this implementation doesn't show all ties, just one of the record with the biggest count\n",
    "    def rank_reducer(self, key, value):\n",
    "        # most frequent vistor of the webpage\n",
    "        yield key, max(value)\n",
    "           \n",
    "    \n",
    "    # 0. MapReduce steps\n",
    "    def steps(self):\n",
    "        count_jobconf = {  #key value pairs                        \n",
    "            'mapreduce.job.maps': '1',\n",
    "            'mapreduce.job.reduces': '1', # must only use 1 reducer to have the proper order\n",
    "        }\n",
    "        \n",
    "        rank_jobconf = {\n",
    "            #'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            #'mapreduce.partition.keycomparator.options': '-k1,1r', # reverse order page ID '-k2,2nr',\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            #'stream.num.map.output.key.fields': '2',\n",
    "            #'mapreduce.map.output.key.field.separator': '',\n",
    "            #'stream.map.output.field.separator': ' ',            \n",
    "        }\n",
    "        return [MRStep(mapper=self.convert_mapper, reducer=self.count_reducer, jobconf=count_jobconf)\n",
    "                ,MRStep(mapper=self.rank_mapper, reducer=self.rank_reducer, jobconf=rank_jobconf)\n",
    "               ]\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    FreqVisitor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_4.leiyang.20160207.221613.945000\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_4.leiyang.20160207.221613.945000/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/HW4_4.leiyang.20160207.221613.945000/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar8262919340003780482/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob8665858738147888764.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar4820819124527597538/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob6003131137893457708.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/HW4_4.leiyang.20160207.221613.945000/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/HW4_4.leiyang.20160207.221613.945000\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/HW4_4.leiyang.20160207.221613.945000 from HDFS\n",
      "\"V_1000\"\t[1, \"C_42679 - URL: /regwiz\"]\n",
      "\"V_1001\"\t[1, \"C_42710 - URL: /support\"]\n",
      "\"V_1002\"\t[1, \"C_42592 - URL: /athome\"]\n",
      "\"V_1003\"\t[1, \"C_42709 - URL: /kb\"]\n",
      "\"V_1004\"\t[1, \"C_42707 - URL: /search\"]\n",
      "\"V_1005\"\t[1, \"C_42698 - URL: /norge\"]\n",
      "\"V_1006\"\t[1, \"C_42612 - URL: /misc\"]\n",
      "\"V_1007\"\t[1, \"C_42664 - URL: /ie\"]\n",
      "\"V_1008\"\t[1, \"C_42711 - URL: /msdownload\"]\n",
      "\"V_1009\"\t[1, \"C_42707 - URL: /windows\"]\n",
      "\"V_1010\"\t[1, \"C_42698 - URL: /vbasic\"]\n",
      "\"V_1011\"\t[1, \"C_42557 - URL: /officedev\"]\n",
      "\"V_1012\"\t[1, \"C_42650 - URL: /outlookdev\"]\n",
      "\"V_1013\"\t[1, \"C_42698 - URL: /vbasicsupport\"]\n",
      "\"V_1014\"\t[1, \"C_42698 - URL: /officefreestuff\"]\n",
      "\"V_1015\"\t[1, \"C_42626 - URL: /msexcel\"]\n",
      "\"V_1016\"\t[1, \"C_42626 - URL: /excel\"]\n",
      "\"V_1017\"\t[1, \"C_42692 - URL: /products\"]\n",
      "\"V_1018\"\t[1, \"C_42710 - URL: /isapi\"]\n",
      "\"V_1019\"\t[1, \"C_42311 - URL: /mspowerpoint\"]\n",
      "\"V_1020\"\t[1, \"C_42694 - URL: /msdn\"]\n",
      "\"V_1021\"\t[1, \"C_42523 - URL: /visualc\"]\n",
      "\"V_1022\"\t[1, \"C_42574 - URL: /truetype\"]\n",
      "\"V_1023\"\t[1, \"C_42665 - URL: /spain\"]\n",
      "\"V_1024\"\t[1, \"C_42692 - URL: /iis\"]\n",
      "\"V_1025\"\t[1, \"C_42657 - URL: /gallery\"]\n",
      "\"V_1026\"\t[1, \"C_42708 - URL: /sitebuilder\"]\n",
      "\"V_1027\"\t[1, \"C_42708 - URL: /intdev\"]\n",
      "\"V_1028\"\t[1, \"C_42063 - URL: /oledev\"]\n",
      "\"V_1029\"\t[1, \"C_42675 - URL: /clipgallerylive\"]\n",
      "\"V_1030\"\t[1, \"C_42707 - URL: /ntserver\"]\n",
      "\"V_1031\"\t[1, \"C_42667 - URL: /msoffice\"]\n",
      "\"V_1032\"\t[1, \"C_42700 - URL: /games\"]\n",
      "\"V_1033\"\t[1, \"C_41867 - URL: /logostore\"]\n",
      "\"V_1034\"\t[1, \"C_42705 - URL: /ie\"]\n",
      "\"V_1035\"\t[1, \"C_42710 - URL: /windowssupport\"]\n",
      "\"V_1036\"\t[1, \"C_42629 - URL: /organizations\"]\n",
      "\"V_1037\"\t[1, \"C_42650 - URL: /windows95\"]\n",
      "\"V_1038\"\t[1, \"C_42708 - URL: /sbnmember\"]\n",
      "\"V_1039\"\t[1, \"C_42656 - URL: /isp\"]\n",
      "\"V_1040\"\t[1, \"C_42698 - URL: /office\"]\n",
      "\"V_1041\"\t[1, \"C_42708 - URL: /workshop\"]\n",
      "\"V_1042\"\t[1, \"C_42467 - URL: /vstudio\"]\n",
      "\"V_1043\"\t[1, \"C_42626 - URL: /smallbiz\"]\n",
      "\"V_1044\"\t[1, \"C_42673 - URL: /mediadev\"]\n",
      "\"V_1045\"\t[1, \"C_42633 - URL: /netmeeting\"]\n",
      "\"V_1046\"\t[1, \"C_42702 - URL: /iesupport\"]\n",
      "\"V_1048\"\t[1, \"C_42541 - URL: /publisher\"]\n",
      "\"V_1049\"\t[1, \"C_42539 - URL: /supportnet\"]\n",
      "\"V_1050\"\t[1, \"C_42525 - URL: /macoffice\"]\n",
      "\"V_1051\"\t[1, \"C_41980 - URL: /scheduleplus\"]\n",
      "\"V_1052\"\t[1, \"C_42665 - URL: /word\"]\n",
      "\"V_1053\"\t[1, \"C_42688 - URL: /visualj\"]\n",
      "\"V_1054\"\t[1, \"C_42577 - URL: /exchange\"]\n",
      "\"V_1055\"\t[1, \"C_42424 - URL: /kids\"]\n",
      "\"V_1056\"\t[1, \"C_42507 - URL: /sports\"]\n",
      "\"V_1057\"\t[1, \"C_42667 - URL: /powerpoint\"]\n",
      "\"V_1058\"\t[1, \"C_42707 - URL: /referral\"]\n",
      "\"V_1059\"\t[1, \"C_42706 - URL: /sverige\"]\n",
      "\"V_1060\"\t[1, \"C_42659 - URL: /msword\"]\n",
      "\"V_1061\"\t[1, \"C_42576 - URL: /promo\"]\n",
      "\"V_1062\"\t[1, \"C_42331 - URL: /msaccess\"]\n",
      "\"V_1063\"\t[1, \"C_42337 - URL: /intranet\"]\n",
      "\"V_1064\"\t[1, \"C_42704 - URL: /activeplatform\"]\n",
      "\"V_1065\"\t[1, \"C_42641 - URL: /java\"]\n",
      "\"V_1066\"\t[1, \"C_42260 - URL: /musicproducer\"]\n",
      "\"V_1067\"\t[1, \"C_42456 - URL: /frontpage\"]\n",
      "\"V_1068\"\t[1, \"C_42025 - URL: /vbscript\"]\n",
      "\"V_1069\"\t[1, \"C_42613 - URL: /windowsce\"]\n",
      "\"V_1070\"\t[1, \"C_42692 - URL: /activex\"]\n",
      "\"V_1071\"\t[1, \"C_42621 - URL: /automap\"]\n",
      "\"V_1072\"\t[1, \"C_42446 - URL: /vinterdev\"]\n",
      "\"V_1073\"\t[1, \"C_42469 - URL: /taiwan\"]\n",
      "\"V_1074\"\t[1, \"C_42597 - URL: /ntworkstation\"]\n",
      "\"V_1075\"\t[1, \"C_42622 - URL: /jobs\"]\n",
      "\"V_1076\"\t[1, \"C_42701 - URL: /ntwkssupport\"]\n",
      "\"V_1077\"\t[1, \"C_42626 - URL: /msofficesupport\"]\n",
      "\"V_1078\"\t[1, \"C_42643 - URL: /ntserversupport\"]\n",
      "\"V_1079\"\t[1, \"C_42572 - URL: /australia\"]\n",
      "\"V_1080\"\t[1, \"C_42432 - URL: /brasil\"]\n",
      "\"V_1081\"\t[1, \"C_42694 - URL: /accessdev\"]\n",
      "\"V_1082\"\t[1, \"C_42240 - URL: /access\"]\n",
      "\"V_1083\"\t[1, \"C_42448 - URL: /msaccesssupport\"]\n",
      "\"V_1084\"\t[1, \"C_42516 - URL: /uk\"]\n",
      "\"V_1085\"\t[1, \"C_42577 - URL: /exchangesupport\"]\n",
      "\"V_1086\"\t[1, \"C_40233 - URL: /oem\"]\n",
      "\"V_1087\"\t[1, \"C_42577 - URL: /proxy\"]\n",
      "\"V_1088\"\t[1, \"C_42650 - URL: /outlook\"]\n",
      "\"V_1089\"\t[1, \"C_42598 - URL: /officereference\"]\n",
      "\"V_1090\"\t[1, \"C_42445 - URL: /gamessupport\"]\n",
      "\"V_1091\"\t[1, \"C_42650 - URL: /hwdev\"]\n",
      "\"V_1092\"\t[1, \"C_41717 - URL: /vfoxpro\"]\n",
      "\"V_1093\"\t[1, \"C_42025 - URL: /vba\"]\n",
      "\"V_1094\"\t[1, \"C_39554 - URL: /mshome\"]\n",
      "\"V_1095\"\t[1, \"C_42234 - URL: /catalog\"]\n",
      "\"V_1096\"\t[1, \"C_42566 - URL: /mspress\"]\n",
      "\"V_1097\"\t[1, \"C_42435 - URL: /latam\"]\n",
      "\"V_1098\"\t[1, \"C_42616 - URL: /devonly\"]\n",
      "\"V_1099\"\t[1, \"C_42543 - URL: /cio\"]\n",
      "\"V_1100\"\t[1, \"C_42568 - URL: /education\"]\n",
      "\"V_1101\"\t[1, \"C_41626 - URL: /oledb\"]\n",
      "\"V_1102\"\t[1, \"C_42546 - URL: /homeessentials\"]\n",
      "\"V_1103\"\t[1, \"C_41711 - URL: /works\"]\n",
      "\"V_1104\"\t[1, \"C_41560 - URL: /hk\"]\n",
      "\"V_1105\"\t[1, \"C_42281 - URL: /france\"]\n",
      "\"V_1106\"\t[1, \"C_41001 - URL: /cze\"]\n",
      "\"V_1107\"\t[1, \"C_38331 - URL: /slovakia\"]\n",
      "\"V_1108\"\t[1, \"C_42598 - URL: /teammanager\"]\n",
      "\"V_1109\"\t[1, \"C_42537 - URL: /technet\"]\n",
      "\"V_1110\"\t[1, \"C_41897 - URL: /mastering\"]\n",
      "\"V_1111\"\t[1, \"C_41318 - URL: /ssafe\"]\n",
      "\"V_1112\"\t[1, \"C_42445 - URL: /canada\"]\n",
      "\"V_1113\"\t[1, \"C_42682 - URL: /security\"]\n",
      "\"V_1114\"\t[1, \"C_41916 - URL: /servad\"]\n",
      "\"V_1115\"\t[1, \"C_36277 - URL: /hun\"]\n",
      "\"V_1116\"\t[1, \"C_40678 - URL: /switzerland\"]\n",
      "\"V_1117\"\t[1, \"C_41101 - URL: /sidewinder\"]\n",
      "\"V_1118\"\t[1, \"C_42598 - URL: /sql\"]\n",
      "\"V_1119\"\t[1, \"C_42453 - URL: /corpinfo\"]\n",
      "\"V_1120\"\t[1, \"C_10241 - URL: /switch\"]\n",
      "\"V_1121\"\t[1, \"C_41556 - URL: /magazine\"]\n",
      "\"V_1122\"\t[1, \"C_41995 - URL: /mindshare\"]\n",
      "\"V_1123\"\t[1, \"C_42708 - URL: /germany\"]\n",
      "\"V_1124\"\t[1, \"C_42667 - URL: /industry\"]\n",
      "\"V_1125\"\t[1, \"C_42237 - URL: /imagecomposer\"]\n",
      "\"V_1126\"\t[1, \"C_42030 - URL: /mediamanager\"]\n",
      "\"V_1127\"\t[1, \"C_42699 - URL: /netshow\"]\n",
      "\"V_1128\"\t[1, \"C_10286 - URL: /msf\"]\n",
      "\"V_1129\"\t[1, \"C_27780 - URL: /ado\"]\n",
      "\"V_1130\"\t[1, \"C_42603 - URL: /syspro\"]\n",
      "\"V_1131\"\t[1, \"C_42418 - URL: /moneyzone\"]\n",
      "\"V_1132\"\t[1, \"C_40053 - URL: /msmoneysupport\"]\n",
      "\"V_1133\"\t[1, \"C_41992 - URL: /frontpagesupport\"]\n",
      "\"V_1134\"\t[1, \"C_42479 - URL: /backoffice\"]\n",
      "\"V_1135\"\t[1, \"C_42659 - URL: /mswordsupport\"]\n",
      "\"V_1136\"\t[1, \"C_42364 - URL: /usa\"]\n",
      "\"V_1137\"\t[1, \"C_42704 - URL: /mscorp\"]\n",
      "\"V_1138\"\t[1, \"C_42572 - URL: /mind\"]\n",
      "\"V_1139\"\t[1, \"C_41482 - URL: /k-12\"]\n",
      "\"V_1140\"\t[1, \"C_42678 - URL: /netherlands\"]\n",
      "\"V_1141\"\t[1, \"C_42605 - URL: /europe\"]\n",
      "\"V_1142\"\t[1, \"C_42467 - URL: /southafrica\"]\n",
      "\"V_1143\"\t[1, \"C_42286 - URL: /workshoop\"]\n",
      "\"V_1144\"\t[1, \"C_41640 - URL: /devnews\"]\n",
      "\"V_1145\"\t[1, \"C_39965 - URL: /vfoxprosupport\"]\n",
      "\"V_1146\"\t[1, \"C_42269 - URL: /msp\"]\n",
      "\"V_1147\"\t[1, \"C_42555 - URL: /msft\"]\n",
      "\"V_1148\"\t[1, \"C_42697 - URL: /channel\"]\n",
      "\"V_1149\"\t[1, \"C_39863 - URL: /adc\"]\n",
      "\"V_1150\"\t[1, \"C_42197 - URL: /infoserv\"]\n",
      "\"V_1151\"\t[1, \"C_41774 - URL: /mspowerpointsupport\"]\n",
      "\"V_1152\"\t[1, \"C_42312 - URL: /rus\"]\n",
      "\"V_1153\"\t[1, \"C_39053 - URL: /venezuela\"]\n",
      "\"V_1154\"\t[1, \"C_42467 - URL: /project\"]\n",
      "\"V_1155\"\t[1, \"C_42353 - URL: /sidewalk\"]\n",
      "\"V_1156\"\t[1, \"C_41710 - URL: /powered\"]\n",
      "\"V_1157\"\t[1, \"C_42001 - URL: /win32dev\"]\n",
      "\"V_1158\"\t[1, \"C_42705 - URL: /imedia\"]\n",
      "\"V_1159\"\t[1, \"C_41444 - URL: /transaction\"]\n",
      "\"V_1160\"\t[1, \"C_41764 - URL: /visualcsupport\"]\n",
      "\"V_1161\"\t[1, \"C_42263 - URL: /workssupport\"]\n",
      "\"V_1162\"\t[1, \"C_42285 - URL: /infoservsupport\"]\n",
      "\"V_1163\"\t[1, \"C_40475 - URL: /opentype\"]\n",
      "\"V_1164\"\t[1, \"C_41702 - URL: /smsmgmt\"]\n",
      "\"V_1165\"\t[1, \"C_42007 - URL: /poland\"]\n",
      "\"V_1166\"\t[1, \"C_41248 - URL: /mexico\"]\n",
      "\"V_1167\"\t[1, \"C_42650 - URL: /hwtest\"]\n",
      "\"V_1168\"\t[1, \"C_42646 - URL: /salesinfo\"]\n",
      "\"V_1169\"\t[1, \"C_42642 - URL: /msproject\"]\n",
      "\"V_1170\"\t[1, \"C_41306 - URL: /mail\"]\n",
      "\"V_1171\"\t[1, \"C_42374 - URL: /merchant\"]\n",
      "\"V_1172\"\t[1, \"C_42129 - URL: /belgium\"]\n",
      "\"V_1173\"\t[1, \"C_31767 - URL: /moli\"]\n",
      "\"V_1174\"\t[1, \"C_40040 - URL: /nz\"]\n",
      "\"V_1175\"\t[1, \"C_41737 - URL: /msprojectsupport\"]\n",
      "\"V_1176\"\t[1, \"C_41737 - URL: /jscript\"]\n",
      "\"V_1177\"\t[1, \"C_42437 - URL: /events\"]\n",
      "\"V_1178\"\t[1, \"C_31500 - URL: /msdownload.\"]\n",
      "\"V_1179\"\t[1, \"C_41490 - URL: /colombia\"]\n",
      "\"V_1180\"\t[1, \"C_35728 - URL: /slovenija\"]\n",
      "\"V_1181\"\t[1, \"C_42083 - URL: /kidssupport\"]\n",
      "\"V_1182\"\t[1, \"C_38633 - URL: /fortran\"]\n",
      "\"V_1183\"\t[1, \"C_42613 - URL: /italy\"]\n",
      "\"V_1184\"\t[1, \"C_42626 - URL: /msexcelsupport\"]\n",
      "\"V_1185\"\t[1, \"C_41832 - URL: /sna\"]\n",
      "\"V_1186\"\t[1, \"C_42539 - URL: /college\"]\n",
      "\"V_1187\"\t[1, \"C_42036 - URL: /odbc\"]\n",
      "\"V_1188\"\t[1, \"C_42506 - URL: /korea\"]\n",
      "\"V_1189\"\t[1, \"C_41768 - URL: /internet\"]\n",
      "\"V_1190\"\t[1, \"C_41570 - URL: /repository\"]\n",
      "\"V_1191\"\t[1, \"C_41812 - URL: /management\"]\n",
      "\"V_1192\"\t[1, \"C_38976 - URL: /visualjsupport\"]\n",
      "\"V_1193\"\t[1, \"C_41518 - URL: /offdevsupport\"]\n",
      "\"V_1194\"\t[1, \"C_40708 - URL: /china\"]\n",
      "\"V_1195\"\t[1, \"C_40458 - URL: /portugal\"]\n",
      "\"V_1196\"\t[1, \"C_11431 - URL: /ie40\"]\n",
      "\"V_1197\"\t[1, \"C_42285 - URL: /sqlsupport\"]\n",
      "\"V_1198\"\t[1, \"C_40310 - URL: /pictureit\"]\n",
      "\"V_1199\"\t[1, \"C_11644 - URL: /feedback\"]\n",
      "\"V_1200\"\t[1, \"C_42451 - URL: /benelux\"]\n",
      "\"V_1201\"\t[1, \"C_42203 - URL: /hardware\"]\n",
      "\"V_1202\"\t[1, \"C_41172 - URL: /advtech\"]\n",
      "\"V_1203\"\t[1, \"C_42518 - URL: /danmark\"]\n",
      "\"V_1204\"\t[1, \"C_40792 - URL: /msscheduleplus\"]\n",
      "\"V_1205\"\t[1, \"C_41597 - URL: /hardwaresupport\"]\n",
      "\"V_1206\"\t[1, \"C_42321 - URL: /select\"]\n",
      "\"V_1207\"\t[1, \"C_42008 - URL: /icp\"]\n",
      "\"V_1208\"\t[1, \"C_41548 - URL: /israel\"]\n",
      "\"V_1209\"\t[1, \"C_42513 - URL: /turkey\"]\n",
      "\"V_1210\"\t[1, \"C_31871 - URL: /snasupport\"]\n",
      "\"V_1211\"\t[1, \"C_42344 - URL: /smsmgmtsupport\"]\n",
      "\"V_1212\"\t[1, \"C_42071 - URL: /worldwide\"]\n",
      "\"V_1213\"\t[1, \"C_37985 - URL: /corporate\"]\n",
      "\"V_1214\"\t[1, \"C_35031 - URL: /finserv\"]\n",
      "\"V_1215\"\t[1, \"C_42572 - URL: /developer\"]\n",
      "\"V_1216\"\t[1, \"C_41329 - URL: /vrml\"]\n",
      "\"V_1217\"\t[1, \"C_38711 - URL: /ireland\"]\n",
      "\"V_1218\"\t[1, \"C_42541 - URL: /publishersupport\"]\n",
      "\"V_1219\"\t[1, \"C_20439 - URL: /ads\"]\n",
      "\"V_1220\"\t[1, \"C_41611 - URL: /macofficesupport\"]\n",
      "\"V_1221\"\t[1, \"C_41273 - URL: /mstv\"]\n",
      "\"V_1222\"\t[1, \"C_42103 - URL: /msofc\"]\n",
      "\"V_1223\"\t[1, \"C_42649 - URL: /finland\"]\n",
      "\"V_1224\"\t[1, \"C_40025 - URL: /atec\"]\n",
      "\"V_1225\"\t[1, \"C_42453 - URL: /piracy\"]\n",
      "\"V_1226\"\t[1, \"C_41980 - URL: /msschedplussupport\"]\n",
      "\"V_1227\"\t[1, \"C_42435 - URL: /argentina\"]\n",
      "\"V_1228\"\t[1, \"C_40882 - URL: /vtest\"]\n",
      "\"V_1229\"\t[1, \"C_26913 - URL: /uruguay\"]\n",
      "\"V_1230\"\t[1, \"C_40928 - URL: /mailsupport\"]\n",
      "\"V_1231\"\t[1, \"C_41626 - URL: /win32devsupport\"]\n",
      "\"V_1232\"\t[1, \"C_37637 - URL: /standards\"]\n",
      "\"V_1233\"\t[1, \"C_14363 - URL: /vbscripts\"]\n",
      "\"V_1234\"\t[1, \"C_42626 - URL: /off97cat\"]\n",
      "\"V_1235\"\t[1, \"C_37345 - URL: /onlineeval\"]\n",
      "\"V_1236\"\t[1, \"C_38325 - URL: /globaldev\"]\n",
      "\"V_1237\"\t[1, \"C_32583 - URL: /devdays\"]\n",
      "\"V_1238\"\t[1, \"C_26885 - URL: /exceldev\"]\n",
      "\"V_1239\"\t[1, \"C_38020 - URL: /msconsult\"]\n",
      "\"V_1240\"\t[1, \"C_39891 - URL: /thailand\"]\n",
      "\"V_1241\"\t[1, \"C_34793 - URL: /india\"]\n",
      "\"V_1242\"\t[1, \"C_41937 - URL: /msgarden\"]\n",
      "\"V_1243\"\t[1, \"C_28084 - URL: /usability\"]\n",
      "\"V_1244\"\t[1, \"C_41835 - URL: /devwire\"]\n",
      "\"V_1245\"\t[1, \"C_31934 - URL: /ofc\"]\n",
      "\"V_1246\"\t[1, \"C_42323 - URL: /gamesdev\"]\n",
      "\"V_1247\"\t[1, \"C_30024 - URL: /wineguide\"]\n",
      "\"V_1248\"\t[1, \"C_18347 - URL: /softimage\"]\n",
      "\"V_1249\"\t[1, \"C_41914 - URL: /fortransupport\"]\n",
      "\"V_1250\"\t[1, \"C_37938 - URL: /middleeast\"]\n",
      "\"V_1251\"\t[1, \"C_40693 - URL: /referencesupport\"]\n",
      "\"V_1252\"\t[1, \"C_37561 - URL: /giving\"]\n",
      "\"V_1253\"\t[1, \"C_32638 - URL: /worddev\"]\n",
      "\"V_1254\"\t[1, \"C_20190 - URL: /ie3\"]\n",
      "\"V_1255\"\t[1, \"C_22918 - URL: /msmq\"]\n",
      "\"V_1256\"\t[1, \"C_30930 - URL: /sia\"]\n",
      "\"V_1257\"\t[1, \"C_40127 - URL: /devvideos\"]\n",
      "\"V_1258\"\t[1, \"C_30514 - URL: /peru\"]\n",
      "\"V_1259\"\t[1, \"C_21424 - URL: /controls\"]\n",
      "\"V_1260\"\t[1, \"C_21894 - URL: /trial\"]\n",
      "\"V_1261\"\t[1, \"C_36145 - URL: /diyguide\"]\n",
      "\"V_1262\"\t[1, \"C_37425 - URL: /chile\"]\n",
      "\"V_1263\"\t[1, \"C_27503 - URL: /services\"]\n",
      "\"V_1264\"\t[1, \"C_40427 - URL: /se\"]\n",
      "\"V_1265\"\t[1, \"C_39038 - URL: /ssafesupport\"]\n",
      "\"V_1266\"\t[1, \"C_35105 - URL: /licenses\"]\n",
      "\"V_1267\"\t[1, \"C_42071 - URL: /caribbean\"]\n",
      "\"V_1268\"\t[1, \"C_27503 - URL: /javascript\"]\n",
      "\"V_1269\"\t[1, \"C_41054 - URL: /business\"]\n",
      "\"V_1270\"\t[1, \"C_28493 - URL: /developr\"]\n",
      "\"V_1271\"\t[1, \"C_28493 - URL: /mdsn\"]\n",
      "\"V_1272\"\t[1, \"C_28493 - URL: /softlib\"]\n",
      "\"V_1273\"\t[1, \"C_28493 - URL: /mdn\"]\n",
      "\"V_1274\"\t[1, \"C_28493 - URL: /pdc\"]\n",
      "\"V_1275\"\t[1, \"C_28903 - URL: /security.\"]\n",
      "\"V_1276\"\t[1, \"C_40810 - URL: /vtestsupport\"]\n",
      "\"V_1277\"\t[1, \"C_30111 - URL: /stream\"]\n",
      "\"V_1278\"\t[1, \"C_41317 - URL: /hed\"]\n",
      "\"V_1279\"\t[1, \"C_31062 - URL: /msgolf\"]\n",
      "\"V_1280\"\t[1, \"C_41643 - URL: /music\"]\n",
      "\"V_1281\"\t[1, \"C_37099 - URL: /intellimouse\"]\n",
      "\"V_1282\"\t[1, \"C_41244 - URL: /home\"]\n",
      "\"V_1283\"\t[1, \"C_41033 - URL: /cinemania\"]\n",
      "\"V_1284\"\t[1, \"C_41108 - URL: /partner\"]\n",
      "\"V_1295\"\t[1, \"C_42616 - URL: /train\"]\n"
     ]
    }
   ],
   "source": [
    "### running the job locally\n",
    "#!python HW4_4.py anonymous-msweb.data > HW4_4_results\n",
    "\n",
    "### running the job on hadoop\n",
    "!python HW4_4.py anonymous-msweb.data --hadoop-home '/usr/local/Cellar/hadoop/2.7.1/' -r hadoop > HW4_4_results\n",
    "\n",
    "### results sample\n",
    "!cat HW4_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW 4.5* \n",
    "Here you will use a different dataset consisting of word-frequency distributions\n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "- 0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "- 1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "- 2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "- 3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "**topUsers_Apr-Jul_2014_1000-words.txt**\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several\n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "- (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "- (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the\n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "**Extra Notes:**\n",
    "- For (A), we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "- For (B), (C), and (D) you will have to use data from the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "- (1) The 1000-user-wide aggregate, which you will perturb for initializations\n",
    "in parts (B) and (C), and\n",
    "- (2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "\n",
    "In part (D), just use the (row-normalized) class-level aggregates as 'trained'\n",
    "starting centroids (the training is already done for you!).\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate\n",
    "(after initially normalizing by its sum, which is also provided).\n",
    "So if in (B) you want to create 2 perturbations of the aggregate, start\n",
    "with (1), normalize, and generate 1000 random numbers uniformly\n",
    "from the unit interval (0,1) twice (for two centroids), using:\n",
    "\n",
    "    from numpy import random\n",
    "    numbers = random.sample(1000)\n",
    "\n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids.\n",
    "\n",
    "\n",
    "###Data pre-processing & Initialization\n",
    "- (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "- (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "- (D) K=4 \"trained\" centroids, determined by the sums across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pre-processing done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# load the raw data\n",
    "raw_data = np.genfromtxt('topUsers_Apr-Jul_2014_1000-words.txt', delimiter=',')\n",
    "# normalize\n",
    "train_data = [a[3:]/a[2] for a in raw_data]\n",
    "# save as training data file\n",
    "np.savetxt('HW4_5_train_data.csv', train_data, delimiter = \",\")\n",
    "\n",
    "# load the auxiliary file\n",
    "!tail -n +2 topUsers_Apr-Jul_2014_1000-words_summaries.txt | cut -d ',' -f 3-2000 > HW4_5_aux.txt\n",
    "aux_data = np.genfromtxt('HW4_5_aux.txt', delimiter=',')\n",
    "aux_norm = [a[1:]/a[0] for a in aux_data]\n",
    "\n",
    "# get centroids initialization A\n",
    "K, n = 4, 1000\n",
    "idx = np.random.randint(n, size=4)\n",
    "centroids = [train_data[i] for i in idx]\n",
    "np.savetxt('HW4_5_init_A.txt', centroids, delimiter = \",\")\n",
    "\n",
    "# get centroids initialization B\n",
    "K, n = 2, 1000\n",
    "centroids = [[a+p for a, p in zip(aux_norm[0], np.random.sample(n))] for i in range(K)]\n",
    "norm = np.sum(centroids, axis = 1)\n",
    "cen_norm = [c/n for c,n in zip(centroids, norm)]\n",
    "np.savetxt('HW4_5_init_B.txt', cen_norm, delimiter = \",\")\n",
    "\n",
    "# get centroids initialization C\n",
    "K, n = 4, 1000\n",
    "centroids = [[a+p for a, p in zip(aux_norm[0], np.random.sample(n))] for i in range(K)]\n",
    "norm = np.sum(centroids, axis = 1)\n",
    "cen_norm = [c/n for c,n in zip(centroids, norm)]\n",
    "np.savetxt('HW4_5_init_C.txt', cen_norm, delimiter = \",\")\n",
    "\n",
    "# get centroids initialization D\n",
    "centroids = aux_norm[1:]\n",
    "np.savetxt('HW4_5_init_D.txt', centroids, delimiter = \",\")\n",
    "\n",
    "print 'Data pre-processing done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MrJob to provide *one* training iteration for k-mean model\n",
    "- ready for running on Hadoop\n",
    "- assuming a file *Centroids.txt*, that stores centroids coordinates, will be updated on Hadoop cluster by the training handler/driver after each iteration\n",
    "- **input**: a csv of all training sample coordinates\n",
    "- **output**: centroids coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_5_Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_5_Kmeans.py\n",
    "\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import subprocess\n",
    "\n",
    "#Calculate find the nearest centroid for data point\n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = np.array(datapoint)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    diff = datapoint - centroid_points\n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = np.argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    # TODO: figure out why set from mapper_init doesn't preserve value\n",
    "    k, n = 4, 1000\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, \n",
    "                   mapper=self.mapper, \n",
    "                   combiner = self.combiner, \n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        hadoopPath = 'Centroids.txt'\n",
    "        localPath = '/Users/leiyang/GitHub/mids/w261/HW4-Questions/ref/Centroids.txt'\n",
    "        cat = subprocess.Popen([\"cat\", hadoopPath], stdout=subprocess.PIPE)\n",
    "        self.centroid_points = [map(float, s.strip().split(',')) for s in cat.stdout]\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point\n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        yield int(MinDist(D,self.centroid_points)), (D,1)\n",
    "\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        num = 0\n",
    "        sum_d = [0]*self.n        \n",
    "        # iterate through the generator\n",
    "        for d,c in inputdata:\n",
    "            num += c                \n",
    "            sum_d = [a+b for a,b in zip(sum_d, d)]\n",
    "        yield idx, (sum_d, num)\n",
    "\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata):\n",
    "        num = [0]*self.k\n",
    "        centroids = [[0]*self.n for i in range(self.k)]\n",
    "        # iterate through the generator\n",
    "        for d, c in inputdata:\n",
    "            num[idx] += c\n",
    "            centroids[idx] = [a+b for a,b in zip(centroids[idx], d)]          \n",
    "        # recalculate centroids\n",
    "        centroids[idx] = [a/num[idx] for a in centroids[idx]]\n",
    "        yield idx, (centroids[idx])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mean training driver\n",
    "- control the training process \n",
    "- update Centroids.txt file from Hadoop clusters after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_5_Kmeans_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_5_Kmeans_driver.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "from HW4_5_Kmeans import MRKmeans, stop_criterion\n",
    "\n",
    "# create MrJob: specify input, available file, hadoop home\n",
    "mr_job = MRKmeans(args=['HW4_5_train_data.csv', '--file', 'Centroids.txt',\n",
    "                        '--hadoop-home', '/usr/local/Cellar/hadoop/2.7.1/', '-r', 'hadoop'])\n",
    "\n",
    "# load initial centroids\n",
    "centroid_points = np.genfromtxt('Centroids.txt', delimiter=',')\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 1\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    np.savetxt('c_old', centroid_points_old, delimiter=',')\n",
    "    print \"Iteration \"+str(i)+\" ...\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output         \n",
    "        for line in runner.stream_output():\n",
    "            key,value = mr_job.parse_output_line(line)            \n",
    "            centroid_points[key] = value        \n",
    "            \n",
    "    i += 1\n",
    "    # put the latest centroids in the file for next iteration    \n",
    "    np.savetxt('Centroids.txt', centroid_points, delimiter = \",\")\n",
    "        \n",
    "    # check stopping criterion\n",
    "    if stop_criterion(np.genfromtxt('c_old', delimiter=','), centroid_points, 0.001):\n",
    "        break\n",
    "        \n",
    "print '\\nK-means training completed after %d iterations!' %(i-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from HW4_5_Kmeans import stop_criterion\n",
    "    \n",
    "c1 = np.genfromtxt('Centroids.txt', delimiter=',')\n",
    "c2 = np.genfromtxt('c_old', delimiter=',')\n",
    "stop_criterion(c1,c2,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat HW4_5_init_C.txt > Centroids.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find cluster composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW4_5_get_composition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW4_5_get_composition.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from HW4_5_Kmeans import MinDist\n",
    "\n",
    "# load the centroids\n",
    "centroids = np.genfromtxt('Centroids.txt', delimiter=',') \n",
    "compo = [[0,0,0,0] for i in range(len(centroids))]\n",
    "\n",
    "# load the raw data\n",
    "raw_data = np.genfromtxt('topUsers_Apr-Jul_2014_1000-words.txt', delimiter=',')\n",
    "\n",
    "# find the cluster for each point, and record code count\n",
    "for r in raw_data:\n",
    "    code, point = r[1], r[3:]/r[2]\n",
    "    compo[int(MinDist(point, centroids))][int(code)] += 1\n",
    "    \n",
    "# save results\n",
    "#with open('Composition.txt', 'w+') as f:\n",
    "#    f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "for i in range(len(compo)):\n",
    "    print '\\n'\n",
    "    for j in range(4):\n",
    "        print 'Centroid %d - code %d: %d (%.2f%%)' %(i, j, compo[i][j], 100.0*compo[i][j]/sum(compo[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "- run MrJob to train k-means model\n",
    "- check composition of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 ...\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "Iteration 5 ...\n",
      "Iteration 6 ...\n",
      "Iteration 7 ...\n",
      "\n",
      "K-means training completed after 7 iterations!\n",
      "\n",
      "\n",
      "Centroid 0 - code 0: 67 (45.89%)\n",
      "Centroid 0 - code 1: 0 (0.00%)\n",
      "Centroid 0 - code 2: 12 (8.22%)\n",
      "Centroid 0 - code 3: 67 (45.89%)\n",
      "\n",
      "\n",
      "Centroid 1 - code 0: 1 (0.76%)\n",
      "Centroid 1 - code 1: 88 (66.67%)\n",
      "Centroid 1 - code 2: 39 (29.55%)\n",
      "Centroid 1 - code 3: 4 (3.03%)\n",
      "\n",
      "\n",
      "Centroid 2 - code 0: 683 (94.73%)\n",
      "Centroid 2 - code 1: 3 (0.42%)\n",
      "Centroid 2 - code 2: 3 (0.42%)\n",
      "Centroid 2 - code 3: 32 (4.44%)\n",
      "\n",
      "\n",
      "Centroid 3 - code 0: 1 (100.00%)\n",
      "Centroid 3 - code 1: 0 (0.00%)\n",
      "Centroid 3 - code 2: 0 (0.00%)\n",
      "Centroid 3 - code 3: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "!cat HW4_5_init_A.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat HW4_5_init_B.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 ...\r\n"
     ]
    }
   ],
   "source": [
    "!cat HW4_5_init_C.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Execution: (D) K=4 \"trained\" centroids, determined by the sums across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat HW4_5_init_D.txt > Centroids.txt\n",
    "!python HW4_5_Kmeans_driver.py\n",
    "!python HW4_5_get_composition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*W4.6  (OPTIONAL)* Scaleable K-MEANS++\n",
    "\n",
    "- Read the following paper entitled \"Scaleable K-MEANS++\" located at:\n",
    "\n",
    "http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf\n",
    "\n",
    "- In MrJob, implement K-MEANS|| and compare with a random initialization for the dataset above.\n",
    "Report on the number passes over the training data, and time required to run all  clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping historyserver\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ stop historyserver "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
