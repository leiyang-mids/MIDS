{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write some words to MostUsedWord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Slides/Lecture-04-ClusteringInMrJob/MrJob/MrJobIntroJobs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo foo foo quux labs foo bar quux > MostUsedWord.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append some more records to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo New line by Jimi >> MostUsedWord.txt\n",
    "!echo New line by Jimi 2222222 >> MostUsedWord.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the code of MrJob class for Most frequent/used Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Mostused.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Mostused.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "#    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    SORT_VALUES = True\n",
    "    def jobconfqqqq(self):\n",
    "        orig_jobconf = super(MRWordFreqCount, self).jobconf()        \n",
    "        custom_jobconf = {  #key value pairs\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "        return [\n",
    "            self.MRStep(mapper=self.mapper_get_words,       # STEP 1: word count step\n",
    "                    combiner=self.combiner_count_words,\n",
    "                    reducer=self.reducer_count_words),\n",
    "            self.MRStep(reducer=self.reducer_find_max_word) # Step 2: most frequent word\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code has two mapreduce. The first Mapreduce Mapper outputs (word, 1) key value pairs, and then the combiner combines the sum locally. Lastly, Reducer sums them up. The second mapreduce output the word having max number of count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\r\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\r\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/Mostused.leiyang.20160206.040224.463672\r\n",
      "\r\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"Mostused.py\", line 53, in <module>\r\n",
      "    MRMostUsedWord.run()\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 461, in run\r\n",
      "    mr_job.execute()\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 479, in execute\r\n",
      "    super(MRJob, self).execute()\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 153, in execute\r\n",
      "    self.run_job()\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 216, in run_job\r\n",
      "    runner.run()\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/runner.py\", line 470, in run\r\n",
      "    self._run()\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/sim.py\", line 169, in _run\r\n",
      "    for step_num, step in enumerate(self._get_steps()):\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/inline.py\", line 123, in _get_steps\r\n",
      "    self._steps = self._mrjob_cls(args=job_args)._steps_desc()\r\n",
      "  File \"/Users/leiyang/anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 648, in _steps_desc\r\n",
      "    for step_num, step in enumerate(self.steps()):\r\n",
      "  File \"Mostused.py\", line 46, in steps\r\n",
      "    self.MRStep(mapper=self.mapper_get_words,       # STEP 1: word count step\r\n",
      "AttributeError: 'MRMostUsedWord' object has no attribute 'MRStep'\r\n"
     ]
    }
   ],
   "source": [
    "!python Mostused.py MostUsedWord.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code though python driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, u'foo')\n"
     ]
    }
   ],
   "source": [
    "from Mostused import MRMostUsedWord\n",
    "mr_job = MRMostUsedWord(args=['MostUsedWord.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
