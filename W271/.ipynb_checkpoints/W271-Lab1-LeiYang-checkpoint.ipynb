{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MIDS DATASCI W271 [Lab #1](https://drive.google.com/open?id=0B3l4mqbuf82YNFk2Y0x5S0ZLc1U)\n",
    "---------\n",
    "####Due 2016-01-28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 5 (6 Points)\n",
    "\n",
    "####1. Find the value of x that minimizes E(Y). Show that your result is really the minimum.\n",
    "We first expand the definition of $Y$:\n",
    "\n",
    "$$Y=a+b(X-x)^2=a+bx^2+bX^2-2bxX$$\n",
    "\n",
    "then $E(Y)$ becomes:\n",
    "\n",
    "$$E(Y)=a+bx^2+bE(X^2)-2bxE(X)$$\n",
    "\n",
    "to find the $x$ that minimums $E(Y)$, we take derivative with respect to $x$:\n",
    "\n",
    "$$\\frac{dE(Y)}{dx}=2bx-2bE(X)$$\n",
    "\n",
    "set it to $0$ and we will the $x$ that minimums $E(Y)$:\n",
    "\n",
    "$$\\color{red}{x=E(Y)}$$\n",
    "\n",
    "Geometrically, by subtracting $E(X)$ from X we move the whole population of $X$ around zero, which will give the minimum square value of the population.\n",
    "\n",
    "####2. Find the value of E(Y) for the choice of x you found in (1)?\n",
    "Plug in $x=E(X)$ into the formula of $E(Y)$, we have:\n",
    "\n",
    "$$E(Y)=a+bE(X)^2+bE(X^2)-2bE(X)^2=a+b(E(X^2)-E(X)^2)$$\n",
    "\n",
    "and we have\n",
    "\n",
    "$$\\color{red}{E(Y)_{min}=a+bVar(X)}$$\n",
    "\n",
    "####3. Suppose $Y = ax + b(X âˆ’ x)^2$. Find the values of $x$ that minimizes $E(Y)$. Show that your result is really the minimum.\n",
    "Similar to part \\#1, we first obtain the new $E(Y)$:\n",
    "\n",
    "$$E(Y)=ax+bx^2+bE(X^2)-2bxE(X)$$\n",
    "\n",
    "take derivative of $E(Y)$:\n",
    "\n",
    "$$\\frac{dE(Y)}{dx}=a+2bx-2bE(X)$$\n",
    "\n",
    "set it to zero, we have the $x$ that minimums $E(Y)$\n",
    "\n",
    "$$x=E(X)-\\frac{a}{2b}$$\n",
    "\n",
    "plug $x$ into $E(Y)$, and the minimum is:\n",
    "\n",
    "$$\\color{red}{E(Y)_{min}=aE(X)+bVar(X)-\\frac{a^2}{2b}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 8 (10 Points)\n",
    "Let $Y_1, ..., Y_n$ be $n$ random variables, such that any two of them are uncorrelated, and all share the same mean $\\mu$ and variance $\\sigma^2$. Let $\\bar{Y}$ be the average $Y_i$, which is also a random variable.\n",
    "Define the class of linear estimators of $\\mu$ by\n",
    "\n",
    "$$W=\\sum_{i=1}^n{a_iY_i}$$\n",
    "\n",
    "where the $a_i$ are constants.\n",
    "\n",
    "####1. What restriction on the $a_i$ is needed for $W$ to be an unbiased estimator of $\\mu$?\n",
    "\n",
    "We know the unbiased estimator of population mean $\\mu$ is just the sample mean $\\bar{Y}$, for $W$ to be unbiased $E(W)=\\mu$:\n",
    "\n",
    "$$E(W)=E(\\sum_{i=1}^n{a_iY_i})=\\sum_{i=1}^n{a_iE(Y_i)}$$\n",
    "\n",
    "$\\forall \\quad i \\in 1,...,n$, we have $E(Y_i)=\\mu$, plug back in:\n",
    "\n",
    "$$E(W)=\\sum_{i=1}^n{(a_i\\mu)}=\\mu\\sum_{i=1}^n{a_i}=\\mu$$\n",
    "\n",
    "thus the restriction on $a_i$ is $\\color{red}{\\sum_{i=1}^n{a_i}=1}$\n",
    "\n",
    "####2. Find $Var(W)$.\n",
    "From $Var(W)=E(W^2)-E(W)^2$, where\n",
    "\n",
    "$$E(W^2)=E((\\sum_{i=1}^n{a_iY_i})^2)=E(\\sum_{i=1}^n\\sum_{j=1}^n{a_ia_jY_iY_j})=E(\\sum_{i=1}^na_i^2Y_i^2)+E(\\sum_{i\\neq j}{a_ia_jY_iY_j})$$\n",
    "\n",
    "thus $$E(W^2)=\\sum_i{a_i^2E(Y_i^2)}+\\sum_{i\\neq j}{a_ia_jE(Y_iY_j)}$$\n",
    "\n",
    "where $E(Y_i^2)=Var(Y_i)+E(Y_i)^2=\\sigma^2+\\mu^2$, and because any two of $Y_i$ are uncorrelated, we have $E(Y_iY_j)=E(Y_i)E(Y_j)=\\mu^2$, hence \n",
    "\n",
    "$$E(W^2)=(\\sigma^2+\\mu^2)\\sum_ia_i^2+\\mu^2\\sum_{i\\neq j}a_ia_j=\\sigma^2\\sum_ia_i^2+\\mu^2(\\sum_ia_i^2+\\sum_{i\\neq j}a_ia_j)=\\sigma^2\\sum_ia_i^2+\\mu^2(\\sum_ia_i)^2$$\n",
    "\n",
    "then we have $E(W)^2=(\\sum_i{a_iE(Y_i)})^2=\\mu^2(\\sum_ia_i)^2$, then the variance of $W$ is:\n",
    "\n",
    "$$Var(W)=E(W^2)-E(W)^2=\\color{red}{\\sigma^2\\sum_{i=1}^na_i^2}$$\n",
    "\n",
    "####3. Given a set of numbers $a_1, a_2, ..., a_n$, the following inequality holds:\n",
    "\n",
    "$$\\frac{1}{n}(\\sum_{i=1}^n{a_i})^2\\leq \\sum_{i=1}^n{a_i^2}$$\n",
    "\n",
    "Use this inequality, along with the previous parts of this question, to show that $Var(W) \\geq Var(\\bar{Y})$ whenever $W$ is unbiased. We say that $\\bar{Y}$ is the best linear unbiased estimator (BLUE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 9 (10 Points)\n",
    "\n",
    "Let $\\bar{Y}$ denote the average of $n$ independent draws from a population distribution with\n",
    "mean $\\mu$ and variance $\\sigma^2$. Consider two alternative estimators of $\\mu$: $W1=(\\frac{n-1}{n})\\bar{Y}$ and\n",
    "$W2 = k\\bar{Y}$ , where $0 < k < 1$.\n",
    "\n",
    "####1. Compute the biases of both $W_1$ and $W_2$. Which estimator is consistent?\n",
    "\n",
    "The bias $B$ of an estimator $e$ for statistic $s$ is defined as the difference between the expectation of the estimator $E(e)$ and the true value $s$ from the population, thus\n",
    "\n",
    "$$B(W_1)=E(W_1)-\\mu,$$ \n",
    "\n",
    "where $$E(W_1)=E((\\frac{n-1}{n})\\bar{Y})=(\\frac{n-1}{n})E(\\bar{Y})=(\\frac{n-1}{n})\\mu$$, because the sample average $\\bar{Y}$ is the unbiased estimation population $\\mu$,\n",
    "\n",
    "thus $\\color{red}{B(W_1)=(\\frac{n-1}{n})\\mu-\\mu=-\\frac{\\mu}{n}}.$\n",
    "\n",
    "Similarly $B(W_2)=E(W_2)-\\mu$, where\n",
    "\n",
    "$E(W_2)=E(k\\bar{Y})=kE(\\bar{Y})=k\\mu$, then $$\\color{red}{B(W_2)=k\\mu-\\mu}.$$\n",
    "\n",
    "As $n$ increases, $B(W_1)$ will decreases to $0$, so $W_1$ is consistent, while $W_2$ is not.\n",
    "\n",
    "####2. Compute $Var(W_1)$ and $Var(W_2)$. Which estimator has lower variance?\n",
    "\n",
    "In general $Var(X)=E(X^2)-E(X)^2$, so we need $E(W_1^2)$ and $E(W_1^2)$:\n",
    "\n",
    "$$E(W_1^2)=E((\\frac{n-1}{n})^2\\bar{Y}^2)=(\\frac{n-1}{n})^2E(\\bar{Y}^2)$$\n",
    "$$E(W_2^2)=E(k^2\\bar{Y}^2)=k^2E(\\bar{Y}^2)$$\n",
    "\n",
    "where $E(\\bar{Y}^2)=Var(\\bar{Y})+E(\\bar{Y})^2$, \n",
    "\n",
    "then $Var(\\bar{Y})=Var(\\frac{1}{n}\\sum_i{X_i})=\\frac{1}{n^2}Var(\\sum_i{X_i})$.\n",
    "\n",
    "Because $X_i$ are independent draws, $Var(\\sum_i{X_i})=\\sum_i{Var(X_i)}=n\\sigma^2$, then $Var(\\bar{Y})=\\frac{1}{n^2}\\times n\\times \\sigma^2=\\frac{\\sigma^2}{n}$\n",
    "\n",
    "And we can obtain $E(\\bar{Y}^2)=\\frac{\\sigma^2}{n}+\\mu^2$, plug it back to get $E(W_1^2)$ and $E(W_2^2)$:\n",
    "\n",
    "$$E(W_1^2)=(\\frac{n-1}{n})^2\\times (\\frac{\\sigma^2}{n}+\\mu^2)$$\n",
    "$$E(W_2^2)=k^2\\times \\frac{\\sigma^2}{n}+\\mu^2$$\n",
    "\n",
    "Finally we have $Var(W_1)$ and $Var(W_2)$:\n",
    "\n",
    "$$Var(W_1)=E(W_1^2)-E(W_1)^2=(\\frac{n-1}{n})^2\\times (\\frac{\\sigma^2}{n}+\\mu^2) - (\\frac{n-1}{n})^2\\times\\mu^2 = \\color{red}{\\frac{((n-1)\\sigma)^2}{n^3}}$$\n",
    "$$Var(W_2)=E(W_2^2)-E(W_2)^2=k^2\\times \\frac{\\sigma^2}{n}+\\mu^2 - k^2\\mu^2=\\color{red}{\\frac{k^2\\sigma^2}{n}+\\mu^2(1-k)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###Question 10 (10 Points)\n",
    "Given a random sample $Y_1, Y_2,...,Y_n$ from some distribution $F(.)$ with mean $\\mu$ and variance $\\sigma^2$, where both $\\mu$ and $\\sigma^2$ are unknown parameters.\n",
    "\n",
    "Let $\\bar{Y}$ be the average of the sample. Consider the following estimator for $\\sigma^2$:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\frac{1}{n}\\sum_{i=1}^n{(Y_i-\\bar{Y})^2}$$\n",
    "\n",
    "####1. Show that $E(\\bar{Y}) = E(Y_i)  \\forall i \\in 1,2,...,n$\n",
    "\n",
    "$E(\\bar{Y})=E(\\frac{1}{n}\\sum_{i=1}^n{Y_i})=\\frac{1}{n}\\sum_{i=1}^n{E(Y_i)}$, considering $\\forall i \\quad E(Y_i)=E(Y)$, we have\n",
    "\n",
    "$E(\\bar{Y})=\\frac{1}{n}\\sum_{i=1}^n{E(Y)}=\\frac{1}{n}\\times n\\times E(Y)=E(Y_i)$\n",
    "\n",
    "####2. Show that $Var(\\bar{Y}) = \\frac{1}{n}Var(Y_i) \\quad \\forall i \\in 1,2,...,n$,\n",
    "\n",
    "$Var(\\bar{Y})=Var(\\frac{1}{n}\\sum_i{Y_i})=\\frac{1}{n^2}Var(\\sum_i{Y_i})$ where\n",
    "\n",
    "$Var(\\sum_i{Y_i})=\\sum_{i=1}^n{Var(Y_i)}+\\sum_{i\\neq j}{Cov(Y_i,Y_j)}$\n",
    "\n",
    "because the sample is randomly drawn, applying $\\color{red}{i.i.d}$ property, we have $Cov(Y_i,Y_j)=0$\n",
    "\n",
    "and since $\\forall i \\quad Var(Y_i)=Var(Y)$\n",
    "\n",
    "thus $Var(\\bar{Y})=\\frac{1}{n^2}\\sum_{i=1}^n{Var(Y_i)}=\\frac{1}{n^2}\\times n\\times Var(Y)=\\frac{1}{n}Var(Y_i)$\n",
    "\n",
    "\n",
    "####3. Compute the expectation of $\\hat{\\sigma^2}$ in terms of n and $\\sigma^2$. In your derivation, make sure make use of the $i.i.d$ property and identify where you use it.\n",
    "\n",
    "Let's massage $\\hat{\\sigma^2}$ first:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\frac{1}{n}\\sum_{i=1}^n{((Y_i-\\mu)-(\\bar{Y}-\\mu))^2}=\\frac{1}{n}\\sum_i{((Y_i-\\mu)^2+(\\bar{Y}-\\mu)^2-2(Y_i-\\mu)(\\bar{Y}-\\mu))}$$\n",
    "\n",
    "expand the summation, we have:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\frac{1}{n}\\sum_i(Y_i-\\mu)^2+(\\bar{Y}-\\mu)^2-\\frac{1}{n}\\sum_i{(2(Y_i-\\mu)(\\bar{Y}-\\mu))}$$\n",
    "\n",
    "the first summation term is the definition of $\\sigma^2$, we further expand the last summation:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\sigma^2+(\\bar{Y}-\\mu)^2-\\frac{2(\\bar{Y}-\\mu)}{n}\\sum_i{(Y_i-\\mu)}$$\n",
    "\n",
    "with $\\sum_i{Y_i}=n\\bar{Y}$:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\sigma^2+(\\bar{Y}-\\mu)^2-\\frac{2(\\bar{Y}-\\mu)}{n}(n\\bar{Y}-n\\mu)=\\sigma^2-(\\bar{Y}-\\mu)^2$$\n",
    "\n",
    "now take expectation:\n",
    "\n",
    "$$E(\\hat{\\sigma^2})=E(\\sigma^2)-E((\\bar{Y}-\\mu)^2)=\\sigma^2-E(\\bar{Y}^2)-\\mu^2+2\\mu E(\\bar{Y})$$\n",
    "\n",
    "from (1): $E(\\bar{Y})=E(Y_i)=\\mu$, we have:\n",
    "\n",
    "$$E(\\hat{\\sigma^2})=\\sigma^2+\\mu^2-E(\\bar{Y}^2)$$\n",
    "\n",
    "where $E(\\bar{Y}^2)=Var(\\bar{Y})+E(\\bar{Y})^2$, from (2), we have:\n",
    "\n",
    "$$E(\\bar{Y}^2)=\\frac{\\sigma^2}{n}+\\mu^2$$\n",
    "\n",
    "plug back in, the expectation of $\\hat{\\sigma^2}$:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\sigma^2+\\mu^2-\\frac{\\sigma^2}{n}-\\mu^2=\\color{red}{\\frac{n-1}{n}\\sigma^2}$$\n",
    "\n",
    "####4. Is this an unbiased estimator for $\\sigma^2$?\n",
    "\n",
    "Obviously **it is a biased estimator**, as we see its expectation is a fraction $\\frac{n-1}{n}$ of the truth $\\sigma^2$\n",
    "\n",
    "\n",
    "####5. If not, what function of $\\hat{\\sigma^2}$ produce an unbiased estimator?\n",
    "\n",
    "To become unbiased, we simply multiply $\\frac{n}{n-1}$ on $\\hat{\\sigma^2}$, and the unbiased estimator is:\n",
    "\n",
    "$$\\color{red}{\\frac{1}{n-1}\\sum_{i=1}^n{(Y_i-\\bar{Y})^2}}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
