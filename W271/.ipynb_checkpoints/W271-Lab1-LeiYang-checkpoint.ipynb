{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MIDS DATASCI W271 [Lab #1](https://drive.google.com/open?id=0B3l4mqbuf82YNFk2Y0x5S0ZLc1U)\n",
    "---------\n",
    "####Due 2016-01-28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 5 (6 Points)\n",
    "\n",
    "####1. Find the value of x that minimizes E(Y). Show that your result is really the minimum.\n",
    "\n",
    "We first expand the definition of $Y$:\n",
    "\n",
    "$$Y=a+b(X-x)^2=a+bx^2+bX^2-2bxX$$\n",
    "\n",
    "then $E(Y)$ becomes:\n",
    "\n",
    "$$E(Y)=a+bx^2+bE(X^2)-2bxE(X)$$\n",
    "\n",
    "to find the $x$ that minimums $E(Y)$, we take derivative with respect to $x$:\n",
    "\n",
    "$$\\frac{dE(Y)}{dx}=2bx-2bE(X)$$\n",
    "\n",
    "set it to $0$ and we will the $x$ that minimums $E(Y)$:\n",
    "\n",
    "$$\\color{red}{x=E(Y)}$$\n",
    "\n",
    "Geometrically, by subtracting $E(X)$ from X we move the whole population of $X$ around zero, which will give the minimum square value of the population.\n",
    "\n",
    "####2. Find the value of E(Y) for the choice of x you found in (1)?\n",
    "\n",
    "Plug in $x=E(X)$ into the formula of $E(Y)$, we have:\n",
    "\n",
    "$$E(Y)=a+bE(X)^2+bE(X^2)-2bE(X)^2=a+b(E(X^2)-E(X)^2)$$\n",
    "\n",
    "and we have\n",
    "\n",
    "$$\\color{red}{E(Y)_{min}=a+bVar(X)}$$\n",
    "\n",
    "####3. Suppose $Y = ax + b(X − x)^2$. Find the values of $x$ that minimizes $E(Y)$. Show that your result is really the minimum.\n",
    "\n",
    "Similar to part \\#1, we first obtain the new $E(Y)$:\n",
    "\n",
    "$$E(Y)=ax+bx^2+bE(X^2)-2bxE(X)$$\n",
    "\n",
    "take derivative of $E(Y)$:\n",
    "\n",
    "$$\\frac{dE(Y)}{dx}=a+2bx-2bE(X)$$\n",
    "\n",
    "set it to zero, we have the $x$ that minimums $E(Y)$\n",
    "\n",
    "$$x=E(X)-\\frac{a}{2b}$$\n",
    "\n",
    "plug $x$ into $E(Y)$, and the minimum is:\n",
    "\n",
    "$$\\color{red}{E(Y)_{min}=aE(X)+bVar(X)-\\frac{a^2}{2b}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 8 (10 Points)\n",
    "Let $Y_1, ..., Y_n$ be $n$ random variables, such that any two of them are uncorrelated, and all share the same mean $\\mu$ and variance $\\sigma^2$. Let $\\bar{Y}$ be the average $Y_i$, which is also a random variable.\n",
    "Define the class of linear estimators of $\\mu$ by\n",
    "\n",
    "$$W=\\sum_{i=1}^n{a_iY_i}$$\n",
    "\n",
    "where the $a_i$ are constants.\n",
    "\n",
    "####1. What restriction on the $a_i$ is needed for $W$ to be an unbiased estimator of $\\mu$?\n",
    "\n",
    "We know the unbiased estimator of population mean $\\mu$ is just the sample mean $\\bar{Y}$, for $W$ to be unbiased $E(W)=\\mu$:\n",
    "\n",
    "$$E(W)=E(\\sum_{i=1}^n{a_iY_i})=\\sum_{i=1}^n{a_iE(Y_i)}$$\n",
    "\n",
    "$\\forall \\quad i \\in 1,...,n$, we have $E(Y_i)=\\mu$, plug back in:\n",
    "\n",
    "$$E(W)=\\sum_{i=1}^n{(a_i\\mu)}=\\mu\\sum_{i=1}^n{a_i}=\\mu$$\n",
    "\n",
    "thus the restriction on $a_i$ is $\\color{red}{\\sum_{i=1}^n{a_i}=1}$\n",
    "\n",
    "####2. Find $Var(W)$.\n",
    "\n",
    "From $Var(W)=E(W^2)-E(W)^2$, where\n",
    "\n",
    "$$E(W^2)=E((\\sum_{i=1}^n{a_iY_i})^2)=E(\\sum_{i=1}^n\\sum_{j=1}^n{a_ia_jY_iY_j})=E(\\sum_{i=1}^na_i^2Y_i^2)+E(\\sum_{i\\neq j}{a_ia_jY_iY_j})$$\n",
    "\n",
    "thus $$E(W^2)=\\sum_i{a_i^2E(Y_i^2)}+\\sum_{i\\neq j}{a_ia_jE(Y_iY_j)}$$\n",
    "\n",
    "where $E(Y_i^2)=Var(Y_i)+E(Y_i)^2=\\sigma^2+\\mu^2$, and because any two of $Y_i$ are uncorrelated, we have $E(Y_iY_j)=E(Y_i)E(Y_j)=\\mu^2$, hence \n",
    "\n",
    "$$E(W^2)=(\\sigma^2+\\mu^2)\\sum_ia_i^2+\\mu^2\\sum_{i\\neq j}a_ia_j=\\sigma^2\\sum_ia_i^2+\\mu^2(\\sum_ia_i^2+\\sum_{i\\neq j}a_ia_j)=\\sigma^2\\sum_ia_i^2+\\mu^2(\\sum_ia_i)^2$$\n",
    "\n",
    "then we have $E(W)^2=(\\sum_i{a_iE(Y_i)})^2=\\mu^2(\\sum_ia_i)^2$, then the variance of $W$ is:\n",
    "\n",
    "$$Var(W)=E(W^2)-E(W)^2=\\color{red}{\\sigma^2\\sum_{i=1}^na_i^2}$$\n",
    "\n",
    "####3. Given a set of numbers $a_1, a_2, ..., a_n$, the following inequality holds:\n",
    "\n",
    " $$\\frac{1}{n}(\\sum_{i=1}^n{a_i})^2\\leq \\sum_{i=1}^n{a_i^2}$$\n",
    "\n",
    "Use this inequality, along with the previous parts of this question, to show that $Var(W) \\geq Var(\\bar{Y})$ whenever $W$ is unbiased. We say that $\\bar{Y}$ is the best linear unbiased estimator (BLUE).\n",
    "\n",
    "**Proof:** From variance definition and property we have $Var(\\bar{Y})=Var(\\frac{1}{n}\\sum_i{Y_i})=\\frac{1}{n^2}Var(\\sum_i{Y_i})$, in which\n",
    "\n",
    "$$Var(\\sum_i{Y_i})=\\sum_{i,j=1}^n{Cov(Y_i,Y_j)}=\\sum_i{Var(Y_i)}+\\sum_{i\\neq i}{Cov(Y_i,Y_j)}$$\n",
    "\n",
    "because any pair of $Y_i$ is uncorrelated, we have $Cov(Y_i,Y_j)=0\\quad\\forall i\\neq j$, then $Var(\\sum_i{Y_i})=\\sum_i{Var(Y_i)}=n\\sigma^2$, \n",
    "thus $Var(\\bar{Y})=\\frac{1}{n^2}\\times n\\times\\sigma^2=\\frac{1}{n}\\sigma^2$\n",
    "\n",
    "from part 1, when $W$ is unbiased, we have $\\sum_ia_i=1$, then $Var(\\bar{Y})$ can be written as $\\sigma^2\\frac{1}{n}(\\sum_ia_i)^2$,\n",
    "\n",
    "because $\\sigma^2\\geq 0$, according to the given Cauchy inequality, we have $\\color{red}{\\sigma^2\\frac{1}{n}(\\sum_ia_i)^2\\leq \\sigma^2\\sum_i{a_i^2}}$, then\n",
    "\n",
    "$$Var(\\bar{Y})\\leq Var(W)$$\n",
    "\n",
    "whenever $W$ is unbiased, and $\\bar{Y}$ is BLUE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 9 (10 Points)\n",
    "\n",
    "Let $\\bar{Y}$ denote the average of $n$ independent draws from a population distribution with\n",
    "mean $\\mu$ and variance $\\sigma^2$. Consider two alternative estimators of $\\mu$: $W1=(\\frac{n-1}{n})\\bar{Y}$ and\n",
    "$W2 = k\\bar{Y}$ , where $0 < k < 1$.\n",
    "\n",
    "####1. Compute the biases of both $W_1$ and $W_2$. Which estimator is consistent?\n",
    "\n",
    "The bias $B$ of an estimator $e$ for statistic $s$ is defined as the difference between the expectation of the estimator $E(e)$ and the true value $s$ from the population, thus\n",
    "\n",
    "$$B(W_1)=E(W_1)-\\mu,$$ \n",
    "\n",
    "where $$E(W_1)=E((\\frac{n-1}{n})\\bar{Y})=(\\frac{n-1}{n})E(\\bar{Y})=(\\frac{n-1}{n})\\mu$$ \n",
    "\n",
    "because the sample average $\\bar{Y}$ is the unbiased estimation population $\\mu$, thus\n",
    "\n",
    "$$\\color{red}{B(W_1)=(\\frac{n-1}{n})\\mu-\\mu=-\\frac{\\mu}{n}}$$\n",
    "\n",
    "Similarly $B(W_2)=E(W_2)-\\mu$, where\n",
    "$E(W_2)=E(k\\bar{Y})=kE(\\bar{Y})=k\\mu$, then $$\\color{red}{B(W_2)=k\\mu-\\mu}.$$\n",
    "\n",
    "As $n$ increases, $B(W_1)$ will decreases to $0$, so $W_1$ is consistent, while $W_2$ is not.\n",
    "\n",
    "####2. Compute $Var(W_1)$ and $Var(W_2)$. Which estimator has lower variance?\n",
    "\n",
    "In general $Var(X)=E(X^2)-E(X)^2$, so we need $E(W_1^2)$ and $E(W_1^2)$:\n",
    "\n",
    "$$E(W_1^2)=E((\\frac{n-1}{n})^2\\bar{Y}^2)=(\\frac{n-1}{n})^2E(\\bar{Y}^2)$$\n",
    "$$E(W_2^2)=E(k^2\\bar{Y}^2)=k^2E(\\bar{Y}^2)$$\n",
    "\n",
    "where $E(\\bar{Y}^2)=Var(\\bar{Y})+E(\\bar{Y})^2$, \n",
    "\n",
    "then $Var(\\bar{Y})=Var(\\frac{1}{n}\\sum_i{Y_i})=\\frac{1}{n^2}Var(\\sum_i{Y_i})$.\n",
    "\n",
    "Because $X_i$ are independent draws, $Var(\\sum_i{X_i})=\\sum_i{Var(X_i)}=n\\sigma^2$, then $Var(\\bar{Y})=\\frac{1}{n^2}\\times n\\times \\sigma^2=\\frac{\\sigma^2}{n}$\n",
    "\n",
    "And we can obtain $E(\\bar{Y}^2)=\\frac{\\sigma^2}{n}+\\mu^2$, plug it back to get $E(W_1^2)$ and $E(W_2^2)$:\n",
    "\n",
    "$$E(W_1^2)=(\\frac{n-1}{n})^2\\times (\\frac{\\sigma^2}{n}+\\mu^2)$$\n",
    "$$E(W_2^2)=k^2\\times \\frac{\\sigma^2}{n}+\\mu^2$$\n",
    "\n",
    "Finally we have $Var(W_1)$ and $Var(W_2)$:\n",
    "\n",
    "$$Var(W_1)=E(W_1^2)-E(W_1)^2=(\\frac{n-1}{n})^2\\times (\\frac{\\sigma^2}{n}+\\mu^2) - (\\frac{n-1}{n})^2\\times\\mu^2 = \\color{red}{\\frac{((n-1)\\sigma)^2}{n^3}}$$\n",
    "$$Var(W_2)=E(W_2^2)-E(W_2)^2=k^2\\times \\frac{\\sigma^2}{n}+\\mu^2 - k^2\\mu^2=\\color{red}{\\frac{k^2\\sigma^2}{n}+\\mu^2(1-k)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###Question 10 (10 Points)\n",
    "Given a random sample $Y_1, Y_2,...,Y_n$ from some distribution $F(.)$ with mean $\\mu$ and variance $\\sigma^2$, where both $\\mu$ and $\\sigma^2$ are unknown parameters.\n",
    "\n",
    "Let $\\bar{Y}$ be the average of the sample. Consider the following estimator for $\\sigma^2$:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\frac{1}{n}\\sum_{i=1}^n{(Y_i-\\bar{Y})^2}$$\n",
    "\n",
    "####1. Show that $E(\\bar{Y}) = E(Y_i)  \\forall i \\in 1,2,...,n$\n",
    "\n",
    " $E(\\bar{Y})=E(\\frac{1}{n}\\sum_{i=1}^n{Y_i})=\\frac{1}{n}\\sum_{i=1}^n{E(Y_i)}$, considering $\\forall i \\quad E(Y_i)=E(Y)$, we have\n",
    "\n",
    "$E(\\bar{Y})=\\frac{1}{n}\\sum_{i=1}^n{E(Y)}=\\frac{1}{n}\\times n\\times E(Y)=E(Y_i)$\n",
    "\n",
    "####2. Show that $Var(\\bar{Y}) = \\frac{1}{n}Var(Y_i) \\quad \\forall i \\in 1,2,...,n$,\n",
    "\n",
    " $Var(\\bar{Y})=Var(\\frac{1}{n}\\sum_i{Y_i})=\\frac{1}{n^2}Var(\\sum_i{Y_i})$ where\n",
    "\n",
    "$Var(\\sum_i{Y_i})=\\sum_{i=1}^n{Var(Y_i)}+\\sum_{i\\neq j}{Cov(Y_i,Y_j)}$\n",
    "\n",
    "because the sample is randomly drawn, applying $\\color{red}{i.i.d}$ property, we have $Cov(Y_i,Y_j)=0$\n",
    "\n",
    "and since $\\forall i \\quad Var(Y_i)=Var(Y)$\n",
    "\n",
    "thus $Var(\\bar{Y})=\\frac{1}{n^2}\\sum_{i=1}^n{Var(Y_i)}=\\frac{1}{n^2}\\times n\\times Var(Y)=\\frac{1}{n}Var(Y_i)$\n",
    "\n",
    "\n",
    "####3. Compute the expectation of $\\hat{\\sigma^2}$ in terms of n and $\\sigma^2$. In your derivation, make sure make use of the $i.i.d$ property and identify where you use it.\n",
    "\n",
    "Let's massage $\\hat{\\sigma^2}$ first:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\frac{1}{n}\\sum_{i=1}^n{((Y_i-\\mu)-(\\bar{Y}-\\mu))^2}=\\frac{1}{n}\\sum_i{((Y_i-\\mu)^2+(\\bar{Y}-\\mu)^2-2(Y_i-\\mu)(\\bar{Y}-\\mu))}$$\n",
    "\n",
    "expand the summation, we have:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\frac{1}{n}\\sum_i(Y_i-\\mu)^2+(\\bar{Y}-\\mu)^2-\\frac{1}{n}\\sum_i{(2(Y_i-\\mu)(\\bar{Y}-\\mu))}$$\n",
    "\n",
    "the first summation term is the definition of $\\sigma^2$, we further expand the last summation:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\sigma^2+(\\bar{Y}-\\mu)^2-\\frac{2(\\bar{Y}-\\mu)}{n}\\sum_i{(Y_i-\\mu)}$$\n",
    "\n",
    "with $\\sum_i{Y_i}=n\\bar{Y}$:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\sigma^2+(\\bar{Y}-\\mu)^2-\\frac{2(\\bar{Y}-\\mu)}{n}(n\\bar{Y}-n\\mu)=\\sigma^2-(\\bar{Y}-\\mu)^2$$\n",
    "\n",
    "now take expectation:\n",
    "\n",
    "$$E(\\hat{\\sigma^2})=E(\\sigma^2)-E((\\bar{Y}-\\mu)^2)=\\sigma^2-E(\\bar{Y}^2)-\\mu^2+2\\mu E(\\bar{Y})$$\n",
    "\n",
    "from (1): $E(\\bar{Y})=E(Y_i)=\\mu$, we have:\n",
    "\n",
    "$$E(\\hat{\\sigma^2})=\\sigma^2+\\mu^2-E(\\bar{Y}^2)$$\n",
    "\n",
    "where $E(\\bar{Y}^2)=Var(\\bar{Y})+E(\\bar{Y})^2$, from (2), we have:\n",
    "\n",
    "$$E(\\bar{Y}^2)=\\frac{\\sigma^2}{n}+\\mu^2$$\n",
    "\n",
    "plug back in, the expectation of $\\hat{\\sigma^2}$:\n",
    "\n",
    "$$\\hat{\\sigma^2}=\\sigma^2+\\mu^2-\\frac{\\sigma^2}{n}-\\mu^2=\\color{red}{\\frac{n-1}{n}\\sigma^2}$$\n",
    "\n",
    "####4. Is this an unbiased estimator for $\\sigma^2$?\n",
    "\n",
    "Obviously **it is a biased estimator**, as we see its expectation is a fraction $\\frac{n-1}{n}$ of the truth $\\sigma^2$\n",
    "\n",
    "\n",
    "####5. If not, what function of $\\hat{\\sigma^2}$ produce an unbiased estimator?\n",
    "\n",
    "To become unbiased, we simply multiply $\\frac{n}{n-1}$ on $\\hat{\\sigma^2}$, and the unbiased estimator is:\n",
    "\n",
    "$$\\color{red}{\\frac{1}{n-1}\\sum_{i=1}^n{(Y_i-\\bar{Y})^2}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 11 (5 Points)\n",
    "For positive random variables $X$ and $Y$, suppose the expected value of $Y$ given $X$ is $E(Y|X)=\\theta X$\n",
    "The unknown parameter $\\theta$ shows how the expected value of $Y$ changes with $X$.\n",
    "\n",
    "####1. Define the random variable $Z=Y/X$. Show that $E(Z)=\\theta$.\n",
    "\n",
    "Evaluate $E(Z|X)=E(Y/X|X)=E(Y|X)/X=(\\theta X)/X=\\theta$, thus\n",
    "\n",
    "$$\\color{red}{E(Z)=E(E(Z|X))=E(\\theta)=\\theta}$$\n",
    "\n",
    "####2. Use part (1) to prove that the estimator $W_1=n^(-1)\\sum_{i=1}^n{(Y_i/X_i)}$ is unbiased for $\\theta$, where ${(X_i,Y_i): i = 1, 2, ..., n}$ is a random sample.\n",
    "\n",
    "Evaluate $E(W_1)=n^{-1}\\sum_{i=1}^n{E((Y_i/X_i))}$, from (1) we have \n",
    "\n",
    "$$\\color{red}{E(W_1)=n^{-1}\\sum_{i=1}^n{E(Z_i)}=\\frac{1}{n}\\times n\\times\\theta=\\theta}$$\n",
    "\n",
    "####3. Explain why the estimator $W2=\\bar{Y}/\\bar{X}$ , where the overbars denote sample averages, is not the same as $W_1$. Nevertheless, show that $W_2$ is also unbiased for $\\theta$.\n",
    "\n",
    "Expand $W_2=\\frac{n^{-1}\\sum_iY_i}{n^{-1}\\sum_iX_i}=\\frac{\\sum_iY_i}{\\sum_iX_i}$, the summation can't be further simplified, obviously $W_1\\neq W_2$.\n",
    "\n",
    "Evaluate $E(W_2)=E(\\frac{\\sum_iY_i}{\\sum_iX_i})$, apply expectaion within the summation, we have:\n",
    "\n",
    "$$\\color{red}{E(W_2)=\\frac{\\sum_i{E(Y_i)}}{\\sum_i{E(X_i)}}=\\frac{\\sum_i{E(Y)}}{\\sum_i{E(X)}}=\\frac{E(Y)}{E(X)}=\\frac{E(Y|X)}{E(X|X)}=\\frac{\\theta X}{X}=\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 12 (10 Points)\n",
    "You are hired by the governor to study whether a tax on liquor has decreased average liquor\n",
    "consumption in your state. You are able to obtain, for a sample of individuals selected at\n",
    "random, the difference in liquor consumption (in ounces) for the years before and after the\n",
    "tax. For person $i$ who is sampled randomly from the population, $Y_i$ denotes the change in\n",
    "liquor consumption. Treat these as a random sample from a Normal $(\\mu,\\sigma^2)$ distribution.\n",
    "\n",
    "####1. The null hypothesis is that there was no change in average liquor consumption. State this formally in terms of $\\mu$.\n",
    "\n",
    "Null hypothesis: $\\color{red}{H_0: \\mu=0}$\n",
    "\n",
    "####2. The alternative is that there was a decline in liquor consumption; state the alternative in terms of $\\mu$.\n",
    "\n",
    "Alternative hypothesis: $\\color{red}{H_1: \\mu<0}$\n",
    "\n",
    "####3. Now, suppose your sample size is $n=900$ and you obtain the estimates $\\bar{y}=-32.8$ and $s=466.4$. Calculate the $t$ statistic for testing $H_0$ against $H_1$; obtain the $p$-value for the test. Do you reject $H_0$ at the $5\\%$ level? At the $1\\%$ level?\n",
    "\n",
    "The $t$ statistic:\n",
    "\n",
    "$$\\color{red}{t=\\frac{\\bar{y}-\\mu_0}{s/\\sqrt{n}}=\\frac{-32.8-0}{466.4/\\sqrt{900}}=-2.11}$$\n",
    "\n",
    " $p$-value: $$\\color{red}{p=0.0176}$$\n",
    "\n",
    " Reject $H_0$ at the $5\\%$ level, and fail to reject $H_0$ at the $1\\%$ level.\n",
    " \n",
    "####4. Would you say that the estimated fall in consumption is large in magnitude? Comment on the practical versus statistical significance of this estimate.\n",
    "\n",
    "For practical significance, we evaluate the Cohen's $d$ effect size:\n",
    "\n",
    "$$\\color{red}{d=\\frac{\\bar{y}-\\mu}{s}=0.070}$$\n",
    "\n",
    "thus there is **little practical significance** from the test.\n",
    "\n",
    "####5. What has been implicitly assumed in your analysis about other determinants of liquor consumption over the two-year period in order to infer causality from the tax change to liquor consumption?\n",
    "\n",
    "It is implicitly assumed that *before* and *after* the tax law is implemented, all other determinants that can affect liquor consumption remain the same, thus the difference is only attribted to tax change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Question 13 (10 Points)\n",
    "The *New York Times* (2/5/90) reported three-point shooting performance for the top 10\n",
    "three-point shooters in the NBA. The following table summarizes these data:\n",
    "\n",
    "| Player | FGA-FGM   |\n",
    "|-----|-------:|\n",
    "|Mark Price |429-188|\n",
    "|Trent Tucker |833-345|\n",
    "|Dale Ellis|1,149-472|\n",
    "|Craig Hodges |1,016-396|\n",
    "|Danny Ainge|1,051-406|\n",
    "|Byron Scott |676-260|\n",
    "|Reggie Miller|416-159|\n",
    "|Larry Bird |1,206-455|\n",
    "|Jon Sundvold|440-166|\n",
    "|Brian Taylor |417-157|\n",
    "\n",
    "For a given player, the outcome of a particular shot can be modelled as a Bernoulli (zero-one)\n",
    "variable: if $Y_i$ is the outcome of shot $i$, then $Y_i=1$ if the shot is made, and $Y_i=0$ if\n",
    "the shot is missed. Let $\\theta$ denote the probability of making any particular three-point shot\n",
    "attempt. The natural estimator of $\\theta$ is $\\bar{Y}=FGM/FGA$.\n",
    "\n",
    "####1. Estimate $\\theta$ for Mark Price.\n",
    "\n",
    " $$\\color{red}{\\theta_{MP}=\\frac{188}{429}=0.4382}$$\n",
    "\n",
    "####2. Find the standard deviation of the estimator $\\bar{Y}$ in terms of $\\theta$ and the number of shot attempts, $n$.\n",
    "\n",
    "\n",
    "####3. The asymptotic distribution of $(\\bar{Y}-\\theta)/se(\\bar{Y})$ is standard normal, where $se(\\bar{Y})=\\sqrt{\\bar{Y}(1-\\bar{Y})/n}$.  Use this fact to test $H_0: \\theta= .5$ against $H_1: \\theta < .5$ for Mark Price. Use a $1\\%$ significance level.\n",
    "\n",
    "For Mark Price: $se(\\bar{Y})=\\sqrt{\\frac{0.4382(1-0.4382)}{429}}=0.0240$, and $\\frac{\\bar{Y}-\\theta}{se(\\bar{Y})}=\\frac{0.4382-0.5}{0.0240}=-2.5786$, $p$-value $=0.005$, thus we reject $H_0$ at $1\\%$ level\n",
    "\n",
    "####4. Define Type I error.\n",
    "\n",
    "Type I error is the probability that $H_0$ is falsely rejected given it's true, namely $P(H_1|H_0)$.\n",
    "\n",
    "####5. What is the probability of Type I error of this test?\n",
    "\n",
    "At $1\\%$ leve, the Type I error is $\\color{red}{\\alpha=1\\%}$\n",
    "\n",
    "####6. Define Type II error.\n",
    "\n",
    "Type II error is the probability of failing to reject $H_0$ given that $H_1$ is true, namely $P(H_0|H_1)$\n",
    "\n",
    "####7. What is the probability of Type II error when using this decision rule, assuming the ”true” population proportion is $\\theta^∗ = 0.45$.\n",
    "\n",
    "For $1\\%$ level to reject $H_0$, the decision rule: $\\frac{\\bar{Y}-0.5}{se(\\bar{Y})}<Z_{0.01}=-2.3263$, or $\\bar{Y}<Z_{0.01}*se(\\bar{Y})+0.5=0.4442$\n",
    "\n",
    "with true $\\theta^*=0.45$:\n",
    "\n",
    "$$\\beta=P(\\bar{Y}\\leq 0.4442|\\theta=0.45)=P(Z\\leq \\frac{0.4442-0.45}{se(\\bar{Y})})=\\color{red}{0.4040}$$\n",
    "\n",
    "####8. Define the power of the test (in general terms).\n",
    "\n",
    "Power of a test is the probability that $H_0$ is rejected given that $H_1$ is true.\n",
    "\n",
    "####9. Calculate the power of this test, again assuming the ”true” population proportion is $\\theta^∗ = 0.45$.\n",
    "\n",
    "The **power of the test** (given $\\theta^*=0.45$) is: $1-\\beta=\\color{red}{0.5960}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
